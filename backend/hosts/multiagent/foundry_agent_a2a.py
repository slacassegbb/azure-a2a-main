"""
Foundry Host Agent - Multi-Agent Orchestration with Azure AI Foundry

This module implements the core host agent that coordinates multiple specialized remote agents
using the Agent-to-Agent (A2A) protocol. It provides:
- Multi-agent workflow orchestration with dynamic task decomposition
- Integration with Azure AI Foundry for LLM-powered coordination
- Real-time WebSocket streaming for UI updates
- Memory service for cross-conversation context
- File handling with Azure Blob Storage support
- Comprehensive error handling and retry logic

The host agent acts as an intelligent orchestrator that:
1. Receives user requests
2. Analyzes which specialized agents can help
3. Coordinates parallel or sequential agent execution
4. Synthesizes responses from multiple agents
5. Maintains conversation context across interactions
"""

import asyncio
import ast
import base64
import re
import json
import uuid
import os
import sys
import logging
import time
from dataclasses import dataclass
from enum import Enum
from datetime import datetime, timezone
from pathlib import Path
from typing import List, Dict, Any, Optional, Iterable, Literal
import httpx
from dotenv import load_dotenv
from openai import AsyncAzureOpenAI

# OpenTelemetry for distributed tracing and monitoring
from opentelemetry import trace
from azure.monitor.opentelemetry import configure_azure_monitor

# Azure authentication - supports multiple credential types for flexibility
from azure.identity import DefaultAzureCredential, ChainedTokenCredential, AzureCliCredential, ManagedIdentityCredential, EnvironmentCredential, ClientSecretCredential

# Azure AI Foundry Agent Service - Official SDK for enterprise agents
from azure.ai.projects.aio import AIProjectClient
from azure.ai.agents.models import (
    AsyncFunctionTool,
    AsyncToolSet,
    MessageDeltaChunk,
    ThreadMessage,
    ThreadRun,
    RunStep,
    AgentStreamEvent,
    MessageRole,
    FilePurpose,
)

# A2A Protocol SDK for agent-to-agent communication
from a2a.client import A2ACardResolver
from a2a.types import (
    AgentCard,
    Artifact,
    DataPart,
    FilePart,
    FileWithBytes,
    FileWithUri,
    Message,
    MessageSendConfiguration,
    MessageSendParams,
    Part,
    Task,
    TaskState,
    TextPart,
)

# Internal modules for agent coordination and data processing
from .remote_agent_connection import RemoteAgentConnections, TaskUpdateCallback, TaskCallbackArg
from .a2a_memory_service import a2a_memory_service
from .a2a_document_processor import a2a_document_processor

# Extracted models and parsers (refactored from this file)
from .models import (
    SessionContext,
    AgentModeTask,
    AgentModePlan,
    NextStep,
    WorkflowStepType,
    ParsedWorkflowStep,
    ParsedWorkflowGroup,
    ParsedWorkflow,
    TaskStateEnum,
    GoalStatus,
)
from .workflow_parser import WorkflowParser
from .tool_context import DummyToolContext
from .utils import (
    get_context_id,
    get_message_id,
    get_task_id,
    normalize_env_bool,
    normalize_env_int,
)
from .instructions import (
    build_agent_mode_instruction,
    build_orchestrator_instruction,
    apply_custom_instruction,
)
from pydantic import BaseModel, Field

# Tenant utilities for multi-tenancy support
from utils.tenant import get_tenant_from_context
# File parts utilities for standardized artifact handling
from utils.file_parts import (
    extract_uri,
    extract_filename,
    extract_mime_type,
    create_file_part,
    is_file_part,
    is_image_part,
    extract_all_images,
    convert_artifact_dict_to_file_part,
)
import time

# Load environment configuration from project root
ROOT_ENV_PATH = Path(__file__).resolve().parents[3] / ".env"
load_dotenv(dotenv_path=ROOT_ENV_PATH, override=False)

# Ensure logging utilities are accessible
backend_dir = Path(__file__).resolve().parents[2]
if str(backend_dir) not in sys.path:
    sys.path.insert(0, str(backend_dir))

from log_config import log_debug, log_info, log_success, log_warning, log_error, log_foundry_debug

logger = logging.getLogger(__name__)

# Configure distributed tracing with Azure Application Insights
application_insights_connection_string = os.environ.get("APPLICATIONINSIGHTS_CONNECTION_STRING")
if application_insights_connection_string:
    configure_azure_monitor(connection_string=application_insights_connection_string)
tracer = trace.get_tracer(__name__)


# Note: SessionContext, AgentModeTask, AgentModePlan, NextStep, WorkflowStepType,
# ParsedWorkflowStep, ParsedWorkflowGroup, ParsedWorkflow, and WorkflowParser
# have been extracted to models.py and workflow_parser.py
# 
# Utility functions (get_context_id, get_message_id, get_task_id, normalize_env_bool,
# normalize_env_int) have been extracted to utils.py


class FoundryHostAgent2:
    def __init__(
        self,
        remote_agent_addresses: List[str],
        http_client: httpx.AsyncClient,
        task_callback: Optional[TaskUpdateCallback] = None,
        enable_task_evaluation: bool = False,
        create_agent_at_startup: bool = True,  # Changed back: Using Foundry Agent Service
    ):
        """
        Initialize the Foundry Host Agent with Azure AI Foundry Agent Service backend.
        
        Args:
            remote_agent_addresses: List of remote agent URLs to connect to
            http_client: Shared HTTP client for agent communication
            task_callback: Optional callback for task status updates
            enable_task_evaluation: Whether to evaluate task completion quality
            create_agent_at_startup: Create agent in Azure AI Foundry at startup (enables portal visibility)
        """
        self.endpoint = os.environ["AZURE_AI_FOUNDRY_PROJECT_ENDPOINT"]
        
        try:
            log_foundry_debug("Initializing Azure AI Foundry Agent Service...")
            print("üí° TIP: If you see authentication errors, run 'python test_azure_auth.py' to diagnose")
            
            from azure.identity.aio import AzureCliCredential, DefaultAzureCredential, ManagedIdentityCredential
            
            # Detect if we're running in Azure Container Apps (managed identity)
            is_azure_container = os.environ.get('CONTAINER_APP_NAME') or os.environ.get('WEBSITE_INSTANCE_ID')
            
            if is_azure_container:
                # Use DefaultAzureCredential in Azure (will use managed identity)
                log_foundry_debug("üîµ Running in Azure Container Apps - using DefaultAzureCredential (Managed Identity)")
                self.credential = DefaultAzureCredential(exclude_interactive_browser_credential=True)
                log_foundry_debug("‚úÖ Using DefaultAzureCredential for managed identity")
            else:
                # Use AzureCliCredential locally
                cli_credential = AzureCliCredential(process_timeout=5)
                self.credential = cli_credential
                log_foundry_debug("‚úÖ Using AzureCliCredential for local development")
                    
        except Exception as e:
            log_foundry_debug(f"‚ö†Ô∏è Credential initialization failed: {e}")
            print("üí° DEBUG: Falling back to DefaultAzureCredential only")
            from azure.identity.aio import DefaultAzureCredential
            self.credential = DefaultAzureCredential(exclude_interactive_browser_credential=True)
            log_foundry_debug("‚úÖ Using DefaultAzureCredential as fallback")
        
        # Initialize Azure AI Project Client (async)
        self.project_client: Optional[AIProjectClient] = None
        self.agents_client = None  # Will be set from project_client.agents
        
        self.agent: Optional[Any] = None  # Agent object from Foundry Agent Service
        self.agent_id: Optional[str] = None
        self.task_callback = task_callback or self._default_task_callback
        self.httpx_client = http_client
        self.remote_agent_connections: Dict[str, RemoteAgentConnections] = {}
        self.cards: Dict[str, AgentCard] = {}
        self.agents: str = ''
        self.session_contexts: Dict[str, SessionContext] = {}
        
        # FOUNDRY AGENT SERVICE: Store thread IDs for conversation management
        self.thread_ids: Dict[str, str] = {}  # context_id -> thread_id
        
        # REMOVED: self.default_contextId = str(uuid.uuid4())
        # We NEVER want to use a UUID fallback - context_id must come from the request
        self.default_contextId = None
        self._agent_tasks: Dict[str, Optional[Task]] = {}
        self.agent_token_usage: Dict[str, dict] = {}  # Store token usage per agent
        self.host_token_usage: Dict[str, int] = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}  # Host agent tokens
        
        self.enable_task_evaluation = enable_task_evaluation
        self._active_conversations: Dict[str, str] = {}
        self.max_retries = 2
        # Configure context-sharing between agents for improved continuity
        # When enabled, agents receive information about previous agent responses
        include_flag = os.environ.get("A2A_INCLUDE_LAST_HOST_TURN", "true").strip().lower()
        self.include_last_host_turn = include_flag not in {"false", "0", "no"}
        
        # Limit context size to prevent token overflow while maintaining useful history
        self.last_host_turn_max_chars = normalize_env_int(
            os.environ.get("A2A_LAST_HOST_TURN_MAX_CHARS"),
            1500,  # Default: ~500 tokens of context
        )
        
        # Number of recent agent interactions to include in context (1-5 turns)
        self.last_host_turns = max(
            1,
            min(
                5,
                normalize_env_int(os.environ.get("A2A_LAST_HOST_TURNS"), 1),
            ),
        )
        
        # Maximum characters for memory search summaries
        self.memory_summary_max_chars = max(
            200,
            normalize_env_int(os.environ.get("A2A_MEMORY_SUMMARY_MAX_CHARS"), 2000),
        )

        self._azure_blob_client = None
        self._init_azure_blob_client()
        self._clear_memory_on_startup()
        self._messages = []
        self._host_responses_sent = set()
        self._host_manager = None
        self.custom_root_instruction = None
        self._cached_token = None
        self._token_expiry = None
        self._create_agent_at_startup = create_agent_at_startup
        self._agent_registry_path = self._find_agent_registry_path()

        loop = asyncio.get_running_loop()
        loop.create_task(self.init_remote_agent_addresses(remote_agent_addresses))
        
        if self._create_agent_at_startup:
            loop.create_task(self._create_agent_at_startup_task())
    
    async def _ensure_project_client(self):
        """
        Initialize Azure AI Project Client and Agents Client if not already done.
        
        IMPORTANT: AIProjectClient must be created in the same event loop where it's used.
        If the event loop has changed, we need to recreate the client.
        """
        current_loop = asyncio.get_running_loop()
        
        # Check if we need to recreate the client for a new event loop
        if self.project_client is not None:
            # Check if the client's loop is still the current loop
            try:
                # If the client was created in a different loop, recreate it
                if hasattr(self, '_project_client_loop') and self._project_client_loop != current_loop:
                    log_foundry_debug("ÔøΩ Event loop changed, recreating AIProjectClient...")
                    # Close the old client if possible
                    if hasattr(self.project_client, 'close'):
                        try:
                            await self.project_client.close()
                        except:
                            pass
                    self.project_client = None
                    self.agents_client = None
                    self.credential = None
            except:
                pass
        
        # Recreate credential if needed
        if self.credential is None:
            from azure.identity.aio import AzureCliCredential, DefaultAzureCredential
            
            # Detect if we're running in Azure Container Apps (managed identity)
            is_azure_container = os.environ.get('CONTAINER_APP_NAME') or os.environ.get('WEBSITE_INSTANCE_ID')
            
            if is_azure_container:
                # Use DefaultAzureCredential in Azure (will use managed identity)
                self.credential = DefaultAzureCredential(exclude_interactive_browser_credential=True)
                log_foundry_debug("Recreated DefaultAzureCredential (Managed Identity) for new event loop")
            else:
                # Use AzureCliCredential locally
                self.credential = AzureCliCredential(process_timeout=5)
                log_foundry_debug("Recreated AzureCliCredential for new event loop")
        
        if self.project_client is None:
            log_foundry_debug("ÔøΩüîß Initializing AIProjectClient...")
            self.project_client = AIProjectClient(
                endpoint=self.endpoint,
                credential=self.credential,
            )
            self._project_client_loop = current_loop
            log_foundry_debug("‚úÖ AIProjectClient initialized")
        
        if self.agents_client is None:
            log_foundry_debug("üîß Getting AgentsClient from project...")
            self.agents_client = self.project_client.agents
            log_foundry_debug("‚úÖ AgentsClient ready")
    
    async def _initialize_function_tools(self):
        """
        Initialize function tools for the agent.
        
        Returns AsyncFunctionTool configured with our agent coordination functions.
        """
        # Define the functions that the agent can call
        # AsyncFunctionTool expects a LIST of functions, not a dict
        user_functions = [
            self.list_remote_agents_sync,
            self.send_message_sync,
        ]
        
        # Create async function tool
        functions = AsyncFunctionTool(user_functions)
        
        # Create toolset and add functions
        toolset = AsyncToolSet()
        toolset.add(functions)
        
        # Enable automatic function call execution (synchronous method)
        self.agents_client.enable_auto_function_calls(toolset)
        
        return toolset

    async def set_session_agents(self, session_agents: List[Dict[str, Any]]):
        """Set the available agents for this session/request.
        
        This clears existing agents and sets only the provided session agents.
        Called before processing each request to ensure session isolation.
        
        Args:
            session_agents: List of agent dicts with url, name, description, skills, etc.
        """
        print(f"üîµ [SET_SESSION_AGENTS] Starting with {len(session_agents)} agents to register")
        
        # Clear existing agents
        self.cards.clear()
        self.remote_agent_connections.clear()
        self.agents = ''
        
        # Register each session agent
        for agent_data in session_agents:
            agent_url = agent_data.get('url')
            agent_name = agent_data.get('name', 'Unknown')
            print(f"üîµ [SET_SESSION_AGENTS] Processing agent: {agent_name}")
            print(f"üîµ [SET_SESSION_AGENTS] Agent URL: {agent_url}")
            if agent_url:
                try:
                    print(f"üîµ [SET_SESSION_AGENTS] Calling retrieve_card for {agent_name}...")
                    await self.retrieve_card(agent_url)
                    print(f"‚úÖ [SET_SESSION_AGENTS] Session agent registered: {agent_name}")
                except Exception as e:
                    print(f"‚ö†Ô∏è [SET_SESSION_AGENTS] Failed to register session agent {agent_url}: {e}")
                    import traceback
                    print(f"‚ö†Ô∏è [SET_SESSION_AGENTS] Traceback: {traceback.format_exc()}")
        
        print(f"üìã [SET_SESSION_AGENTS] Session now has {len(self.cards)} agents: {list(self.cards.keys())}")

    def _find_agent_registry_path(self) -> Path:
        """Resolve the agent registry path within the backend/data directory."""
        backend_root = Path(__file__).resolve().parents[2]
        registry_path = backend_root / "data" / "agent_registry.json"
        registry_path.parent.mkdir(parents=True, exist_ok=True)
        if registry_path.exists():
            log_debug(f"üìã Found agent registry at: {registry_path}")
        else:
            log_debug(f"üìã Agent registry will be created at: {registry_path}")
        return registry_path

    def _load_agent_registry(self) -> List[Dict[str, Any]]:
        """Load agent registry from JSON file."""
        try:
            if self._agent_registry_path.exists():
                with open(self._agent_registry_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            else:
                log_debug(f"üìã Agent registry file not found at {self._agent_registry_path}, returning empty list")
                return []
        except Exception as e:
            print(f"‚ùå Error loading agent registry: {e}")
            return []

    def _save_agent_registry(self, agents: List[Dict[str, Any]]):
        """Save agent registry to JSON file."""
        try:
            # Ensure directory exists
            self._agent_registry_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(self._agent_registry_path, 'w', encoding='utf-8') as f:
                json.dump(agents, f, indent=2, ensure_ascii=False)
            log_debug(f"üìã Saved agent registry with {len(agents)} agents to {self._agent_registry_path}")
        except Exception as e:
            print(f"‚ùå Error saving agent registry: {e}")

    def _agent_card_to_dict(self, card: AgentCard) -> Dict[str, Any]:
        """Convert AgentCard object to dictionary for JSON serialization in the agent registry."""
        try:
            card_dict = {
                "name": card.name,
                "description": card.description,
                "version": getattr(card, 'version', '1.0.0'),
                "url": card.url,
                "defaultInputModes": getattr(card, 'defaultInputModes', ["text"]),
                "defaultOutputModes": getattr(card, 'defaultOutputModes', ["text"]),
            }
            
            if hasattr(card, 'capabilities') and card.capabilities:
                capabilities_dict = {}
                if hasattr(card.capabilities, 'streaming'):
                    capabilities_dict["streaming"] = card.capabilities.streaming
                card_dict["capabilities"] = capabilities_dict
            
            if hasattr(card, 'skills') and card.skills:
                skills_list = []
                for skill in card.skills:
                    skill_dict = {
                        "id": getattr(skill, 'id', ''),
                        "name": getattr(skill, 'name', ''),
                        "description": getattr(skill, 'description', ''),
                        "examples": getattr(skill, 'examples', []),
                        "tags": getattr(skill, 'tags', [])
                    }
                    skills_list.append(skill_dict)
                card_dict["skills"] = skills_list
            
            return card_dict
        except Exception as e:
            print(f"‚ùå Error converting agent card to dict: {e}")
            return {
                "name": getattr(card, 'name', 'Unknown'),
                "description": getattr(card, 'description', ''),
                "version": "1.0.0",
                "url": getattr(card, 'url', ''),
                "defaultInputModes": ["text"],
                "defaultOutputModes": ["text"]
            }

    def _update_agent_registry(self, card: AgentCard):
        """Persist agent card to registry file, updating existing entries or adding new ones."""
        try:
            registry = self._load_agent_registry()
            card_dict = self._agent_card_to_dict(card)
            
            existing_index = None
            # First, check by name (primary identifier)
            for i, existing_agent in enumerate(registry):
                if existing_agent.get("name") == card.name:
                    existing_index = i
                    break
            
            # If not found by name, check by URL (for backward compatibility)
            if existing_index is None:
                for i, existing_agent in enumerate(registry):
                    if existing_agent.get("url") == card.url:
                        existing_index = i
                        break
            
            if existing_index is not None:
                registry[existing_index] = card_dict
                log_debug(f"üìã Updated existing agent in registry: {card.name} at {card.url}")
            else:
                registry.append(card_dict)
                log_debug(f"üìã Added new agent to registry: {card.name} at {card.url}")
            
            self._save_agent_registry(registry)
            
        except Exception as e:
            print(f"‚ùå Error updating agent registry: {e}")

    async def _create_agent_at_startup_task(self):
        """Background task to create the agent at startup with proper error handling."""
        try:
            log_debug("üöÄ Creating Azure AI Foundry agent at startup...")
            await self.create_agent()
            print("‚úÖ Azure AI Foundry agent created successfully at startup!")
        except Exception as e:
            print(f"‚ùå Failed to create agent at startup: {e}")
            print("üí° Agent will be created lazily when first conversation occurs")
            # Don't raise - allow the application to continue and create agent lazily

    def _init_azure_blob_client(self):
        """Initialize Azure Blob Storage client if environment variables are configured."""
        try:
            self._azure_blob_client = None
            self._azure_blob_container = os.getenv('AZURE_BLOB_CONTAINER', 'a2a-files')
            azure_storage_connection_string = os.getenv('AZURE_STORAGE_CONNECTION_STRING')
            azure_storage_account_name = os.getenv('AZURE_STORAGE_ACCOUNT_NAME')
            
            if azure_storage_connection_string:
                from azure.storage.blob import BlobServiceClient
                self._azure_blob_client = BlobServiceClient.from_connection_string(
                    azure_storage_connection_string,
                    api_version="2023-11-03",
                )
                print("‚úÖ Azure Blob Storage initialized with connection string (sync client)")
                print(f"Connection string starts with: {azure_storage_connection_string[:50]}...")
                print(f"Azure storage account: {self._azure_blob_client.account_name}")
            elif azure_storage_account_name:
                from azure.storage.blob import BlobServiceClient
                account_url = f"https://{azure_storage_account_name}.blob.core.windows.net"
                self._azure_blob_client = BlobServiceClient(
                    account_url,
                    credential=self.credential,
                    api_version="2023-11-03",
                )
                print(f"‚úÖ Azure Blob Storage initialized with managed identity (sync client): {account_url}")
            else:
                print("‚ùå Azure Blob Storage not configured - using local storage only")
                print(f"AZURE_STORAGE_CONNECTION_STRING: {azure_storage_connection_string}")
                print(f"AZURE_STORAGE_ACCOUNT_NAME: {azure_storage_account_name}")
                self._azure_blob_container = None
            
            if self._azure_blob_client:
                print(f"ü™£ Target Azure Blob container: {self._azure_blob_container}")
                loop = asyncio.get_running_loop()
                loop.create_task(self._verify_blob_connection())
            else:
                print("‚ÑπÔ∏è Azure Blob client not initialized; uploads will use local storage")

        except ImportError as e:
            print("‚ùå Azure Storage SDK not installed - using local storage only")
            print(f"ImportError details: {e}")
        except Exception as e:
            print(f"‚ùå Failed to initialize Azure Blob Storage: {e}")
            log_error(f"Exception type: {type(e).__name__}")
            self._azure_blob_client = None

    async def _verify_blob_connection(self):
        """Log diagnostics about the configured Azure Blob container."""
        if not self._azure_blob_client or not self._azure_blob_container:
            print("‚ÑπÔ∏è Azure Blob verification skipped: client or container not set")
            return

        try:
            print("üîç Azure Blob check: resolving container client...")
            container_client = self._azure_blob_client.get_container_client(self._azure_blob_container)

            print("üîç Azure Blob check: ensuring container exists...")
            try:
                await asyncio.to_thread(container_client.create_container)
                print(f"‚úÖ Azure Blob container '{self._azure_blob_container}' created")
            except Exception as create_err:
                from azure.core.exceptions import ResourceExistsError
                if isinstance(create_err, ResourceExistsError):
                    print(f"‚ÑπÔ∏è Azure Blob container '{self._azure_blob_container}' already exists")
                else:
                    raise

            print("üîç Azure Blob check: listing blobs (up to 5 entries)...")
            blob_count = 0
            for blob in container_client.list_blobs(name_starts_with="a2a-artifacts/"):
                print(f"   ‚Ä¢ Existing blob: {blob.name} (size={blob.size})")
                blob_count += 1
                if blob_count >= 5:
                    print("   ‚Ä¢ ... additional blobs omitted ...")
                    break
            if blob_count == 0:
                print("   ‚Ä¢ No blobs found yet in this container")

            print("üîç Azure Blob check: uploading connectivity probe...")
            probe_blob_name = f"a2a-artifacts/_connectivity_probe_.txt"
            probe_client = container_client.get_blob_client(probe_blob_name)
            probe_payload = f"connection verified at {datetime.now(timezone.utc).isoformat()}"
            await asyncio.to_thread(probe_client.upload_blob, probe_payload, overwrite=True)
            print(f"‚úÖ Azure Blob probe uploaded: {probe_blob_name}")

        except Exception as e:
            print(f"‚ùå Azure Blob verification failed: {e}")
            import traceback
            print(traceback.format_exc())

    def _clear_memory_on_startup(self):
        """Clear memory index automatically on startup for clean testing"""
        print(f"üßπ Auto-clearing memory index on startup...")
        try:
            success = a2a_memory_service.clear_all_interactions()
            if success:
                print(f"‚úÖ Memory index auto-cleared successfully")
            else:
                print(f"‚ö†Ô∏è Memory index auto-clear had no effect (may be empty)")
        except Exception as e:
            print(f"‚ö†Ô∏è Error auto-clearing memory index: {e}")
            print(f"Continuing with startup...")

    async def _get_auth_headers(self) -> Dict[str, str]:
        """
        Obtain Azure authentication headers with token caching for performance.
        
        Implements:
        - Token caching with 5-minute expiry buffer to avoid auth failures
        - Automatic retry logic (3 attempts) with exponential backoff
        - Timeout protection (8 seconds) to prevent hanging
        - Helpful error messages for common authentication issues
        
        Returns:
            Dict containing Authorization and Content-Type headers
        """
        log_foundry_debug(f"Getting authentication token with timeout handling and caching")
        
        # Return cached token if still valid (with 5-minute safety buffer)
        if self._cached_token and self._token_expiry:
            from datetime import datetime as dt, timedelta
            # Add 5 minute buffer before expiry
            buffer_time = dt.now() + timedelta(minutes=5)
            if self._token_expiry > buffer_time:
                log_foundry_debug(f"‚úÖ Using cached token (expires: {self._token_expiry})")
                return {
                    "Authorization": f"Bearer {self._cached_token}",
                    "Content-Type": "application/json"
                }
        
        # Token acquisition with retry logic for reliability
        max_retries = 3
        for attempt in range(max_retries):
            try:
                log_foundry_debug(f"Token attempt {attempt + 1}/{max_retries}...")
                
                # Get token with proper async handling
                import asyncio
                import inspect
                
                # Check if get_token is async or sync
                get_token_method = self.credential.get_token
                
                if inspect.iscoroutinefunction(get_token_method):
                    # Async credential - await directly with timeout
                    log_foundry_debug("Using async credential.get_token()")
                    token = await asyncio.wait_for(
                        self.credential.get_token("https://ai.azure.com/.default"),
                        timeout=8.0
                    )
                else:
                    # Sync credential - run in executor to avoid blocking
                    log_foundry_debug("Using sync credential.get_token() in executor")
                    async def get_token_async():
                        loop = asyncio.get_event_loop()
                        return await loop.run_in_executor(
                            None, 
                            lambda: self.credential.get_token("https://ai.azure.com/.default")
                        )
                    
                    token = await asyncio.wait_for(get_token_async(), timeout=8.0)
                
                # Cache the token
                self._cached_token = token.token
                from datetime import datetime as dt
                self._token_expiry = dt.fromtimestamp(token.expires_on)
                
                headers = {
                    "Authorization": f"Bearer {token.token}",
                    "Content-Type": "application/json"
                }
                log_foundry_debug(f"‚úÖ Authentication headers obtained successfully (expires: {self._token_expiry})")
                return headers
                
            except asyncio.TimeoutError:
                error_name = "TimeoutError"
                error_msg = "Authentication request timed out after 8 seconds"
                log_foundry_debug(f"‚ö†Ô∏è Auth attempt {attempt + 1} timed out after 8 seconds")
                
            except Exception as e:
                error_name = type(e).__name__
                error_msg = str(e)
                
                if attempt < max_retries - 1:
                    log_foundry_debug(f"‚ö†Ô∏è Auth attempt {attempt + 1} failed ({error_name}), retrying in 3 seconds...")
                    log_foundry_debug(f"‚ö†Ô∏è Error details: {error_msg}")
                    await asyncio.sleep(3)  # Wait 3 seconds before retry
                    continue
                else:
                    log_foundry_debug(f"‚ùå Failed to get authentication token after {max_retries} attempts: {error_name}: {e}")
                    
                    # Provide specific help based on error type
                    if "CredentialUnavailableError" in error_name and "Azure CLI" in error_msg:
                        print("üí° DEBUG: Azure CLI authentication failed. Try:")
                        print("   1. az login --tenant <your-tenant-id>")
                        print("   2. az account set --subscription <your-subscription-id>")
                        print("   3. Set environment variables for service principal auth")
                    elif "TimeoutError" in error_name or "timed out" in error_msg.lower():
                        print("üí° DEBUG: Authentication timed out. Try:")
                        print("   1. Check your network connection")
                        print("   2. Run 'az login' to refresh your session")
                        print("   3. Consider using service principal authentication for better reliability")
                    elif "ChainedTokenCredential" in error_msg:
                        print("üí° DEBUG: All credential types failed. Options:")
                        print("   1. Set environment variables: AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET")
                        print("   2. Run 'az login' and try again")
                        print("   3. Run from Azure environment (VM, Container App, etc.)")
                    
                    raise

    def _clear_cached_token(self):
        """Clear cached authentication token to force refresh on next request"""
        log_foundry_debug(f"üîÑ Clearing cached authentication token")
        self._cached_token = None
        self._token_expiry = None

    async def refresh_azure_cli_session(self):
        """Helper method to refresh Azure CLI session when authentication fails"""
        try:
            log_foundry_debug(f"üîÑ Attempting to refresh Azure CLI session...")
            import subprocess
            import asyncio
            
            # Run az account get-access-token to refresh session
            process = await asyncio.create_subprocess_exec(
                'az', 'account', 'get-access-token', '--output', 'json',
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await asyncio.wait_for(process.communicate(), timeout=10.0)
            
            if process.returncode == 0:
                log_foundry_debug(f"‚úÖ Azure CLI session refreshed successfully")
                self._clear_cached_token()  # Clear cache to use fresh token
                return True
            else:
                log_foundry_debug(f"‚ùå Azure CLI refresh failed: {stderr.decode()}")
                return False
                
        except asyncio.TimeoutError:
            log_foundry_debug(f"‚ùå Azure CLI refresh timed out")
            return False
        except Exception as e:
            log_foundry_debug(f"‚ùå Error refreshing Azure CLI session: {e}")
            return False

    def _format_tools_for_responses_api(self) -> List[Dict[str, Any]]:
        """
        Format agent tools for Responses API.
        
        Returns tools in the Responses API format for function calling.
        Uses _get_tools() to get the tool definitions.
        
        Returns:
            List of tool definitions in Responses API format
        """
        # Use _get_tools() which has the correct tool definitions
        tools = self._get_tools()
        print(f"üîß [TOOLS] _format_tools_for_responses_api returning {len(tools)} tools")
        for tool in tools:
            print(f"  ‚Ä¢ Tool: {tool.get('name', 'unknown')}")
        return tools

    def _get_openai_endpoint(self) -> str:
        """
        Convert AI Foundry endpoint to OpenAI /v1/ endpoint format.
        
        Converts from: https://RESOURCE.services.ai.azure.com/subscriptions/.../
        To: https://RESOURCE.openai.azure.com/openai/v1/
        """
        endpoint = self.endpoint
        
        if "services.ai.azure.com" in endpoint:
            # Extract resource name from AI Foundry endpoint
            # Example: https://simonfoundry.services.ai.azure.com/...
            parts = endpoint.split("//")[1].split(".")[0]
            openai_endpoint = f"https://{parts}.openai.azure.com/openai/v1/"
            log_foundry_debug(f"Converted endpoint: {endpoint} -> {openai_endpoint}")
            return openai_endpoint
        else:
            # Already in OpenAI format or needs manual /openai/v1/ suffix
            openai_endpoint = endpoint if endpoint.endswith("/openai/v1/") else f"{endpoint.rstrip('/')}/openai/v1/"
            log_foundry_debug(f"Using endpoint as-is: {openai_endpoint}")
            return openai_endpoint

    def _get_openai_client(self):
        """
        Get client configured for Responses API via direct HTTP calls.
        
        Note: The Responses API is accessed via direct HTTP to /openai/v1/responses,
        not through the OpenAI SDK's standard client interface.
        
        This method is kept for compatibility but the actual HTTP calls
        are made directly in _create_response_with_streaming.
        """
        # This is now a placeholder - actual HTTP calls are made directly
        return None

    def _get_client(self):
        """Legacy method - now throws error to identify remaining SDK usage"""
        raise NotImplementedError(
            "‚ùå _get_client() is no longer supported! "
            "The foundry_agent_a2a.py now uses HTTP API calls instead of the azure.ai.agents SDK. "
            "This error indicates that some code is still trying to use the old SDK approach. "
            "Please update the calling code to use HTTP-based methods."
        )

    async def _create_response_with_streaming(
        self,
        user_message: str,
        context_id: str,
        session_context: SessionContext,
        tools: List[Dict[str, Any]],  # No longer used - tools are in agent
        instructions: str,  # No longer used - instructions are in agent
        event_logger=None
    ) -> Dict[str, Any]:
        """
        Create a response using Azure AI Foundry Agent Service with streaming.
        
        Uses the Agent Service SDK which provides:
        - Thread-based conversation management
        - Automatic tool execution
        - Built-in streaming support
        - Full Application Insights telemetry
        
        Returns:
            Dict with keys: id, text, tool_calls, status, usage
        """
        log_foundry_debug(f"_create_response_with_streaming: Creating streaming response for context {context_id}")
        
        try:
            # Ensure project client and agent are initialized
            await self._ensure_project_client()
            if not self.agent:
                await self.create_agent()
            
            log_foundry_debug(f"Using agent ID: {self.agent_id}")
            
            # Get or create thread for this context
            if context_id not in self.thread_ids:
                log_foundry_debug(f"Creating new thread for context: {context_id}")
                thread = await self.agents_client.threads.create()
                self.thread_ids[context_id] = thread.id
                log_foundry_debug(f"Created thread ID: {thread.id}")
            
            thread_id = self.thread_ids[context_id]
            log_foundry_debug(f"Using thread ID: {thread_id}")
            
            # Cancel any active runs before creating new message
            # This prevents "Can't add messages while a run is active" errors
            try:
                runs = self.agents_client.runs.list(thread_id=thread_id)
                async for run in runs:
                    if run.status in ["in_progress", "requires_action", "queued"]:
                        print(f"‚ö†Ô∏è Cancelling stuck run {run.id} with status {run.status}")
                        try:
                            await self.agents_client.runs.cancel(thread_id=thread_id, run_id=run.id)
                            print(f"‚úÖ Cancelled run {run.id}")
                        except Exception as cancel_error:
                            print(f"‚ö†Ô∏è Could not cancel run {run.id}: {cancel_error}")
            except Exception as list_error:
                log_foundry_debug(f"Could not list runs: {list_error}")
            
            # Create message in thread
            log_foundry_debug(f"Creating message: {user_message[:100]}...")
            message = await self.agents_client.messages.create(
                thread_id=thread_id,
                role=MessageRole.USER,
                content=user_message
            )
            log_foundry_debug(f"Created message ID: {message.id}")
            
            # Stream the run
            full_text = ""
            run_id = None
            status = "completed"
            
            log_foundry_debug(f"Starting streaming run...")
            
            # Track if we're handling tool calls
            tool_calls_to_execute = []
            
            # IMPORTANT: Use "required" to force the model to call tools
            # With "auto", the model hallucinates agent responses instead of actually calling them
            # The model should call list_remote_agents or send_message - never answer directly about agent tasks
            stream = await self.agents_client.runs.stream(
                thread_id=thread_id,
                agent_id=self.agent_id,
                tool_choice="required"
            )
            
            # Use async context manager - it returns an event handler
            async with stream as event_handler:
                # Iterate over events from the event handler
                async for event in event_handler:
                    log_foundry_debug(f"Stream event: {event}")
                    
                    # Check event type and extract data
                    if hasattr(event, 'event') and hasattr(event, 'data'):
                        event_type = event.event
                        event_data = event.data
                    elif isinstance(event, tuple):
                        event_type, event_data, *_ = event
                    else:
                        # Try to get the data directly
                        event_data = event
                        event_type = type(event).__name__
                    
                    log_foundry_debug(f"Event type: {event_type}, Data type: {type(event_data).__name__}")
                    
                    if isinstance(event_data, MessageDeltaChunk):
                        # TEXT CHUNK - stream to WebSocket
                        chunk = event_data.text
                        if chunk:
                            full_text += chunk
                            await self._emit_text_chunk(chunk, context_id)
                    
                    elif isinstance(event_data, ThreadRun):
                        run_id = event_data.id
                        status = event_data.status
                        log_foundry_debug(f"Run status: {status}")
                        
                        # Capture tool calls if requires_action
                        # Status can be a RunStatus enum or string, convert to string for comparison
                        status_str = str(status).lower() if status else ""
                        if "requires_action" in status_str and hasattr(event_data, 'required_action'):
                            required_action = event_data.required_action
                            if required_action and hasattr(required_action, 'submit_tool_outputs'):
                                tool_calls_to_execute = required_action.submit_tool_outputs.tool_calls
                                log_foundry_debug(f"üîß Captured {len(tool_calls_to_execute)} tool calls to execute")
                    
                    elif event_type == AgentStreamEvent.ERROR:
                        log_error(f"Stream error: {event_data}")
                        raise Exception(f"Streaming error: {event_data}")
                    
                    elif event_type == AgentStreamEvent.DONE:
                        log_foundry_debug("Stream completed")
                        break
            
            log_foundry_debug(f"Stream complete - text length: {len(full_text)}")
            
            # MULTI-TURN TOOL EXECUTION LOOP
            # Keep executing tool calls until status is completed
            max_tool_iterations = 30
            tool_iteration = 0
            
            # Helper to check if status requires action (handles enum and string)
            def status_requires_action(s):
                if s is None:
                    return False
                s_str = str(s).lower()
                return "requires_action" in s_str
            
            while status_requires_action(status) and tool_calls_to_execute and tool_iteration < max_tool_iterations:
                tool_iteration += 1
                log_foundry_debug(f"üîß Tool iteration {tool_iteration}: Executing {len(tool_calls_to_execute)} tool calls...")
                
                # ‚úÖ WORKFLOW VISIBILITY: Show that host agent is calling tools
                for tool_call in tool_calls_to_execute:
                    function_name = tool_call.function.name
                    asyncio.create_task(self._emit_granular_agent_event(
                        "foundry-host-agent",
                        f"üõ†Ô∏è Calling tool: {function_name}",
                        context_id
                    ))
                
                # Separate send_message calls for parallel execution
                send_message_calls = []
                other_calls = []
                
                log_foundry_debug(f"üîß Processing {len(tool_calls_to_execute)} tool calls")
                for tool_call in tool_calls_to_execute:
                    function_name = tool_call.function.name
                    log_foundry_debug(f"üîß Tool call: {function_name} - args: {tool_call.function.arguments[:100] if tool_call.function.arguments else 'None'}...")
                    if function_name == "send_message_sync":
                        send_message_calls.append(tool_call)
                    else:
                        other_calls.append(tool_call)
                
                log_foundry_debug(f"üîß Separated: {len(send_message_calls)} send_message calls, {len(other_calls)} other calls")
                tool_outputs = []
                
                # Execute send_message calls in PARALLEL if there are multiple
                if len(send_message_calls) > 1:
                    log_foundry_debug(f"üöÄ Executing {len(send_message_calls)} send_message calls in PARALLEL...")
                    
                    async def execute_send_message(tool_call):
                        """Execute a single send_message call and return result with tool_call_id"""
                        try:
                            import json
                            arguments = json.loads(tool_call.function.arguments) if tool_call.function.arguments else {}
                            agent_name = arguments.get("agent_name")
                            result = await self.send_message_sync(
                                agent_name=agent_name,
                                message=arguments.get("message")
                            )
                            
                            # Convert response_parts list to clean text for the model
                            # The model needs readable text, not complex JSON structures
                            result_str = self._format_agent_response_for_model(result, agent_name)
                            
                            log_foundry_debug(f"üîß Tool send_message_sync returned: {result_str[:200]}...")
                            asyncio.create_task(self._emit_granular_agent_event(
                                "foundry-host-agent",
                                f"‚úÖ Tool send_message_sync completed for {agent_name}",
                                context_id
                            ))
                            
                            return {"tool_call_id": tool_call.id, "output": result_str}
                        except Exception as e:
                            log_error(f"üîß Error executing send_message_sync: {e}")
                            import traceback
                            traceback.print_exc()
                            return {"tool_call_id": tool_call.id, "output": json.dumps({"error": str(e)})}
                    
                    # Execute all send_message calls in parallel using asyncio.gather
                    parallel_results = await asyncio.gather(
                        *[execute_send_message(tc) for tc in send_message_calls],
                        return_exceptions=True
                    )
                    
                    # Collect results
                    import json as json_module  # Ensure json is available in this scope
                    log_foundry_debug(f"üîß Parallel execution returned {len(parallel_results)} results")
                    for idx, result in enumerate(parallel_results):
                        if isinstance(result, Exception):
                            log_error(f"üîß Parallel execution error for call {idx}: {type(result).__name__}: {result}")
                            import traceback
                            traceback.print_exc()
                            # Still add an error result for this tool_call
                            if idx < len(send_message_calls):
                                tool_call = send_message_calls[idx]
                                tool_outputs.append({
                                    "tool_call_id": tool_call.id,
                                    "output": json_module.dumps({"error": f"Exception: {str(result)}"})
                                })
                        else:
                            log_foundry_debug(f"üîß Parallel result {idx}: {str(result)[:200]}...")
                            tool_outputs.append(result)
                    
                    log_foundry_debug(f"‚úÖ Parallel execution complete - {len(tool_outputs)} outputs collected")
                
                elif len(send_message_calls) == 1:
                    # Single send_message - execute normally
                    tool_call = send_message_calls[0]
                    function_name = tool_call.function.name
                    arguments_str = tool_call.function.arguments
                    tool_call_id = tool_call.id
                    
                    log_foundry_debug(f"üîß Executing single tool: {function_name} with args: {arguments_str}")
                    
                    try:
                        import json
                        arguments = json.loads(arguments_str) if arguments_str else {}
                        agent_name = arguments.get("agent_name")
                        result = await self.send_message_sync(
                            agent_name=agent_name,
                            message=arguments.get("message")
                        )
                        
                        # Use the same clean formatting as parallel execution
                        result_str = self._format_agent_response_for_model(result, agent_name)
                        
                        log_foundry_debug(f"üîß Tool {function_name} returned: {result_str[:200]}...")
                        asyncio.create_task(self._emit_granular_agent_event(
                            "foundry-host-agent",
                            f"‚úÖ Tool {function_name} completed",
                            context_id
                        ))
                        
                        tool_outputs.append({
                            "tool_call_id": tool_call_id,
                            "output": result_str
                        })
                    except Exception as e:
                        log_error(f"üîß Error executing tool {function_name}: {e}")
                        import traceback
                        traceback.print_exc()
                        tool_outputs.append({
                            "tool_call_id": tool_call_id,
                            "output": json.dumps({"error": str(e)})
                        })
                
                # Execute other (non-send_message) tool calls sequentially
                for tool_call in other_calls:
                    function_name = tool_call.function.name
                    arguments_str = tool_call.function.arguments
                    tool_call_id = tool_call.id
                    
                    log_foundry_debug(f"üîß Executing other tool: {function_name} with args: {arguments_str}")
                    
                    try:
                        # Parse arguments
                        import json
                        arguments = json.loads(arguments_str) if arguments_str else {}
                        
                        # Execute the tool function
                        if function_name == "list_remote_agents_sync":
                            result = self.list_remote_agents_sync()  # Synchronous call
                        else:
                            result = {"error": f"Unknown function: {function_name}"}
                        
                        # Convert result to JSON-serializable format
                        # Handle Pydantic objects by using model_dump() or converting to dict
                        if hasattr(result, 'model_dump'):
                            # Pydantic v2 object
                            result_dict = result.model_dump(mode='json')
                            result_str = json.dumps(result_dict)
                        elif hasattr(result, 'dict'):
                            # Pydantic v1 object or dict
                            result_dict = result.dict() if hasattr(result, 'dict') and callable(result.dict) else result
                            result_str = json.dumps(result_dict)
                        elif isinstance(result, str):
                            result_str = result
                        elif isinstance(result, (list, dict)):
                            # Try to serialize, handling nested Pydantic objects
                            try:
                                result_str = json.dumps(result)
                            except TypeError:
                                # If direct serialization fails, convert Pydantic objects
                                def convert_pydantic(obj):
                                    if hasattr(obj, 'model_dump'):
                                        return obj.model_dump(mode='json')
                                    elif hasattr(obj, 'dict') and callable(obj.dict):
                                        return obj.dict()
                                    elif isinstance(obj, list):
                                        return [convert_pydantic(item) for item in obj]
                                    elif isinstance(obj, dict):
                                        return {k: convert_pydantic(v) for k, v in obj.items()}
                                    return obj
                                
                                result_converted = convert_pydantic(result)
                                result_str = json.dumps(result_converted)
                        else:
                            result_str = json.dumps(result)
                        
                        log_foundry_debug(f"üîß Tool {function_name} returned: {result_str[:200]}...")
                        
                        # ‚úÖ WORKFLOW VISIBILITY: Show tool execution result
                        asyncio.create_task(self._emit_granular_agent_event(
                            "foundry-host-agent",
                            f"‚úÖ Tool {function_name} completed",
                            context_id
                        ))
                        
                        tool_outputs.append({
                            "tool_call_id": tool_call_id,
                            "output": result_str
                        })
                    except Exception as e:
                        log_error(f"üîß Error executing tool {function_name}: {e}")
                        tool_outputs.append({
                            "tool_call_id": tool_call_id,
                            "output": json.dumps({"error": str(e)})
                        })
                
                # Submit tool outputs and continue streaming
                log_foundry_debug(f"üîß Submitting {len(tool_outputs)} tool outputs...")
                
                # Submit tool outputs - try without event_handler first
                # The SDK might work like runs.stream() - returns stream directly
                try:
                    stream = await self.agents_client.runs.submit_tool_outputs_stream(
                        thread_id=thread_id,
                        run_id=run_id,
                        tool_outputs=tool_outputs
                    )
                    log_foundry_debug(f"üîß submit_tool_outputs_stream returned: {type(stream)}")
                except TypeError as e:
                    # If it requires event_handler, provide one
                    if "event_handler" in str(e):
                        log_foundry_debug("üîß Retrying with custom event handler to capture response...")
                        from azure.ai.agents.models import AsyncAgentEventHandler
                        
                        # Create custom event handler to capture response text AND tool calls
                        class ResponseCapturingHandler(AsyncAgentEventHandler):
                            def __init__(self):
                                super().__init__()
                                self.response_text = ""
                                self.final_status = None
                                self.tool_calls = []  # Capture tool calls for multi-turn
                            
                            async def on_message_delta(self, delta):
                                """Capture message chunks as they arrive"""
                                if hasattr(delta, 'text') and delta.text:
                                    self.response_text += delta.text
                                    # Emit to WebSocket in real-time
                                    await self._emit_text_chunk(delta.text, context_id)
                            
                            async def on_thread_run(self, run):
                                """Capture final run status and any tool calls"""
                                self.final_status = run.status
                                # Capture tool calls if requires_action
                                status_str = str(run.status).lower() if run.status else ""
                                if "requires_action" in status_str and hasattr(run, 'required_action'):
                                    required_action = run.required_action
                                    if required_action and hasattr(required_action, 'submit_tool_outputs'):
                                        self.tool_calls = required_action.submit_tool_outputs.tool_calls or []
                        
                        # Use our custom handler
                        handler = ResponseCapturingHandler()
                        # Need to bind the emit method
                        handler._emit_text_chunk = self._emit_text_chunk
                        
                        result = await self.agents_client.runs.submit_tool_outputs_stream(
                            thread_id=thread_id,
                            run_id=run_id,
                            tool_outputs=tool_outputs,
                            event_handler=handler
                        )
                        log_foundry_debug(f"üîß With event_handler returned: {type(result)}")
                        
                        # Event handler pattern - wait for completion
                        await handler.until_done()
                        
                        # Extract captured response
                        full_text += handler.response_text
                        if handler.final_status:
                            status = handler.final_status
                        
                        # Capture tool calls for next iteration
                        if handler.tool_calls:
                            tool_calls_to_execute = handler.tool_calls
                            log_foundry_debug(f"üîß Event handler captured {len(tool_calls_to_execute)} tool calls for next iteration")
                        else:
                            tool_calls_to_execute = []
                        
                        log_foundry_debug(f"üîß Event handler completed - captured {len(handler.response_text)} chars, status: {handler.final_status}, tool_calls: {len(tool_calls_to_execute)}")
                        # Skip the streaming loop - already processed
                        stream = None
                    else:
                        raise
                
                # If we have a stream, process it
                if stream is not None:
                    # Continue streaming with tool outputs
                    # Reset tool_calls_to_execute to capture any new ones
                    tool_calls_to_execute = []
                    
                    async with stream as event_handler:
                        async for event in event_handler:
                            log_foundry_debug(f"Stream event (after tools): {event}")
                            
                            if hasattr(event, 'event') and hasattr(event, 'data'):
                                event_type = event.event
                                event_data = event.data
                            elif isinstance(event, tuple):
                                event_type, event_data, *_ = event
                            else:
                                event_data = event
                                event_type = type(event).__name__
                            
                            if isinstance(event_data, MessageDeltaChunk):
                                chunk = event_data.text
                                if chunk:
                                    full_text += chunk
                                    await self._emit_text_chunk(chunk, context_id)
                            
                            elif isinstance(event_data, ThreadRun):
                                run_id = event_data.id
                                status = event_data.status
                                log_foundry_debug(f"Run status (after tools): {status}")
                                
                                # Capture any new tool calls for the loop
                                # Use string comparison to handle RunStatus enum
                                status_str = str(status).lower() if status else ""
                                if "requires_action" in status_str and hasattr(event_data, 'required_action'):
                                    required_action = event_data.required_action
                                    if required_action and hasattr(required_action, 'submit_tool_outputs'):
                                        tool_calls_to_execute = required_action.submit_tool_outputs.tool_calls
                                        log_foundry_debug(f"üîß Captured {len(tool_calls_to_execute)} NEW tool calls for next iteration")
                            
                            elif event_type == AgentStreamEvent.DONE:
                                log_foundry_debug("Stream completed (after tools)")
                                break
                
                log_foundry_debug(f"Tool iteration {tool_iteration} complete - text length: {len(full_text)}, status: {status}")
            
            result = {
                "id": run_id,
                "text": full_text,
                "tool_calls": [],  # Tool calls handled manually above
                "status": status,
                "usage": None  # Could extract from run if needed
            }
            
            return result
            
        except Exception as e:
            log_error(f"Error in _create_response_with_streaming: {e}")
            import traceback
            traceback.print_exc()
            raise

    async def _execute_tool_calls_from_response(
        self,
        tool_calls: List[Dict[str, Any]],
        context_id: str,
        session_context: SessionContext,
        event_logger=None
    ) -> List[Dict[str, Any]]:
        """
        Execute tool calls from Responses API.
        
        This is a simpler version adapted for Responses API that reuses
        the tool execution logic from _handle_tool_calls.
        
        Args:
            tool_calls: List of tool calls from Responses API
            context_id: Conversation context ID
            session_context: Session state
            event_logger: Optional event logger
            
        Returns:
            List of tool outputs in format needed for next response
        """
        log_foundry_debug(f"_execute_tool_calls_from_response: Processing {len(tool_calls)} tool calls")
        
        tool_outputs = []
        
        # Separate send_message calls from other tools
        send_message_calls = []
        other_calls = []
        
        for tool_call in tool_calls:
            function_name = tool_call["function"]["name"]
            # Check for both send_message and send_message_sync (the actual function name)
            if function_name in ("send_message", "send_message_sync"):
                send_message_calls.append(tool_call)
            else:
                other_calls.append(tool_call)
        
        # Execute send_message calls in parallel (if not in agent mode)
        if send_message_calls and not session_context.agent_mode:
            log_foundry_debug(f"Executing {len(send_message_calls)} send_message calls in parallel")
            
            tasks = []
            for tool_call in send_message_calls:
                arguments = json.loads(tool_call["function"]["arguments"]) if isinstance(tool_call["function"]["arguments"], str) else tool_call["function"]["arguments"]
                
                # Create dummy tool context
                tool_context = type('obj', (object,), {'state': session_context})()
                
                task = self.send_message(
                    agent_name=arguments["agent_name"],
                    message=arguments["message"],
                    tool_context=tool_context,
                    suppress_streaming=True
                )
                tasks.append((tool_call, task))
            
            # Execute in parallel
            results = await asyncio.gather(*[task for _, task in tasks], return_exceptions=True)
            
            for (tool_call, _), result in zip(tasks, results):
                if isinstance(result, Exception):
                    output = {"error": str(result)}
                else:
                    output = result
                
                tool_outputs.append({
                    "type": "function_call_output",
                    "call_id": tool_call["id"],
                    "output": json.dumps(output)
                })
        else:
            # Sequential execution for agent mode
            for tool_call in send_message_calls:
                arguments = json.loads(tool_call["function"]["arguments"]) if isinstance(tool_call["function"]["arguments"], str) else tool_call["function"]["arguments"]
                
                tool_context = type('obj', (object,), {'state': session_context})()
                
                try:
                    output = await self.send_message(
                        agent_name=arguments["agent_name"],
                        message=arguments["message"],
                        tool_context=tool_context,
                        suppress_streaming=True
                    )
                except Exception as e:
                    output = {"error": str(e)}
                
                tool_outputs.append({
                    "type": "function_call_output",
                    "call_id": tool_call["id"],
                    "output": json.dumps(output)
                })
        
        # Execute other tool calls sequentially
        for tool_call in other_calls:
            function_name = tool_call["function"]["name"]
            arguments = json.loads(tool_call["function"]["arguments"]) if isinstance(tool_call["function"]["arguments"], str) else tool_call["function"]["arguments"]
            
            await self._emit_tool_call_event("foundry-host-agent", function_name, arguments, context_id)
            
            if function_name == "list_remote_agents":
                output = self.list_remote_agents()
            else:
                output = {"error": f"Unknown function: {function_name}"}
            
            tool_outputs.append({
                "type": "function_call_output",
                "call_id": tool_call["id"],
                "output": json.dumps(output)
            })
        
        log_foundry_debug(f"Tool execution complete - {len(tool_outputs)} outputs")
        return tool_outputs

    async def init_remote_agent_addresses(self, remote_agent_addresses: List[str]):
        async with asyncio.TaskGroup() as task_group:
            for address in remote_agent_addresses:
                task_group.create_task(self.retrieve_card(address))

    async def retrieve_card(self, address: str):
        card_resolver = A2ACardResolver(self.httpx_client, address, '/.well-known/agent.json')
        card = await card_resolver.get_agent_card()
        self.register_agent_card(card)

    def register_agent_card(self, card: AgentCard):
        """
        Register a remote agent by its card, establishing connection and updating UI state.
        Handles both new registrations and updates to existing agents.
        """
        if hasattr(card, 'capabilities') and card.capabilities:
            streaming_flag = getattr(card.capabilities, 'streaming', None)
            if streaming_flag is True:
                print(f"üîÑ [STREAMING] {card.name} supports streaming; enabling granular UI visibility")
            elif streaming_flag is False:
                print(f"‚ÑπÔ∏è [STREAMING] {card.name} does not support streaming; using non-streaming mode")
            else:
                print(f"‚ÑπÔ∏è [STREAMING] {card.name} did not specify streaming capability; defaulting to non-streaming mode")
                try:
                    card.capabilities.streaming = False
                except Exception:
                    pass
        
        self._update_agent_registry(card)
        
        print(f"üîó [CALLBACK] Registering {card.name} with callback: {self.task_callback.__name__ if hasattr(self.task_callback, '__name__') else type(self.task_callback)}")
        remote_connection = RemoteAgentConnections(self.httpx_client, card, self.task_callback)
        self.remote_agent_connections[card.name] = remote_connection
        self.cards[card.name] = card
        agent_info = []
        for ra in self.list_remote_agents():
            agent_info.append(json.dumps(ra))
        self.agents = '\n'.join(agent_info)
        
        if hasattr(self, '_host_manager') and self._host_manager:
            existing_index = next((i for i, a in enumerate(self._host_manager._agents) if a.name == card.name), None)
            
            if existing_index is not None:
                self._host_manager._agents[existing_index] = card
                log_debug(f"üîÑ Updated {card.name} in host manager agent list")
            else:
                self._host_manager._agents.append(card)
                log_debug(f"‚úÖ Added {card.name} to host manager agent list")
        
        self._emit_agent_registration_event(card)
        
        if self.agent:
            asyncio.create_task(self._update_agent_instructions())

    async def create_agent(self) -> Any:
        """
        Create an agent using Azure AI Foundry Agent Service.
        
        This creates a persistent agent that:
        - Appears in Azure AI Foundry portal
        - Has full Application Insights telemetry
        - Supports streaming responses
        - Has managed conversation state (threads)
        
        Returns:
            Agent object from Azure AI Foundry
        """
        if self.agent:
            log_foundry_debug(f"Agent already exists, reusing agent ID: {self.agent_id}")
            return self.agent
        
        log_foundry_debug(f"Creating new agent with Azure AI Foundry Agent Service...")
        log_foundry_debug(f"AZURE_AI_FOUNDRY_PROJECT_ENDPOINT = {os.environ.get('AZURE_AI_FOUNDRY_PROJECT_ENDPOINT', 'NOT SET')}")
        log_foundry_debug(f"AZURE_AI_AGENT_MODEL_DEPLOYMENT_NAME = {os.environ.get('AZURE_AI_AGENT_MODEL_DEPLOYMENT_NAME', 'NOT SET')}")
        
        # Validate required environment variables
        if not os.environ.get("AZURE_AI_FOUNDRY_PROJECT_ENDPOINT"):
            raise ValueError("AZURE_AI_FOUNDRY_PROJECT_ENDPOINT environment variable is required")
        if not os.environ.get("AZURE_AI_AGENT_MODEL_DEPLOYMENT_NAME"):
            raise ValueError("AZURE_AI_AGENT_MODEL_DEPLOYMENT_NAME environment variable is required")
        
        try:
            # Ensure project client is initialized
            await self._ensure_project_client()
            
            model_name = os.environ["AZURE_AI_AGENT_MODEL_DEPLOYMENT_NAME"]
            instructions = self.root_instruction('foundry-host-agent')
            
            log_foundry_debug(f"Agent parameters:")
            print(f"  - model: {model_name}")
            print(f"  - name: foundry-host-agent")
            print(f"  - instructions length: {len(instructions)}")
            
            # Initialize function tools
            toolset = await self._initialize_function_tools()
            
            log_foundry_debug(f"  - tools: initialized with AsyncFunctionTool")
            
            # Create agent using Foundry Agent Service SDK
            log_foundry_debug("Calling agents_client.create_agent()...")
            self.agent = await self.agents_client.create_agent(
                model=model_name,
                name="foundry-host-agent",
                instructions=instructions,
                toolset=toolset,
            )
            
            self.agent_id = self.agent.id
            log_foundry_debug(f"‚úÖ Agent created successfully! ID: {self.agent_id}")
            
            # Debug: Log what tools the agent has
            if hasattr(self.agent, 'tools'):
                print(f"üîß Agent tools: {self.agent.tools}")
            else:
                print(f"üîß Agent object attributes: {[a for a in dir(self.agent) if not a.startswith('_')]}")
            
            logger.info(f"Created Foundry Host agent: {self.agent_id}")
            print(f"üéâ Agent visible in Azure AI Foundry portal: {self.agent_id}")
            
            return self.agent
            
        except Exception as e:
            log_foundry_debug(f"‚ùå Exception in create_agent(): {type(e).__name__}: {e}")
            log_foundry_debug(f"‚ùå Full traceback:")
            import traceback
            traceback.print_exc()
            raise
    
    def list_remote_agents_sync(self):
        """
        Synchronous wrapper for list_remote_agents - for use with AsyncFunctionTool.
        
        The underlying list_remote_agents() method is synchronous, so this wrapper
        can also be synchronous. AsyncFunctionTool will handle it appropriately.
        """
        log_foundry_debug("üîß [TOOL] list_remote_agents_sync called by SDK!")
        result = self.list_remote_agents()
        log_foundry_debug(f"üîß [TOOL] list_remote_agents_sync returning: {result}")
        return result
    
    async def send_message_sync(self, agent_name: str, message: str):
        """
        Async wrapper for send_message - for use with AsyncFunctionTool.
        
        Azure AI Agents SDK's AsyncFunctionTool.execute() checks if the function
        is async (using inspect.iscoroutinefunction) and awaits it if needed.
        Since send_message is async, this wrapper must also be async.
        """
        print(f"\nüî•üî•üî• [SEND_MESSAGE_SYNC] CALLED by Azure SDK!")
        print(f"üî• agent_name: {agent_name}")
        print(f"üî• message: {message[:100]}...")
        
        # Use the current host context ID - NO FALLBACK to UUID!
        context_id_to_use = getattr(self, '_current_host_context_id', None)
        
        log_debug(f"üîç [send_message_sync] _current_host_context_id: {context_id_to_use}")
        log_debug(f"üîç [send_message_sync] session_contexts keys: {list(self.session_contexts.keys())}")
        
        # CRITICAL: If we don't have the current context_id, this is a bug
        if not context_id_to_use:
            raise ValueError(f"send_message_sync called but _current_host_context_id not set! This should be set by run_conversation_with_parts. Available keys: {list(self.session_contexts.keys())}")
        
        # Get existing session context or create new one with proper contextId
        session_ctx = self.session_contexts.get(context_id_to_use)
        if not session_ctx:
            log_debug(f"üîç [send_message_sync] SessionContext NOT FOUND, creating new one with contextId={context_id_to_use}")
            session_ctx = SessionContext(
                agent_mode=False,
                host_task=None,
                plan=None,
                contextId=context_id_to_use  # CRITICAL: Pass contextId to prevent UUID generation
            )
        else:
            log_debug(f"üîç [send_message_sync] SessionContext FOUND with contextId={session_ctx.contextId}")
        
        # Create a task context mock
        tool_context = type('obj', (object,), {
            'state': session_ctx
        })()
        
        # Call the async send_message - SDK will await it
        # NOTE: suppress_streaming=False allows status updates to flow to sidebar
        return await self.send_message(
            agent_name=agent_name,
            message=message,
            tool_context=tool_context,
            suppress_streaming=False  # Enable streaming for sidebar status updates!
        )

    def _format_agent_response_for_model(self, response_parts: list, agent_name: str) -> str:
        """
        Format the response parts from send_message into clean text for the model.
        
        The model expects readable text as tool output, not complex nested JSON.
        This extracts text content from response_parts and formats it cleanly.
        
        Args:
            response_parts: List of Part objects from send_message
            agent_name: Name of the agent that responded
            
        Returns:
            Clean text string suitable for tool output
        """
        import json
        
        if not response_parts:
            return json.dumps({
                "agent": agent_name,
                "status": "completed",
                "response": "Agent completed the task but returned no text content."
            })
        
        text_parts = []
        file_parts = []
        data_parts = []
        
        for part in response_parts:
            try:
                # Handle various part formats
                part_root = getattr(part, 'root', part)
                
                # Check for text content
                if hasattr(part_root, 'text') and part_root.text:
                    text_parts.append(part_root.text)
                elif hasattr(part, 'text') and part.text:
                    text_parts.append(part.text)
                elif isinstance(part, str):
                    text_parts.append(part)
                elif isinstance(part, dict):
                    if 'text' in part:
                        text_parts.append(part['text'])
                    elif 'content' in part:
                        text_parts.append(str(part['content']))
                    else:
                        # Data part
                        data_parts.append(part)
                
                # Check for file content
                if hasattr(part_root, 'file') or hasattr(part, 'file'):
                    file_obj = getattr(part_root, 'file', None) or getattr(part, 'file', None)
                    if file_obj:
                        file_name = getattr(file_obj, 'name', 'unknown_file')
                        file_parts.append(file_name)
                        
            except Exception as e:
                log_foundry_debug(f"Error processing part: {e}")
                # Try to convert to string as fallback
                try:
                    if hasattr(part, 'model_dump'):
                        text_parts.append(str(part.model_dump(mode='json')))
                    else:
                        text_parts.append(str(part))
                except:
                    pass
        
        # Build clean response
        response_text = "\n\n".join(text_parts) if text_parts else ""
        
        result = {
            "agent": agent_name,
            "status": "completed",
            "response": response_text if response_text else "Task completed successfully."
        }
        
        if file_parts:
            result["files_generated"] = file_parts
            
        if data_parts:
            result["additional_data"] = len(data_parts)
        
        return json.dumps(result, ensure_ascii=False)

    async def _update_agent_instructions(self):
        """Update the agent's instructions with the current agent list"""
        if not self.agent:
            print(f"‚ö†Ô∏è No agent exists to update")
            return
            
        try:
            print(f"üîÑ Updating agent instructions with {len(self.cards)} registered agents...")
            
            headers = await self._get_auth_headers()
            endpoint = os.environ["AZURE_AI_FOUNDRY_PROJECT_ENDPOINT"]
            agent_id = self.agent['id']
            api_url = f"{endpoint}/assistants/{agent_id}"
            
            # Get updated instructions with current agent list
            updated_instructions = self.root_instruction('foundry-host-agent')
            
            # Prepare the update payload
            payload = {
                "instructions": updated_instructions
            }
            
            # Make the HTTP request to update the assistant
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    api_url,
                    headers=headers,
                    json=payload,
                    params={"api-version": "2025-05-15-preview"},
                    timeout=30.0
                )
                
                if response.status_code == 200:
                    print(f"‚úÖ Agent instructions updated successfully!")
                    print(f"   Agent now knows about: {', '.join(self.cards.keys())}")
                else:
                    print(f"‚ùå Failed to update agent instructions: {response.status_code} - {response.text}")
                    
        except Exception as e:
            print(f"‚ùå Error updating agent instructions: {e}")
            # Don't fail the registration if instruction update fails
            pass

    def _get_tools(self) -> List[Dict[str, Any]]:
        """
        Define Azure AI Foundry function tools for agent coordination.
        
        NOTE: Responses API format is different from Chat Completions API!
        - Responses API: {"type": "function", "name": "...", "description": "...", "parameters": {...}}
        - Chat Completions: {"type": "function", "function": {"name": "...", ...}}
        """
        return [
            {
                "type": "function",
                "name": "list_remote_agents",
                "description": "List the available remote agents you can use to delegate the task.",
                "parameters": {"type": "object", "properties": {}},
            },
            {
                "type": "function",
                "name": "send_message",
                "description": "Send a message to a remote agent.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "agent_name": {"type": "string", "description": "The name of the agent to send the task to."},
                        "message": {"type": "string", "description": "The message to send to the agent."},
                    },
                    "required": ["agent_name", "message"],
                },
            },
        ]

    def root_instruction(self, current_agent: str, agent_mode: bool = False) -> str:
        """
        Generate system prompt for the host agent based on operational mode.
        Supports custom instruction overrides and agent-mode vs standard orchestration prompts.
        """
        if self.custom_root_instruction:
            return apply_custom_instruction(
                self.custom_root_instruction, 
                self.agents, 
                current_agent
            )

        if agent_mode:
            return build_agent_mode_instruction(self.agents, current_agent)

        return build_orchestrator_instruction(self.agents, current_agent)

    def list_remote_agents(self):
        """
        List available remote agents for the current session.
        Note: self.cards is already session-specific, set via set_session_agents() before each request.
        """
        agents = []
        for card in self.cards.values():
            agent_info = {
                'name': card.name,
                'description': card.description
            }
            
            # Add skills if present
            if hasattr(card, 'skills') and card.skills:
                skills_list = []
                for skill in card.skills:
                    skill_dict = {
                        "id": getattr(skill, 'id', ''),
                        "name": getattr(skill, 'name', ''),
                        "description": getattr(skill, 'description', ''),
                    }
                    skills_list.append(skill_dict)
                agent_info['skills'] = skills_list
            
            agents.append(agent_info)
        return agents

    async def _call_azure_openai_structured(
        self,
        system_prompt: str,
        user_prompt: str,
        response_model: type[BaseModel],
        context_id: str
    ) -> BaseModel:
        """
        Make a structured output request to Azure OpenAI for agent-mode planning.
        Uses Pydantic models to enforce response schema validation.
        """
        try:
            print(f"ü§ñ [Agent Mode] Calling Azure OpenAI for structured output...")
            await self._emit_status_event("Planning next task with AI...", context_id)
            
            # Extract base endpoint from AI Foundry project endpoint
            endpoint = os.environ["AZURE_AI_FOUNDRY_PROJECT_ENDPOINT"]
            model_name = os.environ["AZURE_AI_AGENT_MODEL_DEPLOYMENT_NAME"]
            
            # Extract base endpoint from project endpoint
            base_endpoint = endpoint.split('/api/projects')[0] if '/api/projects' in endpoint else endpoint
            print(f"ü§ñ [Agent Mode] Azure endpoint: {base_endpoint}")
            print(f"ü§ñ [Agent Mode] Model deployment: {model_name}")
            
            # Get Azure credential token
            from azure.identity import DefaultAzureCredential, get_bearer_token_provider
            credential = DefaultAzureCredential()
            token_provider = get_bearer_token_provider(credential, "https://cognitiveservices.azure.com/.default")
            
            # Create Azure OpenAI client with token auth
            client = AsyncAzureOpenAI(
                azure_endpoint=base_endpoint,
                azure_ad_token_provider=token_provider,
                api_version="2024-08-01-preview"  # Version that supports structured outputs
            )
            
            print(f"ü§ñ [Agent Mode] Making structured output request with OpenAI SDK...")
            
            # Use OpenAI SDK's parse method for structured outputs
            completion = await client.beta.chat.completions.parse(
                model=model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                response_format=response_model,
                temperature=0.7,
                max_tokens=2000
            )
            
            parsed = completion.choices[0].message.parsed
            print(f"ü§ñ [Agent Mode] Got structured response: {parsed.model_dump_json()[:200]}...")
            
            # Extract token usage from orchestration call
            if hasattr(completion, 'usage') and completion.usage:
                self.host_token_usage["prompt_tokens"] += completion.usage.prompt_tokens or 0
                self.host_token_usage["completion_tokens"] += completion.usage.completion_tokens or 0
                self.host_token_usage["total_tokens"] += completion.usage.total_tokens or 0
                print(f"üéüÔ∏è [Host Agent] Orchestration tokens: +{completion.usage.total_tokens} (total: {self.host_token_usage['total_tokens']})")
            
            return parsed
                    
        except Exception as e:
            log_error(f"[Agent Mode] Error calling Azure OpenAI: {e}")
            import traceback
            traceback.print_exc()
            raise

    async def _call_azure_openai_raw(
        self,
        system_prompt: str,
        user_prompt: str,
        context_id: str
    ) -> str:
        """
        Make a simple text completion request to Azure OpenAI.
        Used for lightweight tasks like agent selection.
        """
        try:
            endpoint = os.environ["AZURE_AI_FOUNDRY_PROJECT_ENDPOINT"]
            model_name = os.environ["AZURE_AI_AGENT_MODEL_DEPLOYMENT_NAME"]
            base_endpoint = endpoint.split('/api/projects')[0] if '/api/projects' in endpoint else endpoint
            
            from azure.identity import DefaultAzureCredential, get_bearer_token_provider
            credential = DefaultAzureCredential()
            token_provider = get_bearer_token_provider(credential, "https://cognitiveservices.azure.com/.default")
            
            client = AsyncAzureOpenAI(
                azure_endpoint=base_endpoint,
                azure_ad_token_provider=token_provider,
                api_version="2024-08-01-preview"
            )
            
            completion = await client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3,
                max_tokens=100
            )
            
            return completion.choices[0].message.content or ""
            
        except Exception as e:
            log_error(f"[Azure OpenAI Raw] Error: {e}")
            raise

    async def _select_agent_for_task(
        self,
        task_description: str,
        available_agents: List[Dict[str, str]],
        context_id: str
    ) -> Optional[str]:
        """
        Use LLM to select the best agent for a task description.
        
        This is a lightweight call just for agent selection when the workflow
        doesn't explicitly specify which agent to use.
        """
        try:
            system_prompt = """You are an agent selector. Given a task description and available agents, 
select the most appropriate agent. Return ONLY the agent name, nothing else."""
            
            user_prompt = f"""Task: {task_description}

Available Agents:
{json.dumps(available_agents, indent=2)}

Return the name of the best agent for this task (exact match from the list above):"""
            
            response = await self._call_azure_openai_raw(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                context_id=context_id
            )
            
            agent_name = response.strip().strip('"').strip("'")
            
            # Verify it's a valid agent
            for agent in available_agents:
                if agent["name"].lower() == agent_name.lower():
                    return agent["name"]
            
            # Try partial match
            for agent in available_agents:
                if agent_name.lower() in agent["name"].lower() or agent["name"].lower() in agent_name.lower():
                    return agent["name"]
            
            return None
            
        except Exception as e:
            log_error(f"[Agent Selection] Error: {e}")
            return None

    async def _execute_parsed_workflow(
        self,
        parsed_workflow: ParsedWorkflow,
        user_message: str,
        context_id: str,
        session_context: SessionContext
    ) -> List[str]:
        """
        Execute a pre-parsed workflow with support for parallel step groups.
        
        This method uses the Pydantic AgentModePlan for proper state management,
        retry logic, HITL support, and artifact tracking - just like the dynamic
        orchestration loop, but with a pre-defined workflow structure.
        
        Parallel groups (e.g., steps 2a, 2b) are executed concurrently using 
        asyncio.gather() while still creating proper AgentModeTask objects for
        state persistence.
        
        Args:
            parsed_workflow: The parsed workflow with sequential and parallel groups
            user_message: Original user message for context
            context_id: Conversation identifier
            session_context: Session state
            
        Returns:
            List of output strings from all executed steps
        """
        log_info(f"üöÄ [Workflow] Executing parsed workflow with {len(parsed_workflow.groups)} groups")
        print(f"üìã [Workflow] Parsed structure:\n{parsed_workflow}")
        
        # Use the class method for extracting clean text from A2A response objects
        extract_text_from_response = self._extract_text_from_response
        
        # Initialize Pydantic plan for state tracking (just like dynamic orchestration)
        plan = AgentModePlan(goal=user_message, goal_status="incomplete")
        all_task_outputs = []
        
        # Log initial plan
        print(f"\n{'='*80}")
        log_debug(f"üìã [Parsed Workflow] INITIAL PLAN")
        print(f"{'='*80}")
        print(f"Goal: {plan.goal}")
        print(f"Groups: {len(parsed_workflow.groups)}")
        print(f"{'='*80}\n")
        
        for group_idx, group in enumerate(parsed_workflow.groups):
            group_type = "PARALLEL" if group.group_type == WorkflowStepType.PARALLEL else "SEQUENTIAL"
            log_info(f"üì¶ [Workflow] Executing group {group.group_number} ({group_type}, {len(group.steps)} steps)")
            
            if group.group_type == WorkflowStepType.PARALLEL:
                # ============================================================
                # PARALLEL EXECUTION with proper state tracking
                # ============================================================
                await self._emit_status_event(
                    f"Executing parallel group {group.group_number} ({len(group.steps)} agents simultaneously)...",
                    context_id
                )
                
                # Create AgentModeTask objects for each parallel step
                parallel_tasks: List[AgentModeTask] = []
                for step in group.steps:
                    task = AgentModeTask(
                        task_id=str(uuid.uuid4()),
                        task_description=f"[Step {step.step_label}] {step.description}",
                        recommended_agent=None,  # Will be resolved during execution
                        state="pending"
                    )
                    plan.tasks.append(task)
                    parallel_tasks.append(task)
                
                # Execute all steps in parallel
                async def execute_parallel_step(step: ParsedWorkflowStep, task: AgentModeTask):
                    """Execute a single step and update its task state."""
                    task.state = "running"
                    task.updated_at = datetime.now(timezone.utc)
                    
                    try:
                        result = await self._execute_workflow_step_with_state(
                            step=step,
                            task=task,
                            session_context=session_context,
                            context_id=context_id,
                            user_message=user_message,
                            extract_text_fn=extract_text_from_response
                        )
                        return result
                    except Exception as e:
                        task.state = "failed"
                        task.error_message = str(e)
                        task.updated_at = datetime.now(timezone.utc)
                        log_error(f"[Workflow] Parallel step {step.step_label} failed: {e}")
                        return {"error": str(e), "step_label": step.step_label}
                
                # Run in parallel
                results = await asyncio.gather(
                    *[execute_parallel_step(step, task) for step, task in zip(group.steps, parallel_tasks)],
                    return_exceptions=True
                )
                
                # Collect results and check for HITL pause
                for i, result in enumerate(results):
                    step = group.steps[i]
                    task = parallel_tasks[i]
                    
                    if isinstance(result, Exception):
                        log_error(f"[Workflow] Parallel step {step.step_label} exception: {result}")
                        all_task_outputs.append(f"[Step {step.step_label} Error]: {str(result)}")
                    elif isinstance(result, dict):
                        # Check for HITL pause
                        if result.get("hitl_pause"):
                            log_info(f"‚è∏Ô∏è [Workflow] HITL pause triggered by step {step.step_label}")
                            # Store workflow state for resumption
                            session_context.pending_workflow = str(parsed_workflow)
                            session_context.pending_workflow_outputs = all_task_outputs.copy()
                            session_context.pending_workflow_user_message = user_message
                            return all_task_outputs
                        
                        if result.get("output"):
                            all_task_outputs.append(f"[Step {step.step_label} - {result.get('agent', 'unknown')}]: {result['output']}")
                        elif result.get("error"):
                            all_task_outputs.append(f"[Step {step.step_label} Error]: {result['error']}")
                
                log_info(f"‚úÖ [Workflow] Parallel group {group.group_number} completed")
                
            else:
                # ============================================================
                # SEQUENTIAL EXECUTION with proper state tracking
                # ============================================================
                step = group.steps[0]
                
                # Create AgentModeTask for this step
                task = AgentModeTask(
                    task_id=str(uuid.uuid4()),
                    task_description=f"[Step {step.step_label}] {step.description}",
                    recommended_agent=None,
                    state="running"
                )
                plan.tasks.append(task)
                
                await self._emit_status_event(f"Executing step {step.step_label}: {step.description[:50]}...", context_id)
                
                try:
                    result = await self._execute_workflow_step_with_state(
                        step=step,
                        task=task,
                        session_context=session_context,
                        context_id=context_id,
                        user_message=user_message,
                        extract_text_fn=extract_text_from_response
                    )
                    
                    # Check for HITL pause
                    if result.get("hitl_pause"):
                        log_info(f"‚è∏Ô∏è [Workflow] HITL pause triggered by step {step.step_label}")
                        session_context.pending_workflow = str(parsed_workflow)
                        session_context.pending_workflow_outputs = all_task_outputs.copy()
                        session_context.pending_workflow_user_message = user_message
                        return all_task_outputs
                    
                    if result.get("output"):
                        all_task_outputs.append(f"[Step {step.step_label} - {result.get('agent', 'unknown')}]: {result['output']}")
                    elif result.get("error"):
                        all_task_outputs.append(f"[Step {step.step_label} Error]: {result['error']}")
                    
                except Exception as e:
                    task.state = "failed"
                    task.error_message = str(e)
                    task.updated_at = datetime.now(timezone.utc)
                    log_error(f"[Workflow] Sequential step {step.step_label} failed: {e}")
                    all_task_outputs.append(f"[Step {step.step_label} Error]: {str(e)}")
                
                log_info(f"‚úÖ [Workflow] Sequential step {step.step_label} completed")
        
        # Mark plan as completed
        plan.goal_status = "completed"
        plan.updated_at = datetime.now(timezone.utc)
        
        # Log final plan summary
        print(f"\n{'='*80}")
        print(f"üé¨ [Parsed Workflow] FINAL PLAN SUMMARY")
        print(f"{'='*80}")
        print(f"Goal: {plan.goal}")
        print(f"Final Status: {plan.goal_status}")
        print(f"Total Tasks Created: {len(plan.tasks)}")
        print(f"\nTask Breakdown:")
        for i, task in enumerate(plan.tasks, 1):
            print(f"  {i}. [{task.state.upper()}] {task.task_description[:60]}...")
            print(f"     Agent: {task.recommended_agent or 'None'}")
            if task.error_message:
                print(f"     Error: {task.error_message}")
        print(f"\nTask Outputs Collected: {len(all_task_outputs)}")
        print(f"{'='*80}\n")
        
        log_info(f"üéâ [Workflow] All {len(parsed_workflow.groups)} groups completed, collected {len(all_task_outputs)} outputs")
        return all_task_outputs
    
    async def _execute_workflow_step_with_state(
        self,
        step: ParsedWorkflowStep,
        task: AgentModeTask,
        session_context: SessionContext,
        context_id: str,
        user_message: str,
        extract_text_fn
    ) -> Dict[str, Any]:
        """
        Execute a single workflow step with full state tracking via AgentModeTask.
        
        This mirrors the task execution logic from the dynamic orchestration loop,
        including HITL detection, artifact collection, and proper error handling.
        
        Args:
            step: The parsed workflow step
            task: The AgentModeTask to update with execution state
            session_context: Session state
            context_id: Conversation identifier
            user_message: Original user message
            extract_text_fn: Function to extract text from responses
            
        Returns:
            Dict with execution result, agent info, and flags (e.g., hitl_pause)
        """
        log_debug(f"üöÄ [Workflow] Executing step {step.step_label}: {step.description}")
        
        # Resolve agent (by hint or LLM selection)
        agent_name = None
        
        if step.agent_hint:
            # Try exact match first
            for card_name in self.cards.keys():
                if step.agent_hint.lower() in card_name.lower():
                    agent_name = card_name
                    break
        
        if not agent_name:
            # Fall back to LLM agent selection
            available_agents = [{"name": card.name, "description": card.description} for card in self.cards.values()]
            agent_name = await self._select_agent_for_task(step.description, available_agents, context_id)
        
        if not agent_name or agent_name not in self.cards:
            task.state = "failed"
            task.error_message = "No suitable agent found"
            task.updated_at = datetime.now(timezone.utc)
            log_error(f"[Workflow] No suitable agent found for step {step.step_label}")
            return {
                "step_label": step.step_label,
                "agent": None,
                "state": "failed",
                "error": "No suitable agent found",
                "output": None
            }
        
        # Update task with resolved agent
        task.recommended_agent = agent_name
        task.updated_at = datetime.now(timezone.utc)
        
        log_debug(f"ÔøΩ [Workflow] Selected agent: {agent_name}")
        await self._emit_granular_agent_event(
            agent_name=agent_name,
            status_text=f"Starting: {step.description[:50]}...",
            context_id=context_id
        )
        
        # File deduplication for multi-step workflows (same as dynamic orchestration)
        if hasattr(session_context, '_latest_processed_parts') and len(session_context._latest_processed_parts) > 1:
            from collections import defaultdict
            old_count = len(session_context._latest_processed_parts)
            
            MAX_GENERATED_FILES = 3
            editing_roles = defaultdict(lambda: None)
            generated_artifacts = []
            
            for part in reversed(session_context._latest_processed_parts):
                role = None
                if isinstance(part, DataPart) and isinstance(part.data, dict):
                    role = part.data.get('role')
                elif hasattr(part, 'root') and isinstance(part.root, DataPart) and isinstance(part.root.data, dict):
                    role = part.root.data.get('role')
                
                if role in ['base', 'mask', 'overlay']:
                    if role not in editing_roles:
                        editing_roles[role] = part
                else:
                    if len(generated_artifacts) < MAX_GENERATED_FILES:
                        generated_artifacts.append(part)
            
            deduplicated_parts = list(editing_roles.values()) + generated_artifacts
            session_context._latest_processed_parts = deduplicated_parts
            print(f"üìé [Workflow] File management: {old_count} files ‚Üí {len(deduplicated_parts)} files")
        
        try:
            # Create task message with context
            task_message = f"{step.description}\n\nContext: {user_message}"
            
            # Create dummy tool context
            dummy_context = DummyToolContext(session_context, self._azure_blob_client)
            
            # Call the agent
            responses = await self.send_message(
                agent_name=agent_name,
                message=task_message,
                tool_context=dummy_context,
                suppress_streaming=False
            )
            
            if responses and len(responses) > 0:
                response_obj = responses[0] if isinstance(responses, list) else responses
                
                # Check for HITL (input_required)
                if session_context.pending_input_agent:
                    log_info(f"‚è∏Ô∏è [Workflow] Agent '{agent_name}' returned input_required")
                    task.state = "input_required"
                    task.updated_at = datetime.now(timezone.utc)
                    
                    output_text = extract_text_fn(response_obj)
                    return {
                        "step_label": step.step_label,
                        "agent": agent_name,
                        "state": "input_required",
                        "output": output_text,
                        "hitl_pause": True
                    }
                
                # Handle A2A Task response with proper state extraction
                if isinstance(response_obj, Task):
                    task.state = response_obj.status.state
                    task.output = {
                        "task_id": response_obj.id,
                        "state": response_obj.status.state,
                        "result": response_obj.result if hasattr(response_obj, 'result') else None,
                        "artifacts": [a.model_dump() for a in response_obj.artifacts] if response_obj.artifacts else []
                    }
                    task.updated_at = datetime.now(timezone.utc)
                    
                    if task.state == "failed":
                        task.error_message = response_obj.status.message or "Task failed"
                        log_error(f"[Workflow] Task failed: {task.error_message}")
                        return {
                            "step_label": step.step_label,
                            "agent": agent_name,
                            "state": "failed",
                            "error": task.error_message,
                            "output": None
                        }
                    
                    # Collect artifacts
                    output_text = str(response_obj.result) if response_obj.result else ""
                    if response_obj.artifacts:
                        artifact_descriptions = []
                        for artifact in response_obj.artifacts:
                            if hasattr(artifact, 'parts'):
                                for part in artifact.parts:
                                    if not hasattr(session_context, '_latest_processed_parts'):
                                        session_context._latest_processed_parts = []
                                    session_context._latest_processed_parts.append(part)
                                    
                                    if hasattr(part, 'root'):
                                        if hasattr(part.root, 'file'):
                                            file_info = part.root.file
                                            file_name = getattr(file_info, 'name', 'unknown')
                                            artifact_descriptions.append(f"[File: {file_name}]")
                                        elif hasattr(part.root, 'text'):
                                            artifact_descriptions.append(part.root.text)
                        
                        if artifact_descriptions:
                            output_text = f"{output_text}\n\nArtifacts:\n" + "\n".join(artifact_descriptions)
                    
                    log_info(f"‚úÖ [Workflow] Step {step.step_label} completed by {agent_name}")
                    return {
                        "step_label": step.step_label,
                        "agent": agent_name,
                        "state": "completed",
                        "error": None,
                        "output": output_text
                    }
                else:
                    # Simple string response
                    task.state = "completed"
                    output_text = extract_text_fn(response_obj)
                    task.output = {"result": output_text}
                    task.updated_at = datetime.now(timezone.utc)
                    
                    log_info(f"‚úÖ [Workflow] Step {step.step_label} completed by {agent_name}")
                    return {
                        "step_label": step.step_label,
                        "agent": agent_name,
                        "state": "completed",
                        "error": None,
                        "output": output_text
                    }
            else:
                task.state = "failed"
                task.error_message = "No response from agent"
                task.updated_at = datetime.now(timezone.utc)
                return {
                    "step_label": step.step_label,
                    "agent": agent_name,
                    "state": "failed",
                    "error": "No response from agent",
                    "output": None
                }
                
        except Exception as e:
            task.state = "failed"
            task.error_message = str(e)
            task.updated_at = datetime.now(timezone.utc)
            log_error(f"[Workflow] Error executing step {step.step_label}: {e}")
            return {
                "step_label": step.step_label,
                "agent": agent_name,
                "state": "failed",
                "error": str(e),
                "output": None
            }

    async def _execute_orchestrated_task(
        self,
        task: AgentModeTask,
        session_context: SessionContext,
        context_id: str,
        workflow: Optional[str],
        user_message: str,
        extract_text_fn,
        previous_task_outputs: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        Execute a single orchestrated task with full state management.
        
        This method handles:
        - File deduplication for multi-step workflows
        - Agent calling via send_message
        - HITL (input_required) detection
        - Response parsing (A2A Task or legacy format)
        - Artifact collection
        - State updates on the AgentModeTask
        
        Args:
            task: The AgentModeTask to execute
            session_context: Session state
            context_id: Conversation identifier  
            workflow: Optional workflow definition
            user_message: Original user message
            extract_text_fn: Function to extract text from responses
            
        Returns:
            Dict with output, hitl_pause flag, and error info
        """
        recommended_agent = task.recommended_agent
        task_desc = task.task_description
        
        log_debug(f"üöÄ [Agent Mode] Executing task: {task_desc[:50]}...")
        
        # Stream task creation event
        await self._emit_granular_agent_event(
            agent_name=recommended_agent or "orchestrator",
            status_text=f"Executing: {task_desc[:50]}...",
            context_id=context_id
        )
        
        if not recommended_agent or recommended_agent not in self.cards:
            task.state = "failed"
            task.error_message = f"Agent '{recommended_agent}' not found"
            task.updated_at = datetime.now(timezone.utc)
            log_error(f"[Agent Mode] Agent not found: {recommended_agent}")
            return {"error": task.error_message, "output": None}
        
        log_debug(f"üéØ [Agent Mode] Calling agent: {recommended_agent}")
        
        # Build enhanced task message with previous task output for sequential context
        # This enables agents to build upon previous work in the workflow
        enhanced_task_message = task_desc
        
        # For sequential workflows: Include ONLY the immediately previous task output as context
        # This allows step N to access the output from step N-1 without context window explosion
        if previous_task_outputs and len(previous_task_outputs) > 0:
            print(f"üìã [Agent Mode] Including previous task output as context (limited to last step only)")
            # Truncate to prevent context overflow (keep first 1000 chars)
            prev_output = previous_task_outputs[0]
            if len(prev_output) > 1000:
                prev_output = prev_output[:1000] + "... [truncated for context window management]"
            
            enhanced_task_message = f"""{task_desc}

## Context from Previous Step:
{prev_output}

Use the above output from the previous workflow step to complete your task."""
        
        # File deduplication for multi-step workflows
        if hasattr(session_context, '_latest_processed_parts') and len(session_context._latest_processed_parts) > 1:
            from collections import defaultdict
            old_count = len(session_context._latest_processed_parts)
            
            MAX_GENERATED_FILES = 3
            editing_roles = defaultdict(lambda: None)
            generated_artifacts = []
            
            for part in reversed(session_context._latest_processed_parts):
                role = None
                if isinstance(part, DataPart) and isinstance(part.data, dict):
                    role = part.data.get('role')
                elif hasattr(part, 'root') and isinstance(part.root, DataPart) and isinstance(part.root.data, dict):
                    role = part.root.data.get('role')
                
                if role in ['base', 'mask', 'overlay']:
                    if role not in editing_roles:
                        editing_roles[role] = part
                else:
                    if len(generated_artifacts) < MAX_GENERATED_FILES:
                        generated_artifacts.append(part)
            
            deduplicated_parts = list(editing_roles.values()) + generated_artifacts
            session_context._latest_processed_parts = deduplicated_parts
            print(f"üìé [Agent Mode] File management: {old_count} files ‚Üí {len(deduplicated_parts)} files")
        
        # Create tool context and call agent
        dummy_context = DummyToolContext(session_context, self._azure_blob_client)
        
        responses = await self.send_message(
            agent_name=recommended_agent,
            message=enhanced_task_message,  # ‚úÖ Now includes previous task outputs!
            tool_context=dummy_context,
            suppress_streaming=True  # Suppress agent's internal streaming to avoid duplicates in workflow mode
        )
        
        if not responses or len(responses) == 0:
            task.state = "failed"
            task.error_message = "No response from agent"
            task.updated_at = datetime.now(timezone.utc)
            log_error(f"[Agent Mode] No response from agent")
            return {"error": "No response from agent", "output": None}
        
        response_obj = responses[0] if isinstance(responses, list) else responses
        
        # Check for HITL (input_required)
        if session_context.pending_input_agent:
            log_info(f"‚è∏Ô∏è [Agent Mode] Agent '{recommended_agent}' returned input_required")
            task.state = "input_required"
            task.updated_at = datetime.now(timezone.utc)
            
            output_text = extract_text_fn(response_obj)
            log_info(f"‚è∏Ô∏è [Agent Mode] Waiting for user response to '{recommended_agent}'")
            await self._emit_status_event(f"Waiting for your response...", context_id)
            
            return {"output": output_text, "hitl_pause": True}
        
        # Parse response
        if isinstance(response_obj, Task):
            task.state = response_obj.status.state
            task.output = {
                "task_id": response_obj.id,
                "state": response_obj.status.state,
                "result": response_obj.result if hasattr(response_obj, 'result') else None,
                "artifacts": [a.model_dump() for a in response_obj.artifacts] if response_obj.artifacts else []
            }
            task.updated_at = datetime.now(timezone.utc)
            
            if task.state == "failed":
                task.error_message = response_obj.status.message or "Task failed"
                log_error(f"[Agent Mode] Task failed: {task.error_message}")
                return {"error": task.error_message, "output": None}
            
            log_info(f"‚úÖ [Agent Mode] Task completed with state: {task.state}")
            output_text = str(response_obj.result) if response_obj.result else ""
            
            # Collect artifacts
            if response_obj.artifacts:
                artifact_descriptions = []
                for artifact in response_obj.artifacts:
                    if hasattr(artifact, 'parts'):
                        for part in artifact.parts:
                            if not hasattr(session_context, '_latest_processed_parts'):
                                session_context._latest_processed_parts = []
                            session_context._latest_processed_parts.append(part)
                            
                            if hasattr(part, 'root'):
                                if hasattr(part.root, 'file'):
                                    file_info = part.root.file
                                    file_name = getattr(file_info, 'name', 'unknown')
                                    artifact_descriptions.append(f"[File: {file_name}]")
                                elif hasattr(part.root, 'text'):
                                    artifact_descriptions.append(part.root.text)
                
                if artifact_descriptions:
                    output_text = f"{output_text}\n\nArtifacts:\n" + "\n".join(artifact_descriptions)
            
            return {"output": output_text, "hitl_pause": False}
        else:
            # Simple string response (legacy format)
            task.state = "completed"
            output_text = extract_text_fn(response_obj)
            task.output = {"result": output_text}
            task.updated_at = datetime.now(timezone.utc)
            log_info(f"‚úÖ [Agent Mode] Task completed successfully")
            return {"output": output_text, "hitl_pause": False}

    async def _agent_mode_orchestration_loop(
        self,
        user_message: str,
        context_id: str,
        session_context: SessionContext,
        event_logger=None,
        workflow: Optional[str] = None
    ) -> List[str]:
        """
        Execute agent-mode orchestration: AI-driven task decomposition and multi-agent coordination.
        
        This is the core intelligence loop that:
        1. Uses Azure OpenAI to analyze the user's goal and available agents
        2. Breaks down complex requests into discrete, delegable tasks
        3. Selects the best agent for each task based on skills and capabilities
        4. Executes tasks sequentially or in parallel as appropriate
        5. Synthesizes results from multiple agents into coherent responses
        6. Adapts to failures, rate limits, and user feedback dynamically
        
        The loop continues until the goal is marked "completed" by the orchestrator LLM
        or the maximum iteration limit is reached (safety mechanism).
        
        Args:
            user_message: The user's original request or follow-up message
            context_id: Conversation identifier for state management
            session_context: Session state with agent task tracking
            event_logger: Optional callback for logging orchestration events
            workflow: Optional predefined workflow steps to enforce
            
        Returns:
            List of response strings from executed tasks for final synthesis
        """
        log_debug(f"üéØ [Agent Mode] Starting orchestration loop for goal: {user_message[:100]}...")
        log_debug(f"üìã [Agent Mode] Workflow parameter received: {workflow[:100] if workflow else 'None'}")
        
        # Reset host token usage for this workflow
        self.host_token_usage = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
        
        await self._emit_status_event("Initializing orchestration...", context_id)
        
        # =====================================================================
        # LLM ORCHESTRATION PATH: All workflows go through the orchestrator
        # =====================================================================
        # The LLM orchestrator handles both sequential and parallel workflows.
        # For parallel steps (e.g., 2a., 2b.), the LLM will return next_tasks
        # with parallel=True, and we execute them via asyncio.gather().
        # =====================================================================
        # orchestrator LLM decides which agents to call and in what order.
        # =====================================================================
        
        # Handle conversation continuity - distinguish new goals from follow-up clarifications
        # FIXED: Don't treat workflow iterations as follow-ups - they should continue in one loop
        if context_id in self._active_conversations and not workflow:
            # User is providing additional information for an existing goal (only for non-workflow mode)
            original_goal = self._active_conversations[context_id]
            goal_text = f"{original_goal}\n\n[Additional Information Provided]: {user_message}"
            print(f"üîÑ [Agent Mode] Follow-up detected - appending to original goal")
        else:
            # Fresh conversation OR workflow mode - establish goal
            goal_text = user_message
            if context_id not in self._active_conversations:
                self._active_conversations[context_id] = user_message
                print(f"üÜï [Agent Mode] New conversation started")
            else:
                print(f"üîÑ [Agent Mode] Continuing workflow - NOT treating as follow-up")
        
        # Use the class method for extracting clean text from A2A response objects
        extract_text_from_response = self._extract_text_from_response
        
        # Initialize execution plan with empty task list
        plan = AgentModePlan(goal=goal_text, goal_status="incomplete")
        iteration = 0
        max_iterations = 20  # Safety limit to prevent infinite loops
        workflow_step_count = 0  # Will be set if workflow is provided
        
        # Accumulate outputs from all completed tasks
        all_task_outputs = []
        
        # Log initial plan
        print(f"\n{'='*80}")
        log_debug(f"üìã [Agent Mode] INITIAL PLAN")
        print(f"{'='*80}")
        print(f"Goal: {plan.goal}")
        print(f"Status: {plan.goal_status}")
        print(f"Tasks: {len(plan.tasks)} (empty initially)")
        print(f"{'='*80}\n")
        
        # System prompt that guides the orchestrator's decision-making
        # This is the "brain" that decides which agents to use and when
        system_prompt = """You are the Host Orchestrator in an A2A multi-agent system.

PRIMARY RESPONSIBILITIES:
- **FIRST**: Check if a MANDATORY WORKFLOW exists below - if it does, you MUST complete ALL workflow steps before marking goal as "completed"
- Evaluate whether the user's goal is achieved by analyzing all completed tasks and their outputs
- If incomplete, propose the next task(s) that move closer to the goal
- Select the most appropriate agent based on their specialized skills

DECISION-MAKING RULES:
- Analyze the ENTIRE plan history - don't ignore previous tasks or outputs
- Never repeat completed tasks unless explicitly retrying a failure
- Keep each task atomic and delegable to a single agent
- Match tasks to agents using their "skills" field for best results
- If no agent fits, set recommended_agent=null
- Mark goal_status="completed" ONLY when: (1) ALL MANDATORY WORKFLOW steps are completed (if workflow exists), AND (2) the objective is fully achieved

### üîÄ PARALLEL EXECUTION SUPPORT
When the workflow contains parallel steps (indicated by letter suffixes like 2a., 2b., 2c.):
- These steps can be executed SIMULTANEOUSLY - they do not depend on each other
- Use `next_tasks` (list) instead of `next_task` (single) to propose multiple parallel tasks
- Set `parallel=true` to indicate these tasks should run concurrently
- Example: For steps "2a. Legal review" and "2b. Technical assessment":
  ```json
  {
    "goal_status": "incomplete",
    "next_task": null,
    "next_tasks": [
      {"task_description": "Legal review of requirements", "recommended_agent": "Legal Agent"},
      {"task_description": "Technical assessment", "recommended_agent": "Tech Agent"}
    ],
    "parallel": true,
    "reasoning": "Steps 2a and 2b can run in parallel as they are independent"
  }
  ```
- After parallel tasks complete, proceed to the next sequential step (e.g., step 3)
- If NO parallel steps, use `next_task` (single) and set `parallel=false`

MULTI-AGENT STRATEGY:
- **MAXIMIZE AGENT UTILIZATION**: Break complex goals into specialized subtasks
- Use multiple agents when their combined expertise adds value
- Don't force one agent to handle everything when others can help
- The same agent can be used multiple times for related subtasks

FAILURE HANDLING:
- Consider failed tasks in planning
- You can retry with modifications or try alternative agents/approaches

### üîÑ TASK DECOMPOSITION PRINCIPLES
- **Read ALL Agent Skills First**: Before creating any task, carefully read through the skill descriptions of ALL available agents to understand what each can provide.
- **Identify Skill Dependencies**: Determine if completing the goal requires outputs from multiple agents. If Agent B needs information/context that Agent A specializes in, Agent A must be tasked first.
- **Match Task to Skill Domain**: Each task should align with exactly ONE agent's skill domain. If a concept in the goal matches words in an agent's skill name or description, that agent should handle that aspect.
- **Information Producers vs Consumers**: Some agents produce information/context/specifications (e.g., skills about "guidelines", "direction", "specifications"). Others consume that information to execute (e.g., skills about "generate", "create", "build"). Producers come first.
- **Sequential Task Chain**: When the goal involves multiple skill domains, create Task 1 for the information producer, let it complete, then Task 2 for the executor using Task 1's output.
- **No Shortcuts**: Don't try to have one agent do another agent's specialty work. Decompose properly even if it means more tasks.

### üéØ DELEGATION FIRST PRINCIPLE
- ALWAYS delegate to an appropriate agent if you have ANY actionable information related to the goal
- **BUT** check if the task requires prerequisite skills from a different agent - if so, delegate to that agent FIRST
- Each agent should work within their skill domain - use the "skills" field to match task requirements to agent capabilities
- Tasks should arrive at agents with all necessary context already gathered by appropriate upstream agents
"""
        
        # Inject workflow if provided
        print(f"üîç [Agent Mode] Checking workflow: workflow={workflow}, stripped={workflow.strip() if workflow else 'N/A'}")
        if workflow and workflow.strip():
            workflow_section = f"""

### üî• MANDATORY WORKFLOW - FOLLOW ALL STEPS IN ORDER üî•
**CRITICAL**: The following workflow steps are MANDATORY and must ALL be completed before marking the goal as "completed".
Do NOT skip steps. Do NOT mark goal as completed until ALL workflow steps are done.

{workflow.strip()}

**IMPORTANT**: 
- Execute sequential steps (1, 2, 3) one after another
- **PARALLEL STEPS** (e.g., 2a, 2b, 2c): When you see steps with letter suffixes, these can run SIMULTANEOUSLY
  - Use `next_tasks` (list) with `parallel=true` to execute them concurrently
  - Wait for ALL parallel tasks to complete before moving to the next sequential step
- Only mark goal_status="completed" after ALL workflow steps are finished
- If a step fails, you may retry or adapt, but you must complete all steps
"""
            system_prompt += workflow_section
            log_debug(f"üìã [Agent Mode] ‚úÖ Injected workflow into planner prompt ({len(workflow)} chars)")
            log_debug(f"üìã [Agent Mode] Workflow section preview:\n{workflow_section[:500]}...")
        else:
            log_debug(f"üìã [Agent Mode] ‚ùå No workflow to inject (workflow is None or empty)")
        
        # Add workflow-specific completion logic if workflow is present
        if workflow and workflow.strip():
            # Count the workflow steps to make it explicit
            workflow_step_count = len([line for line in workflow.strip().split('\n') if line.strip() and (line.strip()[0].isdigit() or line.strip().startswith('-'))])
            print(f"üìä [Agent Mode] Detected {workflow_step_count} steps in workflow")
            log_debug(f"üìä [Agent Mode] Workflow step count: {workflow_step_count}")
            
            system_prompt += f"""

### üö® CRITICAL: WHEN TO STOP (WORKFLOW MODE)
- A WORKFLOW IS ACTIVE with **{workflow_step_count} MANDATORY STEPS** - You MUST complete ALL {workflow_step_count} workflow steps before marking goal as "completed"
- **STEP COUNTING**: The workflow has EXACTLY {workflow_step_count} steps. Count your completed tasks carefully!
- **VERIFICATION CHECKLIST**:
  1. Count the number of workflow steps above (should be {workflow_step_count})
  2. Count the number of successfully completed tasks in your plan
  3. Match each workflow step to a completed task
  4. If completed tasks < {workflow_step_count}, goal_status MUST be "incomplete"
- **COMPLETION CRITERIA** - Mark goal_status="completed" ONLY when:
  1. You have AT LEAST {workflow_step_count} successfully completed tasks, AND
  2. Each workflow step has been addressed by a completed task, AND
  3. All completed tasks succeeded (or agents are waiting for user input)
- **WARNING**: Do NOT mark as completed after only 1, 2, or 3 steps if the workflow has {workflow_step_count} steps!
- If ANY workflow step is missing or incomplete, goal_status MUST be "incomplete" and you must create the next task"""
        else:
            system_prompt += """

### üö® CRITICAL: WHEN TO STOP (LOOP DETECTION & USER INPUT)
- ONLY mark goal as "completed" in these specific cases:
  1. The goal is actually fully accomplished with successful task outputs
  2. You have 2+ completed tasks where agents explicitly asked the USER for information
  3. The last agent response clearly states they need user input to proceed
- If NO tasks have been created yet, DO NOT mark as completed - create a task first!
- When agents request information, synthesize their questions and present to the user
- When the user provides information in a follow-up, create a NEW task with that information"""
        
        while plan.goal_status == "incomplete" and iteration < max_iterations:
            iteration += 1
            print(f"üîÑ [Agent Mode] Iteration {iteration}/{max_iterations}")
            await self._emit_status_event(f"Planning step {iteration}...", context_id)
            
            # Build user prompt with current plan state
            available_agents = []
            for card in self.cards.values():
                agent_info = {
                    "name": card.name,
                    "description": card.description
                }
                
                # Add skills if present
                if hasattr(card, 'skills') and card.skills:
                    skills_list = []
                    for skill in card.skills:
                        skill_dict = {
                            "id": getattr(skill, 'id', ''),
                            "name": getattr(skill, 'name', ''),
                            "description": getattr(skill, 'description', ''),
                        }
                        skills_list.append(skill_dict)
                    agent_info['skills'] = skills_list
                
                available_agents.append(agent_info)
            
            user_prompt = f"""Goal:
{plan.goal}

Current Plan (JSON):
{json.dumps(plan.model_dump(), indent=2, default=str)}

Available Agents (JSON):
{json.dumps(available_agents, indent=2)}

Analyze the plan and determine the next step. Proceed autonomously - do NOT ask the user for permission or confirmation."""
            
            # Get next step from orchestrator
            try:
                # Log system prompt for debugging (first 2000 chars)
                print(f"üîç [Agent Mode] System prompt being sent to Azure OpenAI (first 2000 chars):\n{system_prompt[:2000]}...")
                print(f"üîç [Agent Mode] System prompt contains 'MANDATORY WORKFLOW': {'MANDATORY WORKFLOW' in system_prompt}")
                
                next_step = await self._call_azure_openai_structured(
                    system_prompt=system_prompt,
                    user_prompt=user_prompt,
                    response_model=NextStep,
                    context_id=context_id
                )
                
                print(f"ü§ñ [Agent Mode] Orchestrator decision: {next_step.reasoning}")
                print(f"ü§ñ [Agent Mode] Goal status: {next_step.goal_status}")
                await self._emit_status_event(f"Reasoning: {next_step.reasoning}", context_id)
                
                # Update plan status
                plan.goal_status = next_step.goal_status
                plan.updated_at = datetime.now(timezone.utc)
                
                # Log plan state after orchestrator decision
                print(f"\n{'='*80}")
                log_debug(f"üìã [Agent Mode] PLAN STATE (Iteration {iteration})")
                print(f"{'='*80}")
                print(f"Goal: {plan.goal}")
                print(f"Goal Status: {plan.goal_status}")
                print(f"Total Tasks: {len(plan.tasks)}")
                for i, task in enumerate(plan.tasks, 1):
                    print(f"\n  Task {i}:")
                    print(f"    Description: {task.task_description}")
                    print(f"    Agent: {task.recommended_agent or 'None'}")
                    print(f"    State: {task.state}")
                    if task.error_message:
                        print(f"    Error: {task.error_message}")
                    if task.output:
                        output_preview = str(task.output).replace('\n', ' ')[:100]
                        print(f"    Output: {output_preview}...")
                print(f"\n  Next Step Reasoning: {next_step.reasoning}")
                print(f"  üîÄ Parallel Flag: {next_step.parallel}")
                if next_step.next_tasks:
                    print(f"  üîÄ Next Tasks (list): {len(next_step.next_tasks)} tasks")
                    for idx, task in enumerate(next_step.next_tasks, 1):
                        print(f"    {idx}. {task.get('task_description', 'N/A')} ‚Üí {task.get('recommended_agent', 'N/A')}")
                elif next_step.next_task:
                    print(f"  Next Task (single): {next_step.next_task.get('task_description', 'N/A')}")
                    print(f"  Target Agent: {next_step.next_task.get('recommended_agent', 'N/A')}")
                print(f"{'='*80}\n")
                
                if next_step.goal_status == "completed":
                    completed_tasks_count = len([t for t in plan.tasks if t.state == "completed"])
                    log_info(f"‚úÖ [Agent Mode] Goal marked as completed after {iteration} iterations")
                    log_info(f"üìä [Agent Mode] Completed tasks: {completed_tasks_count} / Expected workflow steps: {workflow_step_count if workflow and workflow.strip() else 'N/A'}")
                    if workflow and workflow.strip() and completed_tasks_count < workflow_step_count:
                        print(f"‚ö†Ô∏è  [Agent Mode] WARNING: Only {completed_tasks_count} tasks completed but workflow has {workflow_step_count} steps!")
                        print(f"‚ö†Ô∏è  [Agent Mode] LLM reasoning: {next_step.reasoning}")
                    await self._emit_status_event("Goal achieved! Generating final response...", context_id)
                    break
                
                # =========================================================
                # TASK EXECUTION: Handle both sequential and parallel tasks
                # =========================================================
                
                # Determine which tasks to execute
                tasks_to_execute = []
                is_parallel = next_step.parallel and next_step.next_tasks
                
                if is_parallel and next_step.next_tasks:
                    # PARALLEL EXECUTION: Multiple tasks via next_tasks
                    log_info(f"üîÄ [Agent Mode] PARALLEL execution: {len(next_step.next_tasks)} tasks")
                    await self._emit_status_event(f"Executing {len(next_step.next_tasks)} tasks in parallel...", context_id)
                    for task_dict in next_step.next_tasks:
                        tasks_to_execute.append({
                            "task_description": task_dict.get("task_description"),
                            "recommended_agent": task_dict.get("recommended_agent")
                        })
                elif next_step.next_task:
                    # SEQUENTIAL EXECUTION: Single task via next_task
                    tasks_to_execute.append({
                        "task_description": next_step.next_task.get("task_description"),
                        "recommended_agent": next_step.next_task.get("recommended_agent")
                    })
                
                if not tasks_to_execute:
                    print(f"‚ö†Ô∏è [Agent Mode] No tasks to execute, breaking loop")
                    break
                
                # Validate all tasks have descriptions
                for task_dict in tasks_to_execute:
                    if not task_dict.get("task_description"):
                        print(f"‚ö†Ô∏è [Agent Mode] Task missing description, skipping")
                        tasks_to_execute.remove(task_dict)
                
                if not tasks_to_execute:
                    print(f"‚ö†Ô∏è [Agent Mode] No valid tasks after validation, breaking loop")
                    break
                
                # Create AgentModeTask objects for all tasks
                pydantic_tasks = []
                for task_dict in tasks_to_execute:
                    task = AgentModeTask(
                        task_id=str(uuid.uuid4()),
                        task_description=task_dict["task_description"],
                        recommended_agent=task_dict.get("recommended_agent"),
                        state="pending"
                    )
                    plan.tasks.append(task)
                    pydantic_tasks.append(task)
                    log_debug(f"üìã [Agent Mode] Created task: {task.task_description[:50]}...")
                
                # Execute tasks (parallel or sequential)
                if is_parallel:
                    # ============================================
                    # PARALLEL EXECUTION via asyncio.gather()
                    # ============================================
                    import asyncio as async_lib  # Import locally to avoid any scoping issues
                    log_info(f"üîÄ [Agent Mode] Executing {len(pydantic_tasks)} tasks IN PARALLEL")
                    await self._emit_status_event(f"Executing {len(pydantic_tasks)} tasks simultaneously...", context_id)
                    
                    async def execute_task_parallel(task: AgentModeTask) -> Dict[str, Any]:
                        """Execute a single task and return result dict."""
                        task.state = "running"
                        task.updated_at = datetime.now(timezone.utc)
                        
                        try:
                            # For parallel tasks, pass only the LAST task output (from the step before parallel group)
                            # Don't pass all accumulated outputs - that would grow context exponentially
                            previous_output = [all_task_outputs[-1]] if all_task_outputs else None
                            
                            result = await self._execute_orchestrated_task(
                                task=task,
                                session_context=session_context,
                                context_id=context_id,
                                workflow=workflow,
                                user_message=user_message,
                                extract_text_fn=extract_text_from_response,
                                previous_task_outputs=previous_output  # ‚úÖ Only LAST output
                            )
                            return result
                        except Exception as e:
                            task.state = "failed"
                            task.error_message = str(e)
                            task.updated_at = datetime.now(timezone.utc)
                            log_error(f"[Agent Mode] Parallel task failed: {e}")
                            return {"error": str(e), "task_id": task.task_id}
                    
                    # Run all tasks in parallel
                    try:
                        log_info(f"üîÄ [Agent Mode] About to call asyncio.gather with {len(pydantic_tasks)} tasks")
                        results = await async_lib.gather(
                            *[execute_task_parallel(t) for t in pydantic_tasks],
                            return_exceptions=True
                        )
                        log_info(f"üîÄ [Agent Mode] asyncio.gather completed, got {len(results)} results")
                    except Exception as gather_error:
                        log_error(f"[Agent Mode] ERROR in asyncio.gather: {gather_error}")
                        log_error(f"[Agent Mode] Error type: {type(gather_error)}")
                        import traceback
                        log_error(f"[Agent Mode] Traceback:\n{traceback.format_exc()}")
                        raise
                    
                    # Process results
                    hitl_pause = False
                    for i, result in enumerate(results):
                        task = pydantic_tasks[i]
                        if isinstance(result, Exception):
                            log_error(f"[Agent Mode] Parallel task exception: {result}")
                            task.state = "failed"
                            task.error_message = str(result)
                        elif isinstance(result, dict):
                            if result.get("hitl_pause"):
                                hitl_pause = True
                                # Store for HITL resumption
                                if result.get("output"):
                                    all_task_outputs.append(result["output"])
                            elif result.get("output"):
                                all_task_outputs.append(result["output"])
                            elif result.get("error"):
                                log_error(f"[Agent Mode] Task error: {result['error']}")
                        task.updated_at = datetime.now(timezone.utc)
                    
                    # If any task triggered HITL pause, pause the workflow
                    if hitl_pause:
                        log_info(f"‚è∏Ô∏è [Agent Mode] HITL pause triggered during parallel execution")
                        session_context.pending_workflow = workflow
                        session_context.pending_workflow_outputs = all_task_outputs.copy()
                        session_context.pending_workflow_user_message = user_message
                        return all_task_outputs
                    
                    log_info(f"‚úÖ [Agent Mode] All {len(pydantic_tasks)} parallel tasks completed")
                    
                else:
                    # ============================================
                    # SEQUENTIAL EXECUTION (single task)
                    # ============================================
                    task = pydantic_tasks[0]
                    task.state = "running"
                    task.updated_at = datetime.now(timezone.utc)
                    
                    log_debug(f"üìã [Agent Mode] Sequential task: {task.task_description}")
                    # Removed duplicate "Executing:" emit - already emitted in _execute_orchestrated_task
                    
                    try:
                        # For sequential tasks, pass only the LAST task output (from the immediately previous step)
                        # Don't pass all accumulated outputs - that would grow context exponentially
                        previous_output = [all_task_outputs[-1]] if all_task_outputs else None
                        
                        result = await self._execute_orchestrated_task(
                            task=task,
                            session_context=session_context,
                            context_id=context_id,
                            workflow=workflow,
                            user_message=user_message,
                            extract_text_fn=extract_text_from_response,
                            previous_task_outputs=previous_output  # ‚úÖ Only LAST output
                        )
                        
                        if result.get("hitl_pause"):
                            # HITL pause - return immediately
                            if result.get("output"):
                                all_task_outputs.append(result["output"])
                            session_context.pending_workflow = workflow
                            session_context.pending_workflow_outputs = all_task_outputs.copy()
                            session_context.pending_workflow_user_message = user_message
                            return all_task_outputs
                        
                        if result.get("output"):
                            all_task_outputs.append(result["output"])
                        
                    except Exception as e:
                        task.state = "failed"
                        task.error_message = str(e)
                        log_error(f"[Agent Mode] Task execution error: {e}")
                    
                    finally:
                        task.updated_at = datetime.now(timezone.utc)
                        
                        # Log task completion
                        print(f"\n{'~'*80}")
                        log_info(f"‚úÖ [Agent Mode] TASK COMPLETED")
                        print(f"{'~'*80}")
                        print(f"Task ID: {task.task_id}")
                        print(f"Description: {task.task_description}")
                        print(f"Agent: {task.recommended_agent or 'None'}")
                        print(f"Final State: {task.state}")
                        if task.error_message:
                            print(f"Error: {task.error_message}")
                        if task.output:
                            print(f"Output: {json.dumps(task.output, indent=2, default=str)[:500]}...")
                        print(f"{'~'*80}\n")
                
            except Exception as e:
                log_error(f"[Agent Mode] Orchestration error: {e}")
                await self._emit_status_event(f"Error in orchestration: {str(e)}", context_id)
                break
        
        if iteration >= max_iterations:
            print(f"‚ö†Ô∏è [Agent Mode] Reached maximum iterations ({max_iterations})")
            await self._emit_status_event("Maximum iterations reached, completing...", context_id)
        
        # Log final plan summary
        print(f"\n{'='*80}")
        print(f"üé¨ [Agent Mode] FINAL PLAN SUMMARY")
        print(f"{'='*80}")
        print(f"Goal: {plan.goal}")
        print(f"Final Status: {plan.goal_status}")
        print(f"Total Iterations: {iteration}")
        print(f"Total Tasks Created: {len(plan.tasks)}")
        print(f"\nTask Breakdown:")
        for i, task in enumerate(plan.tasks, 1):
            print(f"  {i}. [{task.state.upper()}] {task.task_description[:60]}...")
            print(f"     Agent: {task.recommended_agent or 'None'}")
        print(f"\nTask Outputs Collected: {len(all_task_outputs)}")
        print(f"{'='*80}\n")
        
        # Generate final response from all outputs
        print(f"üé¨ [Agent Mode] Orchestration complete. {len(all_task_outputs)} task outputs collected")
        print(f"üéüÔ∏è [Host Agent] Final token usage: {self.host_token_usage}")
        
        # Emit host token usage to frontend
        try:
            from service.websocket_streamer import get_websocket_streamer
            import asyncio
            
            async def emit_host_tokens():
                streamer = await get_websocket_streamer()
                if streamer:
                    event_data = {
                        "agentName": "foundry-host-agent",
                        "tokenUsage": self.host_token_usage,
                        "state": "completed",
                        "timestamp": __import__('datetime').datetime.now(timezone.utc).isoformat()
                    }
                    await streamer._send_event("host_token_usage", event_data, context_id)
                    print(f"üì° [Host Agent] Emitted token usage to frontend: {self.host_token_usage['total_tokens']} tokens")
            
            asyncio.create_task(emit_host_tokens())
        except Exception as e:
            print(f"‚ö†Ô∏è [Host Agent] Error emitting token usage: {e}")
        
        return all_task_outputs

    async def get_current_root_instruction(self) -> str:
        """Get the current root instruction (custom or default)"""
        return self.root_instruction('foundry-host-agent')

    async def update_root_instruction(self, new_instruction: str) -> bool:
        """Update the root instruction and apply it to the Azure AI Foundry agent"""
        try:
            print(f"üîÑ Updating root instruction...")
            print(f"   New instruction length: {len(new_instruction)} characters")
            
            # Store the custom instruction
            self.custom_root_instruction = new_instruction
            
            # Update the Azure AI Foundry agent with the new instruction
            await self._update_agent_instructions()
            
            print(f"‚úÖ Root instruction updated successfully!")
            return True
            
        except Exception as e:
            print(f"‚ùå Error updating root instruction: {e}")
            return False

    async def reset_root_instruction(self) -> bool:
        """Reset to default root instruction"""
        try:
            print(f"üîÑ Resetting to default root instruction...")
            
            # Clear the custom instruction
            self.custom_root_instruction = None
            
            # Update the Azure AI Foundry agent with the default instruction
            await self._update_agent_instructions()
            
            print(f"‚úÖ Root instruction reset to default!")
            return True
            
        except Exception as e:
            print(f"‚ùå Error resetting root instruction: {e}")
            return False

    def _stream_remote_agent_activity(self, event: TaskCallbackArg, agent_card: AgentCard):
        """Stream remote agent activity to WebSocket for granular UI visibility in thinking box."""
        try:
            agent_name = agent_card.name
            
            # Skip streaming if we've already handled this event type in the detailed callback
            # This prevents duplicate events from being sent to the UI
            if hasattr(event, 'kind'):
                # The detailed streaming callback already handles these event types
                return
            
            status_text = f"processing request"
            
            # Extract granular status from different event types
            if isinstance(event, Task):
                if hasattr(event.status, 'state'):
                    state = event.status.state
                    if hasattr(state, 'value'):
                        state_value = state.value
                    else:
                        state_value = str(state)
                    
                    if state_value == 'working':
                        status_text = "analyzing request"
                    elif state_value == 'completed':
                        status_text = "response ready"
                    elif state_value == 'failed':
                        status_text = "request failed"
                    elif state_value == 'input_required':
                        status_text = "requires additional input"
                    else:
                        status_text = f"status: {state_value}"
            
            # Stream to WebSocket using async background task (only for Task objects, not streaming events)
            import asyncio
            async def stream_activity():
                try:
                    # Use the _emit_status_event method for consistency
                    await self._emit_granular_agent_event(agent_name, status_text)
                except Exception as e:
                    log_debug(f"Error streaming remote agent activity: {e}")
                    pass
            
            # Create background task (non-blocking)
            try:
                asyncio.create_task(stream_activity())
            except RuntimeError:
                # Handle case where no event loop is running
                log_debug(f"No event loop available for streaming agent activity from {agent_name}")
                pass
                
        except Exception as e:
            log_debug(f"Error in _stream_remote_agent_activity: {e}")
            # Don't let streaming errors break the callback
            pass

    async def _emit_tool_call_event(self, agent_name: str, tool_name: str, arguments: dict, context_id: str = None):
        """Emit tool call event to WebSocket for granular UI visibility."""
        try:
            # Import here to avoid circular imports
            from service.websocket_streamer import get_websocket_streamer
            
            streamer = await get_websocket_streamer()
            if streamer:
                event_data = {
                    "agentName": agent_name,
                    "toolName": tool_name,
                    "arguments": arguments,
                    "timestamp": __import__('datetime').datetime.now(timezone.utc).isoformat(),
                    "contextId": context_id
                }
                
                success = await streamer._send_event("tool_call", event_data, context_id)
                if success:
                    log_debug(f"Streamed tool call: {agent_name} - {tool_name}")
                else:
                    log_debug(f"Failed to stream tool call: {agent_name}")
            else:
                log_debug(f"WebSocket streamer not available for tool call")
                
        except Exception as e:
            log_debug(f"Error emitting tool call event: {e}")
            pass

    async def _emit_outgoing_message_event(self, target_agent_name: str, message: str, context_id: str):
        """Emit outgoing message event to WebSocket for DAG display."""
        try:
            # Import here to avoid circular imports
            from service.websocket_streamer import get_websocket_streamer
            
            streamer = await get_websocket_streamer()
            if streamer:
                event_data = {
                    "sourceAgent": "Host Agent",  # Host Agent is sending the message
                    "targetAgent": target_agent_name,  # Remote agent receiving the message
                    "message": message,
                    "timestamp": __import__('datetime').datetime.now(timezone.utc).isoformat(),
                    "contextId": context_id
                }
                
                print(f"üì§ [OUTGOING MESSAGE EVENT] Emitting to frontend:")
                print(f"   ‚Ä¢ Target Agent: {target_agent_name}")
                print(f"   ‚Ä¢ Message: {message[:100]}..." if len(message) > 100 else f"   ‚Ä¢ Message: {message}")
                print(f"   ‚Ä¢ Context ID: {context_id}")
                
                await streamer._send_event("outgoing_agent_message", event_data, context_id)
                print(f"‚úÖ [OUTGOING MESSAGE EVENT] Emitted successfully")
                
        except Exception as e:
            print(f"‚ùå [OUTGOING MESSAGE EVENT] Error: {e}")
            log_debug(f"Error emitting outgoing message event: {e}")
            pass

    async def _emit_tool_response_event(self, agent_name: str, tool_name: str, status: str, error_message: str = None, context_id: str = None):
        """Emit tool response event to WebSocket for granular UI visibility."""
        try:
            # Import here to avoid circular imports
            from service.websocket_streamer import get_websocket_streamer
            from datetime import datetime as dt, timezone as tz
            
            streamer = await get_websocket_streamer()
            if streamer:
                event_data = {
                    "agentName": agent_name,
                    "toolName": tool_name,
                    "status": status,
                    "timestamp": dt.now(tz.utc).isoformat(),
                    "contextId": context_id
                }
                
                if error_message:
                    event_data["error"] = error_message
                
                success = await streamer._send_event("tool_response", event_data, context_id)
                if success:
                    log_debug(f"Streamed tool response: {agent_name} - {tool_name} - {status}")
                else:
                    log_debug(f"Failed to stream tool response: {agent_name}")
                
                # ALSO emit task_updated so the frontend sidebar updates correctly
                # This is the single source of truth for agent sidebar status
                task_state = "completed" if status == "success" else "failed"
                
                # CRITICAL: context_id must be provided, no fallback!
                if not context_id:
                    log_debug(f"‚ö†Ô∏è [_stream_remote_agent_activity] No context_id for agent {agent_name}, cannot emit task_updated")
                    return
                
                task_updated_data = {
                    "taskId": str(uuid.uuid4()),
                    "conversationId": context_id,
                    "contextId": context_id,
                    "state": task_state,
                    "agentName": agent_name,
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "content": f"Agent {status}" if not error_message else error_message
                }
                print(f"üéØ [SIDEBAR] Emitting task_updated for {agent_name}: state={task_state}")
                result = await streamer._send_event("task_updated", task_updated_data, context_id)
                print(f"üéØ [SIDEBAR] task_updated sent, success={result}")
            else:
                log_debug(f"WebSocket streamer not available for tool response")
                
        except Exception as e:
            log_debug(f"Error emitting tool response event: {e}")
            pass

    async def _emit_simple_task_status(
        self, 
        agent_name: str, 
        state: str, 
        context_id: str, 
        task_id: str = None
    ) -> bool:
        """
        Emit a simple task status update to the sidebar via WebSocket.
        
        This is a lightweight helper for emitting task state changes (submitted, working, 
        completed, failed) without the complexity of full Task object handling.
        
        Args:
            agent_name: Name of the agent whose status is changing
            state: Task state string (submitted, working, completed, failed)
            context_id: Context ID for WebSocket routing
            task_id: Optional task ID (generates UUID if not provided)
            
        Returns:
            True if event was sent successfully, False otherwise
        """
        try:
            from service.websocket_streamer import get_websocket_streamer
            streamer = await get_websocket_streamer()
            if streamer:
                event_data = {
                    "taskId": task_id or str(uuid.uuid4()),
                    "conversationId": context_id,
                    "contextId": context_id,
                    "state": state,
                    "agentName": agent_name,
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                }
                log_debug(f"[SIDEBAR] Emitting task_updated for {agent_name}: state={state}")
                return await streamer._send_event("task_updated", event_data, context_id)
            else:
                log_debug(f"No WebSocket streamer available for task status: {agent_name}/{state}")
                return False
        except Exception as e:
            log_debug(f"Error emitting task status {state} for {agent_name}: {e}")
            return False

    async def _emit_file_artifact_event(
        self,
        filename: str,
        uri: str,
        context_id: str,
        agent_name: str,
        content_type: str = "image/png",
        size: int = 0
    ) -> bool:
        """
        Emit a file artifact event to notify the UI of a new file from an agent.
        
        Args:
            filename: Name of the file
            uri: URI/URL where the file can be accessed
            context_id: Context ID for WebSocket routing (use host context)
            agent_name: Name of the agent that produced the file
            content_type: MIME type of the file
            size: File size in bytes (0 if unknown)
            
        Returns:
            True if event was sent successfully, False otherwise
        """
        try:
            from service.websocket_streamer import get_websocket_streamer
            streamer = await get_websocket_streamer()
            if streamer:
                file_info = {
                    "file_id": str(uuid.uuid4()),
                    "filename": filename,
                    "uri": uri,
                    "size": size,
                    "content_type": content_type,
                    "source_agent": agent_name,
                    "contextId": context_id
                }
                await streamer.stream_file_uploaded(file_info, context_id)
                log_debug(f"File uploaded event sent: {filename} from {agent_name}")
                return True
            else:
                log_debug(f"No WebSocket streamer available for file event: {filename}")
                return False
        except Exception as e:
            log_debug(f"Error emitting file artifact event: {e}")
            return False

    async def _emit_granular_agent_event(self, agent_name: str, status_text: str, context_id: str = None):
        """Emit granular agent activity event to WebSocket for thinking box visibility."""
        try:
            # Import here to avoid circular imports
            from service.websocket_streamer import get_websocket_streamer
            
            streamer = await get_websocket_streamer()
            if streamer:
                # DEBUG: Log what contextIds we're working with
                stored_host_context = getattr(self, '_current_host_context_id', None)
                log_debug(f"üîç [CONTEXT DEBUG] _emit_granular_agent_event for {agent_name}:")
                log_debug(f"  - context_id param: {context_id}")
                log_debug(f"  - stored _current_host_context_id: {stored_host_context}")
                
                # CRITICAL: Use the provided context_id, or the stored one - NO UUID FALLBACK!
                routing_context_id = context_id or stored_host_context
                
                if not routing_context_id:
                    log_debug(f"‚ö†Ô∏è [_emit_granular_agent_event] No context_id for {agent_name}, skipping event emission")
                    return
                
                log_debug(f"  - final routing_context_id: {routing_context_id}")
                
                event_data = {
                    "agentName": agent_name,
                    "content": status_text,
                    "timestamp": __import__('datetime').datetime.now(timezone.utc).isoformat(),
                    "contextId": routing_context_id
                }
                
                # Use the remote_agent_activity event type for granular visibility
                success = await streamer._send_event("remote_agent_activity", event_data, routing_context_id)
                if success:
                    log_debug(f"Streamed remote agent activity: {agent_name} - {status_text}")
                else:
                    log_debug(f"Failed to stream remote agent activity: {agent_name}")
            else:
                log_debug(f"WebSocket streamer not available for remote agent activity")
                
        except Exception as e:
            log_debug(f"Error emitting granular agent event: {e}")
            # Don't let streaming errors break the main flow
            pass

    def _default_task_callback(self, event: TaskCallbackArg, agent_card: AgentCard) -> Task:
        """Default task callback optimized for streaming remote agent execution.
        
        CONSOLIDATED: Uses _emit_task_event as the SINGLE source of truth for all
        remote agent status updates to prevent duplicate events in the UI.
        """
        agent_name = agent_card.name
        print(f"üîî [CALLBACK] Task callback invoked from {agent_name}: {type(event).__name__}")
        import sys
        sys.stdout.flush()
        log_debug(f"[STREAMING] Task callback from {agent_name}: {type(event).__name__}")
        
        # Keep session context task mapping in sync per agent
        try:
            context_id_cb = get_context_id(event, None)
            task_id_cb = get_task_id(event, None)
            if context_id_cb and task_id_cb:
                session_ctx = self.get_session_context(context_id_cb)
                session_ctx.agent_task_ids[agent_name] = task_id_cb
                # If status-update, capture state per agent
                if hasattr(event, 'kind') and getattr(event, 'kind', '') == 'status-update':
                    state_obj = getattr(getattr(event, 'status', None), 'state', None)
                    if state_obj is not None:
                        state_str = state_obj.value if hasattr(state_obj, 'value') else str(state_obj)
                        session_ctx.agent_task_states[agent_name] = state_str
                        
                        # HUMAN-IN-THE-LOOP: Track input_required state from remote agents
                        if state_str == 'input_required' or state_str == 'input-required':
                            session_ctx.pending_input_agent = agent_name
                            session_ctx.pending_input_task_id = task_id_cb
                            log_info(f"üîÑ [HITL] Callback detected input_required from '{agent_name}', setting pending_input_agent (task_id: {task_id_cb})")
        except Exception as e:
            # Non-fatal; continue normal processing
            log_debug(f"[STREAMING] Error in task callback context tracking: {e}")
            pass
        
        # CONSOLIDATED: Only status-update and artifact-update events should update the UI
        # 'task' events are for internal task creation/tracking, not UI status updates
        # Emitting 'task' events was causing late "working" events to overwrite "completed" status
        if hasattr(event, 'kind'):
            event_kind = getattr(event, 'kind', 'unknown')
            log_debug(f"[STREAMING] Event kind from {agent_name}: {event_kind}")
            print(f"üîî [STREAMING] Received event from {agent_name}: kind={event_kind}")
            
            # Only emit status-update and artifact-update to UI (NOT 'task' events)
            if event_kind in ['artifact-update', 'status-update']:
                log_debug(f"[STREAMING] Emitting via _emit_task_event for {agent_name}: {event_kind}")
                print(f"üì§ [STREAMING] Calling _emit_task_event for {agent_name}: {event_kind}")
                self._emit_task_event(event, agent_card)
                
                # WORKFLOW VISIBILITY: Also emit granular workflow events
                # Use stored host context if available, otherwise fall back to event's context
                context_id_for_event = getattr(self, '_current_host_context_id', None) or get_context_id(event, None)
                if event_kind == 'status-update':
                    # Extract actual text from status message
                    status_text = "processing"
                    if hasattr(event, 'status') and event.status:
                        if hasattr(event.status, 'message') and event.status.message:
                            if hasattr(event.status.message, 'parts') and event.status.message.parts:
                                for part in event.status.message.parts:
                                    if hasattr(part, 'root') and hasattr(part.root, 'text'):
                                        status_text = part.root.text
                                        break
                        elif hasattr(event.status, 'state'):
                            state = event.status.state
                            state_value = state.value if hasattr(state, 'value') else str(state)
                            status_text = f"status: {state_value}"
                    asyncio.create_task(self._emit_granular_agent_event(agent_name, status_text, context_id_for_event))
                
                elif event_kind == 'artifact-update':
                    asyncio.create_task(self._emit_granular_agent_event(agent_name, "generating artifact", context_id_for_event))
            
            elif event_kind == 'task':
                log_debug(f"[STREAMING] Skipping UI emit for 'task' event from {agent_name} (internal tracking only)")
                # But still emit to workflow for task started
                context_id_for_event = getattr(self, '_current_host_context_id', None) or get_context_id(event, None)
                asyncio.create_task(self._emit_granular_agent_event(agent_name, "task started", context_id_for_event))
            # Skip other intermediate events
        
        # Get or create task for this specific agent
        current_task = self._agent_tasks.get(agent_name)
        
        if isinstance(event, Task):
            # Initial task creation - store per agent
            log_debug(f"[PARALLEL] Storing new task for {agent_name}")
            self._agent_tasks[agent_name] = event
            return event
        
        elif hasattr(event, 'kind'):
            if event.kind == 'task':
                # Initial task event - store per agent
                log_debug(f"[PARALLEL] Storing task event for {agent_name}")
                self._agent_tasks[agent_name] = event
                return event
            
            elif event.kind == 'status-update' and current_task:
                # Update existing task status for this agent
                log_debug(f"[PARALLEL] Updating task status for {agent_name}")
                if hasattr(event, 'status'):
                    current_task.status = event.status
                return current_task
            
            elif event.kind == 'artifact-update' and current_task:
                # Add artifact to existing task for this agent
                log_debug(f"[PARALLEL] Adding artifact for {agent_name}")
                if hasattr(event, 'artifact'):
                    if not current_task.artifacts:
                        current_task.artifacts = []
                    current_task.artifacts.append(event.artifact)
                return current_task
        
        # Fallback: return current task for this agent or create a minimal one
        if current_task:
            return current_task
        
        # Create minimal task for this agent
        # Note: These are fallbacks for events that don't have taskId/contextId
        # This shouldn't happen in normal flow but protects against malformed events
        task_id = get_task_id(event, str(uuid.uuid4()))
        contextId_from_event = get_context_id(event, None)
        
        if not contextId_from_event:
            log_debug(f"‚ö†Ô∏è [_emit_task_event] Event from {agent_card.name} has no contextId, using UUID fallback")
            contextId = str(uuid.uuid4())
        else:
            contextId = contextId_from_event
        
        from a2a.types import TaskStatus, TaskState
        status = TaskStatus(
            state=getattr(event, 'state', TaskState.working),
            message=None,
            timestamp=None
        )
        
        fallback_task = Task(
            id=task_id,
            contextId=contextId,
            status=status,
            history=[],
            artifacts=[]
        )
        
        # Store for this agent
        self._agent_tasks[agent_name] = fallback_task
        log_debug(f"[PARALLEL] Created fallback task for {agent_name}")
        return fallback_task

    def _emit_task_event(self, task: TaskCallbackArg, agent_card: AgentCard):
        """Emit event for task callback, with enhanced agent name context for UI status tracking."""
        agent_name = agent_card.name
        log_debug(f"üîî [EMIT_TASK_EVENT] Called for agent: {agent_name}")
        print(f"üéØ [_emit_task_event] CALLED for agent: {agent_name}, task.kind: {getattr(task, 'kind', 'NO KIND')}")
        log_debug(f"Agent capabilities: {agent_card.capabilities if hasattr(agent_card, 'capabilities') else 'None'}")
        
        # WORKFLOW MODE: Suppress redundant status events from remote agents
        # The workflow orchestrator already emits clean status updates
        contextId = get_context_id(task, None)
        if contextId:
            try:
                session_ctx = self.get_session_context(contextId)
                if session_ctx.agent_mode:
                    # In workflow/agent mode, suppress intermediate "working" and "submitted" status updates
                    # Only allow "completed" and "failed" events through (final states)
                    if hasattr(task, 'kind') and task.kind == 'status-update':
                        if hasattr(task, 'status') and task.status:
                            state_obj = getattr(task.status, 'state', 'working')
                            task_state = state_obj.value if hasattr(state_obj, 'value') else str(state_obj)
                            
                            # Suppress intermediate states in workflow mode - orchestrator handles these
                            if task_state in ['working', 'submitted', 'pending']:
                                log_debug(f"[WORKFLOW MODE] Suppressing intermediate '{task_state}' event from {agent_name}")
                                return task  # Return without emitting event
                    
                    log_debug(f"[WORKFLOW MODE] Allowing event from {agent_name}: {getattr(task, 'kind', 'unknown')}")
            except Exception as e:
                log_debug(f"Error checking workflow mode in _emit_task_event: {e}")
        
        content = None
        task_id = None
        task_state = None
        
        # Extract task state and ID
        if hasattr(task, 'kind') and task.kind == 'status-update':
            print(f"üîç [_emit_task_event] Processing status-update event")
            task_id = get_task_id(task, None)
            # Extract state from status object, handling enum types
            if hasattr(task, 'status') and task.status:
                state_obj = getattr(task.status, 'state', 'working')
                if hasattr(state_obj, 'value'):
                    task_state = state_obj.value  # Extract enum value
                else:
                    task_state = str(state_obj)
            else:
                task_state = 'working'
            
            log_debug(f"Status update extracted: {task_state} for {agent_card.name}")
            print(f"‚ú® [_emit_task_event] Extracted task_state: {task_state} for {agent_name}")
            
            if hasattr(task, 'status') and task.status and task.status.message:
                content = task.status.message
            else:
                # Create a status message
                from a2a.types import Message, Part, TextPart, Role
                content = Message(
                    parts=[Part(root=TextPart(text=f"Task status: {task_state}"))],
                    role=Role.agent,
                    messageId=str(uuid.uuid4()),
                    contextId=contextId,
                    taskId=task_id,
                )
        elif hasattr(task, 'kind') and task.kind == 'artifact-update':
            task_id = get_task_id(task, None)
            task_state = 'completed'  # Artifact updates typically indicate completion
            if hasattr(task, 'artifact') and task.artifact:
                from a2a.types import Message, Role
                content = Message(
                    parts=task.artifact.parts,
                    role=Role.agent,
                    messageId=str(uuid.uuid4()),
                    contextId=contextId,
                    taskId=task_id,
                )
        elif isinstance(task, Task):
            task_id = task.id
            contextId = get_context_id(task, contextId)
            # Extract task state from TaskState enum
            if hasattr(task.status, 'state'):
                if hasattr(task.status.state, 'value'):
                    task_state = task.status.state.value  # Extract enum value
                else:
                    task_state = str(task.status.state)
            
            if task.status and task.status.message:
                content = task.status.message
            elif task.artifacts:
                from a2a.types import Message, Role
                parts = []
                for a in task.artifacts:
                    parts.extend(a.parts)
                content = Message(
                    parts=parts,
                    role=Role.agent,
                    messageId=str(uuid.uuid4()),
                    taskId=task_id,
                    contextId=contextId,
                )
        
        # Create default content if none found
        if not content:
            from a2a.types import Message, Part, TextPart, Role
            status_text = f"Task update for {agent_card.name}"
            if task_state:
                status_text = f"Task status: {task_state}"
            
            content = Message(
                parts=[Part(root=TextPart(text=status_text))],
                role=Role.agent,
                messageId=str(uuid.uuid4()),
                taskId=task_id or str(uuid.uuid4()),
                contextId=contextId or str(uuid.uuid4()),
            )
        
        # Create Event object like ADK version, but with enhanced agent context
        if content:
            from datetime import datetime as dt, timezone as tz
            event_obj = type('Event', (), {
                'id': str(uuid.uuid4()),
                'actor': agent_card.name,  # Use the actual agent name
                'content': content,
                'timestamp': dt.now(tz.utc).timestamp(),
            })()
            
            # Add to host manager events if available
            if hasattr(self, '_host_manager') and self._host_manager:
                self._host_manager.add_event(event_obj)
                log_debug(f"Added event to host manager for agent: {agent_card.name}")
            
            # Stream A2A-compliant task events to WebSocket with agent context
            # CONSOLIDATED: Single event emission point for remote agent status
            log_debug(f"Streaming A2A task event to WebSocket for agent: {agent_card.name}, state: {task_state}")
            try:
                import asyncio

                async def stream_task_event():
                    try:
                        from service.websocket_streamer import get_websocket_streamer

                        streamer = await get_websocket_streamer()
                        if not streamer:
                            log_debug("‚ö†Ô∏è WebSocket streamer not available for task event")
                            return

                        # Extract text content from message parts if available
                        text_content = ""
                        if content and hasattr(content, 'parts'):
                            for part in content.parts:
                                if hasattr(part, 'root') and hasattr(part.root, 'text'):
                                    text_content = part.root.text
                                    break
                        
                        # WORKFLOW MODE: Suppress full response text in workflow activity panel
                        # The orchestrator will display the full response - avoid duplicates
                        try:
                            session_ctx = self.get_session_context(contextId)
                            if session_ctx.agent_mode and text_content and task_state == 'completed':
                                log_debug(f"[WORKFLOW MODE] Suppressing full response text from {agent_card.name} - orchestrator will display")
                                text_content = ""  # Clear the content to avoid duplicate display
                        except Exception as e:
                            log_debug(f"Error checking workflow mode for content suppression: {e}")

                        # CONSOLIDATED: Include ALL relevant data in single task_updated event
                        # This is the ONLY event emitted for remote agent status updates
                        # Use stored host context if available, otherwise fall back to event's context
                        routing_context_id = (getattr(self, '_current_host_context_id', None) or 
                                             contextId or 
                                             getattr(self, 'default_contextId', str(uuid.uuid4())))
                        
                        event_data = {
                            "taskId": task_id or str(uuid.uuid4()),
                            "conversationId": routing_context_id or str(uuid.uuid4()),
                            "contextId": routing_context_id,
                            "state": task_state,
                            "artifactsCount": len(getattr(task, 'artifacts', [])),
                            "agentName": agent_card.name,
                            "timestamp": datetime.now(timezone.utc).isoformat(),
                            # Include message content so frontend has everything in one event
                            "content": text_content if text_content else None,
                        }
                        
                        # Add token usage if available for this agent
                        if agent_card.name in self.agent_token_usage:
                            event_data["tokenUsage"] = self.agent_token_usage[agent_card.name]

                        event_type = "task_updated"
                        if hasattr(task, 'kind') and task.kind == 'status-update':
                            event_type = "task_updated"
                        elif not hasattr(task, 'kind'):
                            event_type = "task_created"

                        # DEBUG: Log what we're sending to the frontend
                        print(f"üì° [A2A STREAM] Emitting {event_type} for {agent_card.name}: state={task_state}")
                        
                        success = await streamer._send_event(event_type, event_data, routing_context_id)
                        if success:
                            log_debug(f"‚úÖ A2A task event streamed: {agent_card.name} -> {task_state}")
                        else:
                            log_debug(f"‚ùå Failed to stream A2A task event: {agent_card.name} -> {task_state}")
                        
                        # REMOVED: Secondary remote_agent_activity emission
                        # All data is now included in the task_updated event above
                        
                    except Exception as e:
                        log_debug(f"‚ùå Error streaming A2A task event: {e}")
                        import traceback
                        traceback.print_exc()

                asyncio.create_task(stream_task_event())

            except Exception as e:
                log_debug(f"‚ùå Error setting up A2A task event streaming: {e}")
                pass

    def _emit_agent_registration_event(self, agent_card: AgentCard):
        """Emit agent registration event to WebSocket for UI sidebar visibility."""
        log_debug(f"Emitting agent registration event for: {agent_card.name}")
        try:
            import asyncio
            
            # Agent registration happens outside of any specific conversation
            # so we use a generic "system" context for routing
            routing_context_id = "system_agent_registry"
            
            async def stream_registration_event():
                try:
                    from service.websocket_streamer import get_websocket_streamer

                    streamer = await get_websocket_streamer()
                    if not streamer:
                        log_debug(f"‚ö†Ô∏è WebSocket streamer not available for agent registration")
                        return

                    event_data = {
                        "agentName": agent_card.name,
                        "status": "registered",
                        "timestamp": __import__('datetime').datetime.now(timezone.utc).isoformat(),
                        "avatar": "/placeholder.svg?height=32&width=32",
                        "agentPath": getattr(agent_card, 'url', ''),
                    }

                    success = await streamer._send_event("agent_registered", event_data, routing_context_id)
                    if success:
                        log_debug(f"‚úÖ Agent registration event streamed to WebSocket for {agent_card.name}")
                    else:
                        log_debug(f"‚ùå Failed to stream agent registration event to WebSocket for {agent_card.name}")
                except Exception as e:
                    log_debug(f"‚ùå Error streaming agent registration event: {e}")
                    import traceback
                    traceback.print_exc()
            
            # Create background task for WebSocket streaming
            asyncio.create_task(stream_registration_event())
            
        except Exception as e:
            log_debug(f"‚ùå Error setting up agent registration event streaming: {e}")
            pass

    def _display_task_status_update(self, status_text: str, event: TaskCallbackArg):
        """Display a task status update in the UI as a message."""
        log_debug(f"_display_task_status_update called with: {status_text}")
        try:
            # Create a message to display in the UI
            from a2a.types import Message, TextPart, Part
            import uuid
            
            message_id = str(uuid.uuid4())
            context_id = getattr(event, 'contextId', getattr(self._current_task, 'contextId', str(uuid.uuid4())))
            log_debug(f"Created message_id: {message_id}, context_id: {context_id}")
            
            # Create a message with the status update
            status_message = Message(
                messageId=message_id,
                contextId=context_id,
                role="agent",  # Use agent role for status updates
                parts=[Part(root=TextPart(text=f"[Status] {status_text}"))]
            )
            
            # Add to the conversation history through the host manager
            if self._host_manager:
                log_debug(f"Host manager found, getting conversation for context_id: {context_id}")
                # Try to find the conversation with the given context_id first
                conversation = self._host_manager.get_conversation(context_id)
                
                # If not found, try to find any active conversation (fallback)
                if not conversation and self._host_manager.conversations:
                    # Use the most recent active conversation
                    active_conversations = [c for c in self._host_manager.conversations if c.is_active]
                    if active_conversations:
                        conversation = active_conversations[-1]  # Most recent
                        # Create a new message with the correct context_id to match the found conversation
                        status_message = Message(
                            messageId=message_id,
                            contextId=conversation.conversation_id,  # Use the correct contextId
                            role="agent",  # Use agent role for status updates
                            parts=[Part(root=TextPart(text=f"[Status] {status_text}"))]
                        )
                        log_debug(f"Using fallback conversation: {conversation.conversation_id}")
                
                if conversation:
                    conversation.messages.append(status_message)
                    log_debug(f"‚úÖ Added status message to conversation: {status_text}")
                else:
                    log_debug(f"‚ùå No conversation found for context_id: {context_id}")
            else:
                log_debug(f"‚ùå No host manager reference available")
            
            # Also add to local messages list
            if hasattr(self, '_messages'):
                self._messages.append(status_message)
                log_debug(f"Added to local messages list")
            
            # Also add to the conversation state that the UI reads from
            # This ensures the status messages appear in the conversation flow
            if hasattr(self, 'session_contexts') and context_id in self.session_contexts:
                session_context = self.session_contexts[context_id]
                if not hasattr(session_context, 'messages'):
                    session_context.messages = []
                session_context.messages.append(status_message)
                log_debug(f"Added to session context messages")
            
            log_debug(f"Status message created successfully: {status_text}")
            
        except Exception as e:
            log_debug(f"‚ùå Error displaying task status update: {e}")
            import traceback
            traceback.print_exc()

    def _get_status_display_text(self, status) -> str:
        """Convert task status to display text."""
        if hasattr(status, 'state'):
            state = status.state
            if hasattr(state, 'value'):
                state_value = state.value
            else:
                state_value = str(state)
            
            state_map = {
                'submitted': 'Task submitted',
                'working': 'Task working',
                'completed': 'Task completed',
                'failed': 'Task failed',
                'canceled': 'Task canceled',
                'input-required': 'Input required',
                'unknown': 'Task status unknown'
            }
            
            return state_map.get(state_value, f"Task {state_value}")
        
        return "Status update"

    def _extract_message_content(self, message) -> str:
        """Extract text content from a message object or dictionary."""
        try:
            # Handle dictionary format (from HTTP API responses)
            if isinstance(message, dict):
                content = message.get('content', [])
                if isinstance(content, list):
                    for item in content:
                        if isinstance(item, dict) and item.get('type') == 'text':
                            text = item.get('text', {})
                            # Handle nested text object or direct string
                            if isinstance(text, dict):
                                return text.get('value', '')
                            return str(text)
                elif isinstance(content, str):
                    return content
                return ""
            
            # Handle object format (from SDK responses)
            if hasattr(message, 'parts'):
                for part in message.parts:
                    if hasattr(part, 'root') and hasattr(part.root, 'text'):
                        return part.root.text
                    elif hasattr(part, 'text'):
                        return part.text
            elif hasattr(message, 'content'):
                return str(message.content)
            return ""
        except Exception as e:
            log_foundry_debug(f"‚ö†Ô∏è Error extracting message content: {e}")
            return ""

    @staticmethod
    def _extract_text_from_response(obj) -> str:
        """Extract clean text from various A2A response types.
        
        Handles Part, TextPart, DataPart objects and avoids ugly Python repr output.
        """
        if obj is None:
            return ""
        if isinstance(obj, str):
            return obj
        # Handle Part objects with root
        if hasattr(obj, 'root'):
            root = obj.root
            if hasattr(root, 'text'):
                return root.text
            if hasattr(root, 'kind') and root.kind == 'text' and hasattr(root, 'text'):
                return root.text
            if hasattr(root, 'data'):
                return str(root.data)
        # Handle TextPart/DataPart directly
        if hasattr(obj, 'kind'):
            if obj.kind == 'text' and hasattr(obj, 'text'):
                return obj.text
            if obj.kind == 'data' and hasattr(obj, 'data'):
                return str(obj.data)
        # Handle dict
        if isinstance(obj, dict):
            if 'text' in obj:
                return obj['text']
            return str(obj)
        # Handle list - extract text from each item
        if isinstance(obj, list):
            texts = [FoundryHostAgent2._extract_text_from_response(item) for item in obj]
            return "\n".join(t for t in texts if t)
        # Fallback - but avoid ugly repr
        result = str(obj)
        # If it looks like a Python repr, try to extract the text
        if result.startswith("kind='text'") and "text='" in result:
            import re
            match = re.search(r"text='([^']*)'", result)
            if match:
                return match.group(1).replace("\\n", "\n")
        return result

    def get_session_context(self, context_id: str) -> SessionContext:
        log_debug(f"üîç [get_session_context] Called with context_id: {context_id}")
        log_debug(f"üîç [get_session_context] Existing session_contexts keys: {list(self.session_contexts.keys())}")
        
        if context_id not in self.session_contexts:
            # Clear host response tracking for new conversations
            if context_id in self._host_responses_sent:
                self._host_responses_sent.remove(context_id)
            log_debug(f"üîç [get_session_context] Creating NEW SessionContext with contextId={context_id}")
            self.session_contexts[context_id] = SessionContext(contextId=context_id)
        else:
            log_debug(f"üîç [get_session_context] FOUND existing SessionContext for key={context_id}")
            
        return self.session_contexts[context_id]

    async def _search_relevant_memory(self, query: str, context_id: str, agent_name: str = None, top_k: int = 5) -> List[Dict[str, Any]]:
        """Search for relevant memory interactions to provide context to remote agents.
        
        Args:
            query: The search query
            context_id: The context ID for tenant-scoped search
            agent_name: Optional agent name to filter by
            top_k: Number of results to return
        """
        
        try:
            # Extract session_id for tenant isolation
            session_id = get_tenant_from_context(context_id)
            
            # Build filters if agent name is specified
            filters = {}
            if agent_name:
                filters["agent_name"] = agent_name
            
            # Search for similar interactions (tenant-scoped)
            memory_results = await a2a_memory_service.search_similar_interactions(
                query=query,
                session_id=session_id,
                filters=filters,
                top_k=top_k
            )
            
            return memory_results
            
        except Exception as e:
            return []

    def clear_memory_index(self, context_id: str = None) -> bool:
        """Clear stored interactions from the memory index.
        
        Args:
            context_id: If provided, only clear interactions for this session.
                       If None, clears ALL interactions (admin use only).
        """
        try:
            session_id = None
            if context_id:
                session_id = get_tenant_from_context(context_id)
            
            success = a2a_memory_service.clear_all_interactions(session_id=session_id)
            if success:
                return success
            else:
                return False
        except Exception as e:
            return False

    async def _create_memory_artifact(self, memory_results: List[Dict[str, Any]], query: str) -> Optional[Artifact]:
        """Create a memory artifact from search results to send to remote agents"""
        if not memory_results:
            return None
            
        try:
            print(f"Creating memory artifact from {len(memory_results)} interactions")
            
            # Process memory results into structured format
            session_timeline = []
            agent_patterns = {}
            file_references = []
            related_interactions = []
            
            for result in memory_results:
                try:
                    # Parse the stored JSON payloads
                    outbound_payload = json.loads(result.get('outbound_payload', '{}'))
                    inbound_payload = json.loads(result.get('inbound_payload', '{}'))
                    
                    agent_name = result.get('agent_name', 'unknown')
                    timestamp = result.get('timestamp', '')
                    processing_time = result.get('processing_time_seconds', 0)
                    
                    # Extract interaction summary
                    interaction_summary = {
                        "timestamp": timestamp,
                        "agent_name": agent_name,
                        "processing_time_seconds": processing_time,
                        "interaction_type": "host_to_remote" if agent_name != "host_agent" else "user_to_host"
                    }
                    
                    # Extract user request from outbound payload
                    if outbound_payload.get('message', {}).get('parts'):
                        parts = outbound_payload['message']['parts']
                        text_parts = [p.get('text', '') for p in parts if p.get('text')]
                        if text_parts:
                            interaction_summary["user_request"] = text_parts[0][:200] + "..." if len(text_parts[0]) > 200 else text_parts[0]
                    
                    # Extract agent response from inbound payload
                    if inbound_payload.get('artifacts'):
                        artifacts = inbound_payload['artifacts']
                        for artifact in artifacts:
                            if artifact.get('parts'):
                                text_parts = [p.get('text', '') for p in artifact['parts'] if p.get('text')]
                                if text_parts:
                                    interaction_summary["agent_response"] = text_parts[0][:200] + "..." if len(text_parts[0]) > 200 else text_parts[0]
                                    break
                    
                    session_timeline.append(interaction_summary)
                    
                    # Track agent patterns
                    if agent_name not in agent_patterns:
                        agent_patterns[agent_name] = {
                            "interaction_count": 0,
                            "avg_processing_time": 0,
                            "common_requests": []
                        }
                    
                    agent_patterns[agent_name]["interaction_count"] += 1
                    agent_patterns[agent_name]["avg_processing_time"] = (
                        agent_patterns[agent_name]["avg_processing_time"] + processing_time
                    ) / 2
                    
                    # Extract file references
                    if outbound_payload.get('message', {}).get('parts'):
                        for part in outbound_payload['message']['parts']:
                            if part.get('file') and 'artifact_uri' in str(part.get('file', {})):
                                file_ref = {
                                    "timestamp": timestamp,
                                    "agent_name": agent_name,
                                    "file_info": part['file']
                                }
                                file_references.append(file_ref)
                    
                    # Store complete interaction for reference
                    related_interactions.append({
                        "timestamp": timestamp,
                        "agent_name": agent_name,
                        "outbound_summary": str(outbound_payload)[:500] + "...",
                        "inbound_summary": str(inbound_payload)[:500] + "..."
                    })
                    
                except Exception as e:
                    continue
            
            # Create the memory artifact data
            memory_data = {
                "search_query": query,
                "search_timestamp": datetime.now(timezone.utc).strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + 'Z',
                "total_results": len(memory_results),
                "session_timeline": sorted(session_timeline, key=lambda x: x.get('timestamp', '')),
                "agent_patterns": agent_patterns,
                "file_references": file_references,
                "related_interactions": related_interactions[:3]  # Limit to top 3 most relevant
            }
            
            # Create the artifact
            memory_artifact = Artifact(
                name="relevant_memory",
                description=f"Historical context and patterns relevant to: {query}",
                parts=[
                    DataPart(data=memory_data)
                ]
            )
            
            print(f"‚úÖ Created memory artifact with {len(session_timeline)} timeline entries")
            print(f"‚úÖ Memory artifact includes {len(agent_patterns)} agent patterns")
            print(f"‚úÖ Memory artifact includes {len(file_references)} file references")
            
            return memory_artifact
            
        except Exception as e:
            import traceback
            print(f"‚ùå Error creating memory artifact: {e}")
            print(f"‚ùå Traceback: {traceback.format_exc()}")
            return None

    async def _evaluate_task_completion(self, original_request: str, task_response: Task, agent_name: str) -> Dict[str, Any]:
        """Use direct Azure OpenAI call to evaluate if a task was truly completed successfully"""
        
        try:
            # Extract response content from LATEST task artifact only (avoid evaluating retry history)
            response_content = ""
            if task_response.artifacts:
                # Get only the most recent artifact (last one in the list)
                latest_artifact = task_response.artifacts[-1]
                for part in latest_artifact.parts:
                    if hasattr(part, 'root') and part.root.kind == 'text':
                        response_content = part.root.text  # Only the latest response
                        break  # Stop after first text part of latest artifact
            
            # If no artifacts, check status message
            if not response_content and task_response.status.message:
                if hasattr(task_response.status.message, 'parts'):
                    for part in task_response.status.message.parts:
                        if hasattr(part, 'root') and part.root.kind == 'text':
                            response_content += part.root.text + "\n"
            
            evaluation_prompt = f"""Evaluate ONLY this specific agent response:

Request: "{original_request}"
Agent: {agent_name}  
Response: "{response_content.strip()}"

SUCCESS criteria (mark as true):
- Agent completed the requested task successfully
- Agent provided a confirmation, result, or request ID
- Agent asked for truly missing required information
- Response directly addresses the user's request

FAILURE criteria (mark as false):
- Agent completely ignored the request
- Agent gave totally irrelevant information
- Agent had major errors or contradictions

IMPORTANT: If the agent provided ANY form of completion, confirmation, or request ID, it's SUCCESS.

Answer with just JSON:

{{"is_successful": true/false, "reason": "brief reason"}}"""

            print(f"Making direct Azure OpenAI call for evaluation...")
            
            # Use the same Azure AI Foundry approach as the rest of the system
            from openai import AsyncAzureOpenAI
            from azure.identity import get_bearer_token_provider
            
            # Use the same endpoint and credentials as the main agent
            azure_endpoint = os.getenv("AZURE_CONTENT_UNDERSTANDING_ENDPOINT") or os.getenv("AZURE_AI_SERVICE_ENDPOINT")
            model_name = os.environ.get("AZURE_AI_AGENT_MODEL_DEPLOYMENT_NAME")
            
            if not azure_endpoint:
                print(f"‚ùå Missing Azure endpoint configuration")
                return {"is_successful": True, "reason": "Missing endpoint config"}
            
            if not model_name:
                print(f"‚ùå Missing model deployment name")
                return {"is_successful": True, "reason": "Missing model config"}
            
            # Use DefaultAzureCredential like the rest of the system
            token_provider = get_bearer_token_provider(
                self.credential, 
                "https://cognitiveservices.azure.com/.default"
            )
            
            # Create Azure OpenAI client with same auth as main system
            client = AsyncAzureOpenAI(
                azure_endpoint=azure_endpoint,
                azure_ad_token_provider=token_provider,
                api_version="2024-02-15-preview"
            )
            
            # Make direct chat completion call
            response = await client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": "You are a task completion evaluator. Analyze agent responses and return JSON evaluations."},
                    {"role": "user", "content": evaluation_prompt}
                ],
                max_tokens=200,
                temperature=0.1
            )
            
            evaluation_response = response.choices[0].message.content
            print(f"Direct OpenAI evaluation response: {evaluation_response}")
            
            # Parse JSON response
            try:
                import json
                # Extract JSON from response (in case there's extra text)
                start_idx = evaluation_response.find('{')
                end_idx = evaluation_response.rfind('}') + 1
                if start_idx >= 0 and end_idx > start_idx:
                    json_str = evaluation_response[start_idx:end_idx]
                    evaluation_result = json.loads(json_str)
                    print(f"‚úÖ Parsed evaluation result: {evaluation_result}")
                    
                    # Add default fields if missing from simplified response
                    if "retry_suggestion" not in evaluation_result:
                        evaluation_result["retry_suggestion"] = "Try rephrasing your request or provide more details"
                    if "alternative_agent" not in evaluation_result:
                        evaluation_result["alternative_agent"] = None
                    if "needs_clarification" not in evaluation_result:
                        evaluation_result["needs_clarification"] = None
                        
                    return evaluation_result
                else:
                    print(f"‚ùå No valid JSON found in evaluation response")
                    return {"is_successful": True, "reason": "Could not parse evaluation"}
                    
            except json.JSONDecodeError as e:
                print(f"‚ùå JSON parsing error: {e}")
                return {"is_successful": True, "reason": "Could not parse evaluation"}
                
        except Exception as e:
            print(f"‚ùå Error during task evaluation: {e}")
            # Default to successful if evaluation fails to avoid blocking user
            return {"is_successful": True, "reason": f"Evaluation error: {str(e)}"}

    async def _log_evaluation_result(self, original_request: str, task_response: Task, agent_name: str):
        """Background evaluation for monitoring - doesn't affect user experience"""
        try:
            print(f"[BACKGROUND] Running evaluation for monitoring...")
            evaluation = await self._evaluate_task_completion(original_request, task_response, agent_name)
            
            # Just log the results for monitoring/analytics
            if evaluation.get("is_successful", True):
                print(f"‚úÖ [BACKGROUND] Task evaluation: SUCCESS - {evaluation.get('reason', '')}")
            else:
                print(f"‚ö†Ô∏è [BACKGROUND] Task evaluation: FAILED - {evaluation.get('reason', '')}")
                print(f"üí° [BACKGROUND] Suggestion: {evaluation.get('retry_suggestion', 'None')}")
                
            # Could store results for analytics dashboard
            # await self._store_evaluation_analytics(original_request, task_response, agent_name, evaluation)
            
        except Exception as e:
            print(f"‚ùå [BACKGROUND] Evaluation error (non-blocking): {e}")
            # Background evaluation errors don't affect user experience



    def _extract_failure_reason(self, failed_task: Task) -> str:
        """Extract meaningful failure reason from failed task"""
        if failed_task.status.message:
            if hasattr(failed_task.status.message, 'parts'):
                for part in failed_task.status.message.parts:
                    if hasattr(part, 'root') and part.root.kind == 'text':
                        return part.root.text
            elif hasattr(failed_task.status.message, 'text'):
                return failed_task.status.message.text
        
        return "Task failed with unknown error"

    def _generate_failure_recovery_options(self, failed_task: Task, agent_name: str, original_request: str) -> List[str]:
        """Generate helpful recovery options for the user"""
        failure_reason = self._extract_failure_reason(failed_task)
        
        options = []
        
        # Always offer to try a different approach
        options.append("Try a different approach to your request")
        
        # Suggest alternative agents if available
        available_agents = [name for name in self.remote_agent_connections.keys() if name != agent_name]
        if available_agents:
            options.append(f"Use a different specialist agent ({', '.join(available_agents[:2])})")
        
        # Suggest breaking down the request
        if len(original_request.split()) > 5:
            options.append("Break your request into smaller, simpler steps")
        
        # Suggest rephrasing
        options.append("Rephrase your request with more details")
        
        # Offer to try again later if it seems like a temporary issue
        if any(term in failure_reason.lower() for term in ['timeout', 'rate limit', 'unavailable', 'busy']):
            options.append("Try again in a few minutes")
        
        return options

    @staticmethod
    def _parse_retry_after_from_task(task) -> int:
        """
        Parse retry-after delay from a failed task's error message.
        
        Looks for rate limit messages and extracts the suggested wait time.
        Returns 0 if no rate limit is detected, otherwise the wait time in seconds.
        """
        try:
            if hasattr(task, 'status') and task.status and getattr(task.status, 'message', None):
                parts = getattr(task.status.message, 'parts', []) or []
                import re
                for p in parts:
                    txt = None
                    if hasattr(p, 'root') and hasattr(p.root, 'text') and p.root.text:
                        txt = p.root.text
                    elif hasattr(p, 'text') and p.text:
                        txt = p.text
                    if not txt:
                        continue
                    lower = txt.lower()
                    if 'rate limit' in lower or 'rate_limit_exceeded' in lower:
                        m = re.search(r"try again in\s+(\d+)\s*seconds", lower)
                        return int(m.group(1)) if m else 15
        except Exception:
            pass
        return 0

    @staticmethod
    def _infer_file_role(explicit_role: Optional[str], name_hint: Optional[str]) -> Optional[str]:
        """
        Infer file role (base, mask, overlay) from filename for image editing workflows.
        
        Role assignment rules:
        1. If explicit_role is provided, use it
        2. Generated/edited files (generated_*, edit_*) get no role (kept separate)
        3. Files with "mask" in name ‚Üí mask role
        4. Files with "_base" in name ‚Üí base role  
        5. Image files and logos ‚Üí overlay role
        6. Everything else ‚Üí no role
        
        Args:
            explicit_role: Role explicitly set on the file
            name_hint: Filename to infer role from
            
        Returns:
            Role string (base, mask, overlay) or None
        """
        if explicit_role:
            return str(explicit_role).lower()

        if not name_hint:
            return None

        name_lower = str(name_hint).lower()

        # Generated/edited outputs get no role - kept as separate artifacts
        if "generated_" in name_lower or "edit_" in name_lower:
            return None

        # Mask files
        if "mask" in name_lower or name_lower.endswith("-mask.png") or name_lower.endswith("_mask.png"):
            return "mask"

        # Base files
        if name_lower.endswith("-base.png") or name_lower.endswith("_base.png") or "_base" in name_lower:
            return "base"

        # Image files default to overlay
        image_exts = (".png", ".jpg", ".jpeg", ".gif", ".bmp", ".webp", ".tif", ".tiff", ".heic", ".heif", ".jfif", ".apng")
        if name_lower.endswith(image_exts) or "logo" in name_lower:
            return "overlay"

        return None

    @staticmethod
    def _normalize_uri(value: Optional[str]) -> Optional[str]:
        """
        Normalize a URI for comparison/deduplication.
        
        Strips whitespace, removes query parameters, and lowercases.
        """
        if not value:
            return None
        normalized = str(value).strip()
        if not normalized:
            return None
        base, _, _ = normalized.partition("?")
        return base.lower()

    @staticmethod
    def _apply_role_to_part(part: Any, role: Optional[str]) -> None:
        """
        Apply a role to a part's metadata for image editing workflows.
        
        Handles both DataPart and FilePart structures.
        """
        if not role:
            return
        normalized_role = str(role).lower()
        target = part.root if isinstance(part, Part) else part
        if isinstance(target, DataPart) and isinstance(target.data, dict):
            target.data["role"] = normalized_role
            meta = target.data.get("metadata") or {}
            meta["role"] = normalized_role
            target.data["metadata"] = meta
        elif isinstance(target, FilePart):
            meta = target.metadata or {}
            meta["role"] = normalized_role
            target.metadata = meta

    @staticmethod
    def _build_mask_parts(
        file_id: str,
        mime_type: str,
        artifact_uri: Optional[str],
        artifact_info: dict,
        file_bytes: Optional[bytes],
        artifact_response: Optional[DataPart],
    ) -> tuple[DataPart, FilePart]:
        """
        Build DataPart + FilePart for a mask file artifact.
        
        Consolidates mask-specific metadata and file part construction
        that was previously duplicated in convert_part.
        
        Args:
            file_id: Filename/identifier
            mime_type: MIME type of the file
            artifact_uri: URI where artifact is stored (if available)
            artifact_info: Artifact metadata dict from save_artifact
            file_bytes: Raw file bytes (fallback if no URI)
            artifact_response: Existing DataPart from save_artifact (if any)
            
        Returns:
            Tuple of (DataPart with metadata, FilePart with URI or bytes)
        """
        # Build or update the metadata DataPart
        if isinstance(artifact_response, DataPart) and hasattr(artifact_response, 'data'):
            artifact_response.data['description'] = artifact_response.data.get('description', 'image mask attachment')
            artifact_response.data['skip-document-processing'] = True
            artifact_response.data['role'] = 'mask'
            metadata = artifact_response.data.get('metadata') or {}
            metadata['role'] = 'mask'
            artifact_response.data['metadata'] = metadata
            mask_metadata_part = artifact_response
            artifact_uri = artifact_uri or artifact_response.data.get('artifact-uri')
        else:
            # Create new DataPart with mask metadata
            mask_metadata_part = DataPart(data={
                'artifact-id': artifact_info.get('artifact_id') or str(uuid.uuid4()),
                'artifact-uri': artifact_uri or artifact_info.get('artifact_uri'),
                'storage-type': artifact_info.get('storage_type', 'unknown'),
                'file-name': artifact_info.get('file_name') or file_id,
                'description': 'image mask attachment',
                'skip-document-processing': True,
                'role': 'mask',
                'metadata': {'role': 'mask'},
            })

        # Build the FilePart - prefer URI, fallback to embedded bytes
        if artifact_uri:
            mask_file_part = FilePart(
                kind="file",
                file=FileWithUri(
                    name=file_id,
                    mimeType=mime_type,
                    uri=artifact_uri,
                    role="mask",
                ),
            )
        else:
            mask_file_part = FilePart(
                kind="file",
                file=FileWithBytes(
                    name=file_id,
                    mimeType=mime_type,
                    bytes=file_bytes or b'',
                    role="mask",
                )
            )

        return mask_metadata_part, mask_file_part

    @staticmethod
    def _store_parts_in_session(tool_context: Any, *parts: Any) -> None:
        """
        Store processed parts in session context for later access.
        
        Parts are appended to session_context._latest_processed_parts list.
        This is used to make file artifacts accessible to remote agents.
        """
        session_context = getattr(tool_context, "state", None)
        if session_context is None:
            return
        latest_parts = getattr(session_context, "_latest_processed_parts", None)
        if latest_parts is None:
            latest_parts = []
            setattr(session_context, "_latest_processed_parts", latest_parts)
        for p in parts:
            if p is not None:
                latest_parts.append(p)

    @staticmethod
    def _wrap_item_for_agent(item: Any) -> List[Part]:
        """
        Wrap any item as A2A Parts for agent communication.
        
        Handles Part, DataPart, FilePart, TextPart, str, dict, and other types.
        For DataParts with artifact data, also creates FilePart for consistency.
        """
        wrapped: List[Part] = []

        if isinstance(item, Part):
            wrapped.append(item)
        elif isinstance(item, DataPart):
            wrapped.append(Part(root=item))
            # Also convert to FilePart for consistent handling
            if isinstance(item.data, dict):
                uri = extract_uri(item)
                if uri:
                    file_part = convert_artifact_dict_to_file_part(item)
                    if file_part:
                        wrapped.append(Part(root=file_part))
                if item.data.get("extracted_content"):
                    wrapped.append(Part(root=TextPart(text=str(item.data["extracted_content"]))))
        elif isinstance(item, FilePart):
            wrapped.append(Part(root=item))
        elif isinstance(item, TextPart):
            wrapped.append(Part(root=item))
        elif isinstance(item, str):
            wrapped.append(Part(root=TextPart(text=item)))
        elif isinstance(item, dict):
            uri = extract_uri(item)
            if uri:
                file_part = convert_artifact_dict_to_file_part(item)
                if file_part:
                    wrapped.append(Part(root=file_part))
            else:
                wrapped.append(Part(root=DataPart(data=item)))
        elif item is not None:
            wrapped.append(Part(root=TextPart(text=str(item))))

        return wrapped

    @staticmethod
    def _flatten_nested(items: Iterable[Any]) -> Iterable[Any]:
        """Flatten nested lists/tuples/sets into a single iterable."""
        def _do_flatten(items):
            for item in items:
                if isinstance(item, (list, tuple, set)):
                    yield from _do_flatten(item)
                else:
                    yield item
        return _do_flatten(items)

    def _build_image_edit_guidance(self, processed_parts: List[Any]) -> Optional[str]:
        """
        Scan processed parts for base/mask image attachments and build
        guidance text for image editing workflows.
        
        Returns guidance string if base attachment found, None otherwise.
        """
        has_base = False
        has_mask = False
        base_filenames: List[str] = []
        mask_filenames: List[str] = []

        for result in self._flatten_nested(processed_parts):
            candidate_part = None
            candidate_data: Optional[Dict[str, Any]] = None

            if isinstance(result, DataPart) and isinstance(result.data, dict):
                candidate_data = result.data
            elif isinstance(result, FilePart):
                candidate_part = result
            elif isinstance(result, Part):
                inner = getattr(result, "root", None)
                if isinstance(inner, DataPart) and isinstance(inner.data, dict):
                    candidate_data = inner.data
                elif isinstance(inner, FilePart):
                    candidate_part = inner

            if candidate_data:
                role_val = (candidate_data.get("role") or (candidate_data.get("metadata") or {}).get("role") or "").lower()
                if role_val == "base":
                    has_base = True
                    name_hint = candidate_data.get("file-name") or candidate_data.get("name")
                    if name_hint:
                        base_filenames.append(str(name_hint))
                if role_val == "mask":
                    has_mask = True
                    name_hint = candidate_data.get("file-name") or candidate_data.get("name")
                    if name_hint:
                        mask_filenames.append(str(name_hint))

            if candidate_part:
                role_attr = getattr(candidate_part.file, "role", None)
                part_name = getattr(candidate_part.file, "name", "")
                name_attr = part_name.lower()
                role_lower = str(role_attr).lower() if role_attr else ""
                if role_lower == "base" or name_attr.endswith("_base.png"):
                    has_base = True
                    if part_name:
                        base_filenames.append(part_name)
                if role_lower == "mask" or "_mask" in name_attr or "-mask" in name_attr:
                    has_mask = True
                    if part_name:
                        mask_filenames.append(part_name)

        if not has_base:
            return None

        lines = [
            "IMPORTANT: Treat this request as an image edit using the provided attachments.",
            "Reuse the supplied base image exactly; do not regenerate a new scene or subject.",
        ]
        if base_filenames:
            lines.append(f"Base image attachment(s): {', '.join(sorted(set(base_filenames)))}.")
        if has_mask:
            lines.append("Apply the requested changes strictly within the transparent region of the provided mask and leave all other pixels unchanged.")
            if mask_filenames:
                lines.append(f"Mask attachment(s): {', '.join(sorted(set(mask_filenames)))} (must include transparency).")
        else:
            lines.append("Apply the requested changes directly to the supplied base image only.")

        return "\n".join(lines)

    async def _handle_pending_input_agent(
        self,
        session_context: Any,
        message_parts: list,
        context_id: str,
        event_logger: Any
    ) -> Optional[list]:
        """
        Handle Human-In-The-Loop (HITL) routing when an agent is waiting for input_required response.
        
        If an agent previously returned input_required, route the user's response directly
        to that agent instead of going through normal orchestration. Also handles resuming
        paused workflows after the agent completes.
        
        Args:
            session_context: The session context with pending agent state
            message_parts: The user's message parts
            context_id: The context/session ID
            event_logger: Event logger for tracking
            
        Returns:
            Response list if HITL was handled, None if no pending agent (fall through to normal flow)
        """
        if not session_context.pending_input_agent:
            return None
            
        pending_agent = session_context.pending_input_agent
        pending_task_id = session_context.pending_input_task_id
        pending_workflow = session_context.pending_workflow
        pending_workflow_outputs = session_context.pending_workflow_outputs or []
        
        log_info(f"üîÑ [HITL] Found pending input_required agent: '{pending_agent}' (task_id: {pending_task_id})")
        log_info(f"üîÑ [HITL] Routing user response directly to waiting agent instead of orchestration")
        if pending_workflow:
            log_info(f"üîÑ [HITL] Workflow will resume after agent completes ({len(pending_workflow_outputs)} outputs collected)")
        
        # Clear the pending state before routing
        session_context.pending_input_agent = None
        session_context.pending_input_task_id = None
        
        # Extract user message from parts
        hitl_user_message = ""
        for part in message_parts:
            if hasattr(part, 'root') and part.root.kind == 'text':
                hitl_user_message = part.root.text
                break
        
        # Helper to extract clean text from responses
        def clean_response(resp):
            if isinstance(resp, list):
                return [self._extract_text_from_response(r) for r in resp]
            return [self._extract_text_from_response(resp)]
        
        try:
            tool_context = DummyToolContext(session_context, self._azure_blob_client)
            hitl_response = await self.send_message(
                agent_name=pending_agent,
                message=hitl_user_message,
                tool_context=tool_context,
                suppress_streaming=False
            )
            log_info(f"üîÑ [HITL] Response from agent '{pending_agent}': {str(hitl_response)[:200]}...")
            
            # Check if agent is STILL requesting input (multi-turn HITL)
            if session_context.pending_input_agent:
                log_info(f"üîÑ [HITL] Agent still requires more input - staying paused")
                return clean_response(hitl_response)
            
            # Agent completed! Check if we need to resume a paused workflow
            if pending_workflow:
                log_info(f"‚ñ∂Ô∏è [HITL] Agent completed - RESUMING WORKFLOW")
                
                # Add this agent's response to the collected outputs
                hitl_outputs = clean_response(hitl_response)
                all_outputs = pending_workflow_outputs + hitl_outputs
                
                # Clear workflow pause state
                session_context.pending_workflow = None
                session_context.pending_workflow_outputs = []
                session_context.pending_workflow_user_message = None
                
                log_info(f"‚ñ∂Ô∏è [HITL] Resuming workflow with {len(all_outputs)} total outputs")
                
                # Continue the workflow from where we left off
                remaining_outputs = await self._agent_mode_orchestration_loop(
                    user_message="Continue the workflow. The previous step has completed.",
                    context_id=context_id,
                    session_context=session_context,
                    event_logger=event_logger,
                    workflow=pending_workflow
                )
                
                # Clean any remaining outputs too
                clean_remaining = [self._extract_text_from_response(out) for out in remaining_outputs]
                return all_outputs + clean_remaining
            
            return clean_response(hitl_response)
            
        except Exception as e:
            log_error(f"üîÑ [HITL] Error routing to pending agent '{pending_agent}': {e}")
            import traceback
            traceback.print_exc()
            # Return None to fall through to normal processing
            return None

    @staticmethod
    def _load_file_bytes(file_part: Any, context_id: Optional[str] = None) -> tuple[Optional[bytes], Optional[str]]:
        """
        Load file bytes from various sources: uploads directory, inline bytes, or HTTP URI.
        
        Args:
            file_part: The file object from FilePart.root.file
            context_id: Context ID for session-scoped directory lookup
            
        Returns:
            Tuple of (file_bytes, error_message). One will be None.
        """
        import os
        
        file_id = getattr(file_part, 'name', 'unknown')
        
        # Strategy 1: Load from /uploads/ URI
        uri = getattr(file_part, 'uri', None)
        if uri and str(uri).startswith('/uploads/'):
            file_uuid = uri.split('/')[-1]
            upload_dir = "uploads"
            
            # Extract session_id for tenant isolation
            session_id = None
            if context_id and '::' in context_id:
                session_id = context_id.split('::')[0]
            
            try:
                # Try session-scoped directory first
                if session_id:
                    session_upload_dir = os.path.join(upload_dir, session_id)
                    if os.path.exists(session_upload_dir):
                        for filename in os.listdir(session_upload_dir):
                            if filename.startswith(file_uuid):
                                file_path = os.path.join(session_upload_dir, filename)
                                with open(file_path, 'rb') as f:
                                    return f.read(), None
                
                # Fall back to flat directory (legacy)
                if os.path.exists(upload_dir):
                    for filename in os.listdir(upload_dir):
                        if os.path.isdir(os.path.join(upload_dir, filename)):
                            continue
                        if filename.startswith(file_uuid):
                            file_path = os.path.join(upload_dir, filename)
                            with open(file_path, 'rb') as f:
                                return f.read(), None
                
                return None, f"Could not find uploaded file {file_id}"
            except Exception as e:
                return None, f"Could not read uploaded file {file_id}: {e}"
        
        # Strategy 2: Inline base64 or raw bytes
        if hasattr(file_part, 'bytes') and file_part.bytes:
            try:
                if isinstance(file_part.bytes, str):
                    return base64.b64decode(file_part.bytes), None
                return file_part.bytes, None
            except Exception as e:
                return None, f"Failed to decode file {file_id}: {e}"
        
        # Strategy 3: HTTP/HTTPS URI download
        if uri and str(uri).lower().startswith(("http://", "https://")):
            try:
                import httpx
                with httpx.Client(timeout=60.0, follow_redirects=True) as client:
                    resp = client.get(uri)
                    resp.raise_for_status()
                    return resp.content, None
            except Exception as e:
                return None, f"Could not download file {file_id}: {e}"
        
        return None, f"No file data found for {file_id}"

    def _get_retry_count(self, session_context: SessionContext) -> int:
        """Get current retry count for this session"""
        return session_context.retry_count

    def _increment_retry_count(self, session_context: SessionContext):
        """Increment retry count for this session"""
        session_context.retry_count += 1

    def _reset_retry_count(self, session_context: SessionContext):
        """Reset retry count after successful completion"""
        session_context.retry_count = 0

    def _update_last_host_turn(
        self,
        session_context: SessionContext,
        agent_name: str,
        responses: List[Any],
    ) -> None:
        """Cache the most recent host-side turn so we can hand it to the next agent."""
        if not self.include_last_host_turn or not responses:
            return

        text_chunks: List[str] = []
        for item in responses:
            # Extract text from various response types
            if isinstance(item, str):
                text = item.strip()
                if text:
                    text_chunks.append(text)
            elif isinstance(item, TextPart):
                text = (item.text or "").strip()
                if text:
                    text_chunks.append(text)
            elif hasattr(item, 'root') and hasattr(item.root, 'text'):
                # Part wrapper with TextPart inside
                text = (item.root.text or "").strip()
                if text:
                    text_chunks.append(text)
            elif isinstance(item, DataPart) and isinstance(item.data, dict):
                # Try to extract meaningful text from DataPart
                if 'text' in item.data:
                    text = str(item.data['text']).strip()
                    if text:
                        text_chunks.append(text)
            elif hasattr(item, 'root') and hasattr(item.root, 'data') and isinstance(item.root.data, dict):
                # Part wrapper with DataPart inside
                if 'text' in item.root.data:
                    text = str(item.root.data['text']).strip()
                if text:
                    text_chunks.append(text)

        if not text_chunks:
            return

        combined = "\n\n".join(text_chunks)
        if len(combined) > self.last_host_turn_max_chars:
            combined = combined[: self.last_host_turn_max_chars] + "..."

        session_context.last_host_turn_text = combined
        session_context.last_host_turn_agent = agent_name
        history = list(getattr(session_context, "host_turn_history", []))
        history.append({"agent": agent_name, "text": combined})
        if len(history) > self.last_host_turns:
            history = history[-self.last_host_turns :]
        session_context.host_turn_history = history
        
        log_debug(f"üìù [Context] Updated host_turn_history with response from {agent_name} ({len(combined)} chars)")
        logger.debug(
            "[A2A] Cached host turn for agent %s (len=%d, history=%d)",
            agent_name,
            len(combined),
            len(getattr(session_context, "host_turn_history", [])),
        )

    async def send_message(
        self,
        agent_name: str,
        message: str,
        tool_context: Any,
        suppress_streaming: bool = True,
    ):
        """
        Send a message to a remote agent and handle the A2A protocol response.
        
        This is the core method for agent-to-agent communication. It:
        - Adds conversation context from previous agent interactions
        - Searches semantic memory for relevant past conversations
        - Handles A2A Task protocol responses (completed, failed, input_required, etc.)
        - Implements retry logic for rate limits and transient failures
        - Converts agent responses into formats usable by other agents
        - Stores all interactions in memory for future retrieval
        
        Args:
            agent_name: Name of the target remote agent
            message: The message/task to send to the agent
            tool_context: Context object with session state and artifact storage
            suppress_streaming: If True, don't stream to main chat (used for sub-agent calls)
                               If False, stream to UI (used for direct user-facing responses)
        
        Returns:
            List of response parts (text, files, data) from the remote agent
        """
        """Sends a task using the A2A protocol with parallel execution support.
        
        This version is optimized for parallel agent execution by:
        1. Preserving shared context IDs across parallel calls
        2. Reducing synchronous pre-await work
        3. Simplifying event processing
        """
        log_debug(f"[SEND_MESSAGE] ENTERING send_message for agent: {agent_name}")
        with tracer.start_as_current_span("send_message") as span:
            span.set_attribute("agent_name", agent_name)
            span.set_attribute("suppress_streaming", suppress_streaming)
            session_context = tool_context.state  # Should be SessionContext
            if not isinstance(session_context, SessionContext):
                raise TypeError(
                    "tool_context.state must be a SessionContext instance for A2A-compliant send_message"
                )

            # CRITICAL: DO NOT generate new contextId - it comes from the session_context
            # The session_context already has the correct contextId from the HTTP request
            import uuid
            if not session_context.task_id:
                session_context.task_id = str(uuid.uuid4())
            if not session_context.message_id:
                session_context.message_id = str(uuid.uuid4())

            # Simplified telemetry - reduced synchronous work for parallel execution
            span.set_attribute("operation.type", "agent_delegation")
            span.set_attribute("agent_name", agent_name)
            span.set_attribute("context_id", session_context.contextId)
            
            # Check if agent exists (quick validation)
            if agent_name not in self.remote_agent_connections:
                available_agents = list(self.remote_agent_connections.keys())
                raise ValueError(f"Agent '{agent_name}' not found. Available agents: {available_agents}")
            
            client = self.remote_agent_connections[agent_name]
            if not client:
                raise ValueError(f"Client not available for {agent_name}")

            # Add conversation context to message (this can be optimized further)
            contextualized_message = await self._add_context_to_message(
                message,
                session_context,
                thread_id=None,
                target_agent_name=agent_name,
            )

            # Respect any active cooldown for this agent due to throttling
            try:
                cool_until = session_context.agent_cooldowns.get(agent_name, 0)
                now_ts = time.time()
                if cool_until and cool_until > now_ts:
                    wait_s = min(60, max(0, int(cool_until - now_ts)))
                    if wait_s > 0:
                        asyncio.create_task(self._emit_granular_agent_event(agent_name, f"throttled; waiting {wait_s}s", session_context.contextId))
                        await asyncio.sleep(wait_s)
            except Exception:
                pass
            
            # Use per-agent taskId only if the previous task for this agent is not in a terminal state
            taskId = None
            last_task_id = session_context.agent_task_ids.get(agent_name)
            last_task_state = session_context.agent_task_states.get(agent_name)
            # Continue same task only if we believe it's in-progress or awaiting input
            if last_task_id and last_task_state in {"working", "submitted", "input-required"}:
                taskId = last_task_id
            contextId = session_context.contextId
            messageId = str(uuid.uuid4())  # Generate fresh message ID for this specific call

            prepared_parts: List[Any] = [Part(root=TextPart(text=contextualized_message))]
            session_parts = []
            if hasattr(session_context, "_latest_processed_parts"):
                session_parts = getattr(session_context, "_latest_processed_parts", []) or []
            
            # DEBUG: Log what we're about to send
            log_foundry_debug(f"Before sending to {agent_name}:")
            log_debug(f"  ‚Ä¢ _latest_processed_parts exists: {hasattr(session_context, '_latest_processed_parts')}")
            log_debug(f"  ‚Ä¢ session_parts count: {len(session_parts)}")
            log_debug(f"  ‚Ä¢ agent_mode: {getattr(session_context, 'agent_mode', False)}")

            if session_parts:
                log_debug(f"üì¶ Prepared {len(session_parts)} parts for remote agent {agent_name} (context {contextId})")
                for idx, prepared_part in enumerate(session_parts):
                    part_root = getattr(prepared_part, "root", prepared_part)
                    kind = getattr(part_root, "kind", getattr(part_root, "type", type(part_root).__name__))
                    log_debug(f"  ‚Ä¢ Prepared part {idx}: kind={kind}")
                    
                    # Enhanced file part logging with role information
                    if isinstance(part_root, FilePart) and hasattr(part_root, "file"):
                        file_obj = part_root.file
                        file_name = getattr(file_obj, 'name', 'unknown')
                        file_uri = getattr(file_obj, 'uri', 'no-uri')
                        # Check role in metadata (primary location for remote agents)
                        file_role = (part_root.metadata or {}).get("role", None) if hasattr(part_root, "metadata") else None
                        log_debug(f"    ‚Üí FilePart: name={file_name} role={file_role or 'no-role'}")
                        log_debug(f"    ‚Üí URI: {file_uri[:80]}..." if len(file_uri) > 80 else f"    ‚Üí URI: {file_uri}")
                    elif hasattr(part_root, "file") and getattr(part_root.file, "uri", None):
                        log_debug(f"    ‚Üí file name={getattr(part_root.file, 'name', 'unknown')} uri={part_root.file.uri}")
                    
                    # Enhanced DataPart logging with role information
                    if isinstance(part_root, DataPart) and getattr(part_root, "data", None):
                        data_keys = list(part_root.data.keys()) if isinstance(part_root.data, dict) else []
                        log_debug(f"    ‚Üí data keys={data_keys}")
                        if isinstance(part_root.data, dict):
                            role = part_root.data.get("role", "no-role")
                            artifact_uri = part_root.data.get("artifact-uri", "")
                            file_name = part_root.data.get("file-name", "unknown")
                            if role != "no-role" or artifact_uri:
                                log_debug(f"    ‚Üí DataPart file: {file_name} role={role}")
                                if artifact_uri:
                                    log_debug(f"    ‚Üí Artifact URI: {artifact_uri[:80]}..." if len(artifact_uri) > 80 else f"    ‚Üí Artifact URI: {artifact_uri}")

                    if isinstance(prepared_part, Part):
                        prepared_parts.append(prepared_part)
                    elif isinstance(prepared_part, (TextPart, DataPart, FilePart)):
                        prepared_parts.append(Part(root=prepared_part))
                    elif isinstance(prepared_part, dict):
                        prepared_parts.append(Part(root=DataPart(data=prepared_part)))
                    elif hasattr(prepared_part, "root"):
                        prepared_parts.append(Part(root=prepared_part.root))
                    elif prepared_part is not None:
                        prepared_parts.append(Part(root=TextPart(text=str(prepared_part))))

            request = MessageSendParams(
                message=Message(
                    role='user',
                    parts=prepared_parts,
                    message_id=messageId,
                    context_id=contextId,  # Use snake_case as per A2A SDK
                    task_id=taskId,
                ),
                configuration=MessageSendConfiguration(
                    acceptedOutputModes=['text', 'text/plain', 'image/png'],
                ),
            )
            
            log_debug(f"üöÄ [PARALLEL] Calling agent: {agent_name} with context: {contextId}")
            log_debug(f"üîç [DEBUG] Message object context_id being sent: {request.message.context_id}")
            log_debug(f"üîç [DEBUG] Full message dict: {request.message.model_dump()}")
            
            # Track start time for processing duration
            start_time = time.time()
            
            # Create a user-friendly query preview for status messages
            # Truncate long queries to keep status messages clean
            query_preview = message[:60] + "..." if len(message) > 60 else message
            # Clean up the query preview - remove newlines and extra whitespace
            query_preview = " ".join(query_preview.split())
            
            # ========================================================================
            # EMIT WORKFLOW MESSAGE: Clear "Calling agent" message for workflow panel
            # ========================================================================
            asyncio.create_task(self._emit_granular_agent_event(agent_name, f"Contacting {agent_name}...", contextId))
            
            # ========================================================================
            # EMIT INITIAL STATUS: "submitted" - task has been sent to remote agent
            # This is for the SIDEBAR to show the agent is starting work
            # ========================================================================
            asyncio.create_task(self._emit_simple_task_status(agent_name, "submitted", contextId, taskId))
            
            try:
                # CRITICAL: Store HOST's contextId for use in callbacks
                # Callbacks receive events with remote agent's contextId, but we need
                # to route WebSocket events using the host's session contextId
                self._current_host_context_id = contextId
                host_context_id = contextId
                
                # SIMPLIFIED: Callback for streaming execution that handles file artifacts
                # Status events are handled ONLY in _default_task_callback -> _emit_task_event
                # Track if we've emitted "working" status for this callback session
                _working_emitted = {"emitted": False}
                
                def streaming_task_callback(event, agent_card):
                    """Enhanced callback for streaming execution that captures detailed agent activities"""
                    agent_name = agent_card.name
                    log_debug(f"üé¨ [streaming_task_callback] CALLED for {agent_name}: {type(event).__name__}")
                    log_debug(f"[STREAMING] Detailed callback from {agent_name}: {type(event).__name__}")
                    
                    # ========================================================================
                    # EMIT "working" status on FIRST callback - shows agent is processing
                    # ========================================================================
                    if not _working_emitted["emitted"]:
                        _working_emitted["emitted"] = True
                        log_debug(f"[WORKING] First callback - emitting working status for {agent_name}")
                        asyncio.create_task(self._emit_simple_task_status(agent_name, "working", contextId, taskId))
                    
                    # Emit granular events based on the type of update
                    if hasattr(event, 'kind'):
                        event_kind = getattr(event, 'kind', 'unknown')
                        
                        if event_kind == 'status-update':
                            # Extract detailed status information
                            status_text = "processing"
                            if hasattr(event, 'status') and event.status:
                                if hasattr(event.status, 'message') and event.status.message:
                                    if hasattr(event.status.message, 'parts') and event.status.message.parts:
                                        # Process ALL parts - don't break early so we catch all image artifacts
                                        for part in event.status.message.parts:
                                            # Check for text parts
                                            if hasattr(part, 'root') and hasattr(part.root, 'text'):
                                                status_text = part.root.text
                                                # Continue processing to find image artifacts - don't break!
                                            # Check for image artifacts in DataPart
                                            elif hasattr(part, 'root') and hasattr(part.root, 'data') and isinstance(part.root.data, dict):
                                                artifact_uri = part.root.data.get('artifact-uri')
                                                if artifact_uri:
                                                    log_debug(f"Found image artifact in streaming event: {artifact_uri}")
                                                    # Register in agent file registry for file history persistence
                                                    session_id = host_context_id.split('::')[0] if '::' in host_context_id else host_context_id
                                                    from service.agent_file_registry import register_agent_file
                                                    register_agent_file(
                                                        session_id=session_id,
                                                        uri=artifact_uri,
                                                        filename=part.root.data.get("file-name", "agent-artifact.png"),
                                                        content_type="image/png",
                                                        source_agent=agent_name
                                                    )
                                                    # Emit file_uploaded event - USE HOST'S contextId for routing!
                                                    asyncio.create_task(self._emit_file_artifact_event(
                                                        filename=part.root.data.get("file-name", "agent-artifact.png"),
                                                        uri=artifact_uri,
                                                        context_id=host_context_id,
                                                        agent_name=agent_name,
                                                        content_type="image/png",
                                                        size=part.root.data.get("file-size", 0)
                                                    ))
                                            # Check for image artifacts in FilePart
                                            elif hasattr(part, 'root') and hasattr(part.root, 'file'):
                                                file_obj = part.root.file
                                                if isinstance(file_obj, FileWithUri):
                                                    file_uri = file_obj.uri
                                                    if file_uri and str(file_uri).startswith(("http://", "https://")):
                                                        log_debug(f"Found image artifact in streaming event (FilePart): {file_uri}")
                                                        # Capture values to avoid closure issues
                                                        file_name = file_obj.name
                                                        mime_type = file_obj.mimeType if hasattr(file_obj, 'mimeType') else 'image/png'
                                                        # Register in agent file registry for file history persistence
                                                        session_id = host_context_id.split('::')[0] if '::' in host_context_id else host_context_id
                                                        from service.agent_file_registry import register_agent_file
                                                        register_agent_file(
                                                            session_id=session_id,
                                                            uri=str(file_uri),
                                                            filename=file_name,
                                                            content_type=mime_type,
                                                            source_agent=agent_name
                                                        )
                                                        # Emit file_uploaded event - USE HOST'S contextId for routing!
                                                        asyncio.create_task(self._emit_file_artifact_event(
                                                            filename=file_name,
                                                            uri=str(file_uri),
                                                            context_id=host_context_id,
                                                            agent_name=agent_name,
                                                            content_type=mime_type,
                                                            size=0
                                                        ))
                                elif hasattr(event.status, 'state'):
                                    state = event.status.state
                                    if hasattr(state, 'value'):
                                        state_value = state.value
                                    else:
                                        state_value = str(state)
                                    
                                    # Calculate elapsed time for context
                                    elapsed_seconds = int(time.time() - start_time)
                                    elapsed_str = f" ({elapsed_seconds}s)" if elapsed_seconds >= 5 else ""
                                    
                                    # Make status messages friendly and personalized with user's query context
                                    if state_value == "working":
                                        status_text = f"{agent_name} is working on: \"{query_preview}\"{elapsed_str}"
                                    elif state_value == "submitted":
                                        status_text = f"Request sent to {agent_name}: \"{query_preview}\""
                                    else:
                                        status_text = f"{agent_name}: {state_value}{elapsed_str}"
                            
                            # Stream detailed status to UI - USE HOST'S contextId for routing!
                            asyncio.create_task(self._emit_granular_agent_event(agent_name, status_text, host_context_id))
                            
                        elif event_kind == 'artifact-update':
                            # Agent is generating artifacts - USE HOST'S contextId for routing!
                            elapsed_seconds = int(time.time() - start_time)
                            elapsed_str = f" ({elapsed_seconds}s)" if elapsed_seconds >= 5 else ""
                            asyncio.create_task(self._emit_granular_agent_event(agent_name, f"{agent_name} is preparing results{elapsed_str}", host_context_id))
                        
                        elif event_kind == 'task':
                            # Initial task creation - USE HOST'S contextId for routing!
                            asyncio.create_task(self._emit_granular_agent_event(agent_name, f"{agent_name} has started working on: \"{query_preview}\"", host_context_id))
                    
                    # Call the original callback for task management
                    return self._default_task_callback(event, agent_card)                # Emit outgoing message event for DAG display (use original message, not contextualized)
                clean_message = message
                if isinstance(message, dict):
                    clean_message = message.get('text', message.get('message', str(message)))
                elif not isinstance(message, str):
                    clean_message = str(message)
                
                # Truncate very long messages for DAG display
                if len(clean_message) > 500:
                    clean_message = clean_message[:497] + "..."
                
                asyncio.create_task(self._emit_outgoing_message_event(agent_name, clean_message, contextId))
                
                log_debug(f"\nüìûüìûüìû [ABOUT TO CALL] client.send_message for {agent_name} with streaming_task_callback")
                log_debug(f"üìû Callback type: {type(streaming_task_callback)}, callable: {callable(streaming_task_callback)}")
                response = await client.send_message(request, streaming_task_callback)
                log_debug(f"‚úÖ [STREAMING] Agent {agent_name} responded successfully!")

                
            except Exception as e:
                log_debug(f"‚ùå [STREAMING] Agent {agent_name} failed: {e}")
                
                import traceback
                log_debug(f"‚ùå Full traceback: {traceback.format_exc()}")
                raise
            
            log_debug(f"üîÑ [STREAMING] Processing response from {agent_name}: {type(response)}")
            
            # Simplified response processing for streaming execution
            if isinstance(response, Task):
                task = response
                
                # DEBUG: Log task response structure
                log_debug(f"üìä Received Task response from {agent_name}:")
                log_debug(f"  ‚Ä¢ Task ID: {task.id if hasattr(task, 'id') else 'N/A'}")
                log_debug(f"  ‚Ä¢ Task state: {task.status.state if hasattr(task, 'status') else 'N/A'}")
                log_debug(f"  ‚Ä¢ Has status.message: {hasattr(task, 'status') and hasattr(task.status, 'message') and task.status.message is not None}")
                log_debug(f"  ‚Ä¢ Has artifacts: {hasattr(task, 'artifacts') and task.artifacts is not None}")
                if hasattr(task, 'artifacts') and task.artifacts:
                    log_debug(f"  ‚Ä¢ Artifacts count: {len(task.artifacts)}")
                
                # Update session context only with essential info
                context_id = get_context_id(task)
                if context_id:
                    session_context.contextId = context_id
                # Record the task id for this specific agent only
                t_id = get_task_id(task)
                session_context.agent_task_ids[agent_name] = t_id
                # Track latest state for this agent's task
                try:
                    state_val = task.status.state.value if hasattr(task.status.state, 'value') else str(task.status.state)
                except Exception:
                    state_val = "working"
                session_context.agent_task_states[agent_name] = state_val
                
                # Handle task states
                log_debug(f"Checking task state: {task.status.state}")
                if task.status.state == TaskState.completed:
                    # ========================================================================
                    # EMIT COMPLETED STATUS for sidebar
                    # ========================================================================
                    log_debug(f"Task completed - emitting completed status for {agent_name}")
                    asyncio.create_task(self._emit_simple_task_status(agent_name, "completed", contextId, taskId))
                    
                    # Emit workflow message for completed task
                    asyncio.create_task(self._emit_granular_agent_event(agent_name, f"{agent_name} has completed the task successfully", contextId))
                    
                    response_parts = []
                    
                    # DEBUG: Check what's in the task
                    log_debug(f"Task completed - message exists: {task.status.message is not None}")
                    if task.status.message:
                        log_debug(f"Task completed - parts count: {len(task.status.message.parts) if task.status.message.parts else 0}")
                    log_debug(f"  ‚Ä¢ task.artifacts exists: {task.artifacts is not None}")
                    if task.artifacts:
                        log_debug(f"  ‚Ä¢ task.artifacts count: {len(task.artifacts)}")
                        for idx, art in enumerate(task.artifacts):
                            log_debug(f"  ‚Ä¢ artifact[{idx}].parts count: {len(art.parts) if art.parts else 0}")
                    
                    # Extract token usage from message parts before converting
                    if task.status.message and task.status.message.parts:
                        for part in task.status.message.parts:
                            if hasattr(part, 'root') and hasattr(part.root, 'kind') and part.root.kind == 'data':
                                data = getattr(part.root, 'data', {})
                                if isinstance(data, dict) and data.get('type') == 'token_usage':
                                    self.agent_token_usage[agent_name] = {
                                        'prompt_tokens': data.get('prompt_tokens', 0),
                                        'completion_tokens': data.get('completion_tokens', 0),
                                        'total_tokens': data.get('total_tokens', 0)
                                    }
                                    log_debug(f"üí∞ [TASK] Extracted token usage for {agent_name}: {self.agent_token_usage[agent_name]}")
                                    break
                    
                    if task.status.message:
                        response_parts.extend(
                            await self.convert_parts(task.status.message.parts, tool_context)
                        )
                    if task.artifacts:
                        for artifact in task.artifacts:
                            response_parts.extend(
                                await self.convert_parts(artifact.parts, tool_context)
                            )
                            # Note: Artifacts will be added to _agent_generated_artifacts with deduplication
                            # in the processing loop below (lines 3131-3168)

                    # DEBUG: Log what's now in _latest_processed_parts after conversion
                    if hasattr(session_context, "_latest_processed_parts"):
                        latest = session_context._latest_processed_parts
                        log_debug(f"üì¶ After convert_parts, _latest_processed_parts has {len(latest)} items total (accumulated)")
                        
                    # Add file artifacts from THIS response to _agent_generated_artifacts for UI display
                    # Support both FilePart (preferred) and DataPart (legacy) formats
                    mode = "Agent Mode" if session_context.agent_mode else "Standard Mode"
                    log_debug(f"üîç [{mode}] Checking response_parts ({len(response_parts)} items) for file artifacts...")
                    for item in response_parts:
                        # Check for FilePart (new standard format)
                        is_file_part = isinstance(item, FilePart) or (hasattr(item, 'root') and isinstance(item.root, FilePart))
                        # Check for DataPart with artifact-uri (legacy format)
                        is_data_artifact = isinstance(item, DataPart) or (hasattr(item, 'root') and isinstance(item.root, DataPart))
                        
                        if is_file_part or is_data_artifact:
                            if not hasattr(session_context, '_agent_generated_artifacts'):
                                session_context._agent_generated_artifacts = []
                            session_context._agent_generated_artifacts.append(item)
                            part_type = "FilePart" if is_file_part else "DataPart"
                            log_debug(f"üìé [STREAMING - {mode}] Added {part_type} from THIS response to _agent_generated_artifacts")
                    
                    if hasattr(session_context, '_agent_generated_artifacts'):
                        log_debug(f"‚úÖ [{mode}] Total _agent_generated_artifacts: {len(session_context._agent_generated_artifacts)}")

                    self._update_last_host_turn(session_context, agent_name, response_parts)
                    
                    # Store interaction in background (don't await to avoid blocking streaming execution)
                    asyncio.create_task(self._store_a2a_interaction_background(
                        outbound_request=request,
                        inbound_response=response,
                        agent_name=agent_name,
                        processing_time=time.time() - start_time,
                        span=span,
                        context_id=contextId
                    ))
                    
                    return response_parts
                    
                elif task.status.state == TaskState.failed:
                    log_debug(f"Task failed for {agent_name}")
                    # Detect rate limit and retry after suggested delay
                    retry_after = self._parse_retry_after_from_task(task)
                    max_rate_limit_retries = 3
                    retry_attempt = 0

                    while retry_after and retry_after > 0 and retry_attempt < max_rate_limit_retries:
                        retry_attempt += 1
                        session_context.agent_task_states[agent_name] = 'failed'
                        session_context.agent_cooldowns[agent_name] = time.time() + retry_after
                        try:
                            asyncio.create_task(self._emit_granular_agent_event(agent_name, f"rate limited; retrying in {retry_after}s (attempt {retry_attempt}/{max_rate_limit_retries})", session_context.contextId))
                        except Exception:
                            pass

                        await asyncio.sleep(min(60, retry_after))

                        retry_request = MessageSendParams(
                            id=str(uuid.uuid4()),
                            message=Message(
                                role='user',
                                parts=[Part(root=TextPart(text=contextualized_message))],
                                messageId=str(uuid.uuid4()),
                                contextId=session_context.contextId,
                                taskId=None,
                            ),
                            configuration=MessageSendConfiguration(
                                acceptedOutputModes=['text', 'text/plain', 'image/png'],
                            ),
                        )
                        try:
                            retry_response = await client.send_message(retry_request, self.task_callback)
                        except Exception as e:
                            log_debug(f"[STREAMING] Retry after rate limit failed for {agent_name}: {e}")
                            break

                        if isinstance(retry_response, Task):
                            task2 = retry_response
                            context_id2 = get_context_id(task2)
                            if context_id2:
                                session_context.contextId = context_id2
                            session_context.agent_task_ids[agent_name] = task2.id
                            try:
                                state_val2 = task2.status.state.value if hasattr(task2.status.state, 'value') else str(task2.status.state)
                            except Exception:
                                state_val2 = 'working'
                            session_context.agent_task_states[agent_name] = state_val2

                            if task2.status.state == TaskState.completed:
                                retry_parts = []
                                if task2.status.message:
                                    retry_parts.extend(await self.convert_parts(task2.status.message.parts, tool_context))
                                if task2.artifacts:
                                    for artifact in task2.artifacts:
                                        retry_parts.extend(await self.convert_parts(artifact.parts, tool_context))
                                self._update_last_host_turn(session_context, agent_name, retry_parts)
                                return retry_parts

                            if task2.status.state == TaskState.input_required:
                                if task2.status.message:
                                    retry_input = await self.convert_parts(task2.status.message.parts, tool_context)
                                    self._update_last_host_turn(session_context, agent_name, retry_input)
                                    return retry_input
                                return [f"Agent {agent_name} requires additional input"]

                            if task2.status.state == TaskState.failed:
                                retry_after = self._parse_retry_after_from_task(task2)
                                if retry_after:
                                    continue

                            return [f"Agent {agent_name} is processing your request"]

                        if isinstance(retry_response, Message):
                            retry_result = await self.convert_parts(retry_response.parts, tool_context)
                            self._update_last_host_turn(session_context, agent_name, retry_result)
                            return retry_result

                        return [str(retry_response)]

                    return [f"Agent {agent_name} failed to complete the task"]

                elif task.status.state == TaskState.input_required:
                    log_debug(f"‚ö†Ô∏è [STREAMING] Agent {agent_name} requires input")
                    if task.status.message:
                        response_parts = await self.convert_parts(task.status.message.parts, tool_context)
                        self._update_last_host_turn(session_context, agent_name, response_parts)
                        return response_parts
                    return [f"Agent {agent_name} requires additional input"]
                    
                else:
                    # Handle working/pending states
                    return [f"Agent {agent_name} is processing your request"]
                    
            elif isinstance(response, Message):
                # Direct message response
                result = await self.convert_parts(response.parts, tool_context)
                self._update_last_host_turn(session_context, agent_name, result)
                
                # Store interaction in background
                asyncio.create_task(self._store_a2a_interaction_background(
                    outbound_request=request,
                    inbound_response=response,
                    agent_name=agent_name,
                    processing_time=time.time() - start_time,
                    span=span,
                    context_id=contextId
                ))
                
                return result
                
            elif isinstance(response, str):
                log_debug(f"[STREAMING] String response from {agent_name}: {response[:200]}...")
                self._update_last_host_turn(session_context, agent_name, [response])
                return [response]
                
            else:
                log_debug(f"[STREAMING] Unknown response type from {agent_name}: {type(response)}")
                return [str(response)]
            
    async def _add_context_to_message(
        self,
        message: str,
        session_context: SessionContext,
        thread_id: str = None,
        target_agent_name: Optional[str] = None,
    ) -> str:
        """
        Enhance agent messages with relevant context for better continuity and accuracy.
        
        Context enrichment strategies:
        1. **Last Turn Context**: Include recent responses from other agents so the current
           agent knows what has already been discussed/done (configurable via env vars)
        
        2. **Memory Search**: Query semantic memory service to find similar past conversations
           that might provide useful precedents or patterns (only if enabled for this session)
        
        3. **Cross-Agent Handoff**: When agent B is called after agent A, include agent A's
           response so agent B can build on it rather than starting from scratch
        
        The context is carefully sized to stay within token limits while maximizing
        the useful information provided to the agent.
        
        Args:
            message: The original message to send to the agent
            session_context: Current session state with conversation history
            thread_id: Optional thread ID for Azure AI Foundry conversation
            target_agent_name: Name of the agent receiving this message
            
        Returns:
            Enhanced message with relevant context prepended
        """
        """Add relevant conversation context and memory insights to the message for better agent responses.
        
        Following Google A2A best practices: host manages context and includes it in agent messages.
        
        Context injection behavior based on mode and inter-agent memory toggle:
        
        AGENT MODE + MEMORY OFF (minimal context for focused workflows):
          - Recent agent outputs: Only immediate previous agent (1 response)
          - Vector search: Disabled
          - Use case: Sequential workflows where agents only need the last step
          
        AGENT MODE + MEMORY ON (full context):
          - Recent agent outputs: Last N agents (default: 1, configurable)
          - Vector search: Enabled (searches all past interactions)
          - Use case: Complex reasoning requiring broader context
          
        STANDARD MODE (always full context):
          - Recent agent outputs: Last N agents (default: 1, configurable)
          - Vector search: Enabled when memory ON, disabled when OFF
          - Use case: User conversations with multi-turn context
        """
        enable_memory = getattr(session_context, 'enable_inter_agent_memory', True)
        is_agent_mode = hasattr(session_context, 'agent_mode') and session_context.agent_mode
        mode_label = "Agent Mode" if is_agent_mode else "Standard Mode"
        
        context_parts = []
        
        # Primary approach: Use semantic memory search for relevant context (only if enabled)
        if enable_memory:
            log_debug(f"üéØ [{mode_label}] Inter-agent memory enabled - searching vector memory")
        try:
            log_debug(f"üß† Searching memory for semantically relevant context...")
            log_debug(f"About to call _search_relevant_memory...")
            memory_results = await self._search_relevant_memory(
                query=message,
                context_id=session_context.contextId,
                agent_name=None,  # Search across all interactions for maximum relevance
                top_k=5  # Get top 5 most relevant pieces of context
            )
            print(f"‚úÖ _search_relevant_memory completed, got {len(memory_results) if memory_results else 0} results")
            
            if memory_results:
                context_parts.append("Relevant context from previous interactions:")
                print(f"Processing {len(memory_results)} memory results...")
                
                # Debug: Show structure of first result
                if memory_results:
                    first_result = memory_results[0]
                    print(f"First result keys: {list(first_result.keys()) if isinstance(first_result, dict) else 'Not a dict'}")
                    if isinstance(first_result, dict) and 'a2a_inbound_response' in first_result:
                        inbound = first_result['a2a_inbound_response']
                        print(f"Inbound response type: {type(inbound)}")
                        if isinstance(inbound, dict):
                            print(f"Inbound response keys: {list(inbound.keys())}")
                        elif isinstance(inbound, str):
                            print(f"Inbound response (string): {inbound[:200]}...")
                
                # Process memory results to extract key information
                for i, result in enumerate(memory_results, 1):
                    try:
                        # Extract the most relevant parts from each memory result
                        agent_name = result.get('agent_name', 'Unknown')
                        timestamp = result.get('timestamp', 'Unknown')
                            
                        # Get the actual content - try multiple possible locations
                        content_summary = ""
                        
                        # Method 1: Look for direct content field in inbound response
                        if 'inbound_payload' in result and result['inbound_payload']:
                            inbound = result['inbound_payload']
                            
                            # Parse JSON string if needed
                            if isinstance(inbound, str):
                                try:
                                    inbound = json.loads(inbound)
                                except json.JSONDecodeError:
                                    inbound = {}
                            
                            # Try direct content field (DocumentProcessor format)
                            if isinstance(inbound, dict) and 'content' in inbound:
                                content_summary = str(inbound['content'])
                            
                            # Try parts array (A2A Message structure)
                            elif isinstance(inbound, dict) and 'parts' in inbound:
                                parts_content = []
                                for part in inbound['parts']:
                                    if isinstance(part, dict):
                                        # Look for text in various structures
                                        if 'text' in part:
                                            parts_content.append(str(part['text']))
                                        elif 'root' in part and isinstance(part['root'], dict) and 'text' in part['root']:
                                            parts_content.append(str(part['root']['text']))
                                if parts_content:
                                    content_summary = " ".join(parts_content)
                        
                        # Method 2: Look in outbound payload if inbound didn't work
                        if not content_summary and 'outbound_payload' in result:
                            outbound = result['outbound_payload']
                            
                            if isinstance(outbound, str):
                                try:
                                    outbound = json.loads(outbound)
                                except json.JSONDecodeError:
                                    outbound = {}
                            
                            if isinstance(outbound, dict) and 'message' in outbound and 'parts' in outbound['message']:
                                parts_content = []
                                for part in outbound['message']['parts']:
                                    if isinstance(part, dict) and 'root' in part and 'text' in part['root']:
                                        parts_content.append(str(part['root']['text']))
                                if parts_content:
                                    content_summary = " ".join(parts_content)
                        
                        # Method 3: Fallback - try to extract any text from the raw result
                        if not content_summary:
                            # Convert entire result to string and look for meaningful content
                            result_str = str(result)
                            if len(result_str) > 50:  # Only if there's substantial content
                                content_summary = result_str
                        
                        # Add to context if we found content
                        if content_summary:
                            # Truncate long content for context efficiency
                            if len(content_summary) > self.memory_summary_max_chars:
                                content_summary = content_summary[: self.memory_summary_max_chars] + "..."
                            context_parts.append(f"  {i}. From {agent_name}: {content_summary}")
                        else:
                            print(f"‚ö†Ô∏è No content found in memory result {i} from {agent_name}")
                    
                    except Exception as e:
                        print(f"‚ö†Ô∏è Error processing memory result {i}: {e}")
                        continue
            
            else:
                log_debug(f"üß† No relevant memory context found")
        
        except Exception as e:
            print(f"‚ùå Error searching memory: {e}")
            context_parts.append("Note: Unable to retrieve relevant context from memory")
        else:
            log_debug(f"üéØ [{mode_label}] Inter-agent memory disabled - skipping vector search")
        
        # Include recent host-side turns (previous agent outputs)
        # Behavior depends on mode and inter-agent memory setting:
        # - Agent Mode + Memory OFF: Only immediate previous agent (limit=1)
        # - Agent Mode + Memory ON: Last few agents (limit=self.last_host_turns)
        # - Standard Mode: Always use self.last_host_turns setting
        if self.include_last_host_turn:
            history: List[Dict[str, str]] = list(getattr(session_context, "host_turn_history", []))

            # Back-compat: fall back to single cached turn if list empty
            if not history and getattr(session_context, "last_host_turn_text", None):
                history = [
                    {
                        "agent": getattr(session_context, "last_host_turn_agent", "host_agent"),
                        "text": getattr(session_context, "last_host_turn_text", ""),
                    }
                ]

            # Determine how many previous responses to include
            if is_agent_mode and not enable_memory:
                # Agent Mode with memory OFF: Only pass immediate previous agent
                max_turns = 1
                log_debug(f"üéØ [Agent Mode] Memory disabled - passing only immediate previous agent output")
            else:
                # Standard mode or Agent Mode with memory ON: Use configured limit
                max_turns = self.last_host_turns
                log_debug(f"üéØ [{mode_label}] Passing up to {max_turns} recent agent outputs")

            selected: List[Dict[str, str]] = []
            for entry in reversed(history):  # newest first
                agent = entry.get("agent")
                text = (entry.get("text") or "").strip()
                if not text:
                    continue
                if target_agent_name and agent == target_agent_name:
                    continue
                selected.append({"agent": agent or "host_agent", "text": text})
                if len(selected) >= max_turns:
                    break

            if selected:
                logger.debug(
                    "[A2A] Injecting %d host turn(s) into message for %s",
                    len(selected),
                    target_agent_name or "unknown",
                )
                context_parts.append("Previous context from host conversation:")
                for idx, entry in enumerate(selected, start=1):
                    truncated_text = entry["text"]
                    if len(truncated_text) > self.last_host_turn_max_chars:
                        truncated_text = truncated_text[: self.last_host_turn_max_chars] + "..."
                    context_parts.append(f"  {idx}. From {entry['agent']}: {truncated_text}")
            else:
                logger.debug(
                    "[A2A] No eligible host turns to inject for agent %s",
                    target_agent_name,
                )

        # Fallback: Add minimal recent thread context only if memory search failed
        if not context_parts and thread_id:
            try:
                print(f"üßµ Fallback: Using recent thread context (memory search failed)")
                messages = await self._http_list_messages(thread_id, limit=5)
                
                if messages and len(messages) > 0:
                    context_parts.append("Recent conversation context:")
                    # Only include last 5 exchanges to avoid context bloat
                    for msg in messages[:5]:  # Already in reverse order from _http_list_messages
                        if msg.get('content'):
                            for content in msg['content']:
                                if content.get('type') == 'text' and content.get('text', {}).get('value'):
                                    role = msg.get('role', 'unknown')
                                    text = content['text']['value']
                                    context_parts.append(f"{role}: {text}")
                print(f"üßµ Added {len(context_parts)-1} recent messages to context")
            except Exception as e:
                print(f"‚ùå Error accessing thread context: {e}")
        
        # Combine context with original message
        if context_parts:
            full_context = "\n".join(context_parts)
            return f"{full_context}\n\nCurrent request: {message}"
        else:
            return message

    async def _store_a2a_interaction_background(
        self, 
        outbound_request: MessageSendParams,
        inbound_response: Any,
        agent_name: str,
        processing_time: float,
        span: Any,
        context_id: str = None
    ):
        """Background task for storing A2A interactions without blocking parallel execution"""
        try:
            await self._store_a2a_interaction(
                outbound_request=outbound_request,
                inbound_response=inbound_response,
                agent_name=agent_name,
                processing_time=processing_time,
                span=span,
                context_id=context_id
            )
        except Exception as e:
            print(f"‚ùå Background A2A interaction storage failed for {agent_name}: {e}")
            # Don't let storage errors affect parallel execution

    async def _store_a2a_interaction(
        self, 
        outbound_request: MessageSendParams,
        inbound_response: Any,
        agent_name: str,
        processing_time: float,
        span: Any,
        context_id: str = None
    ):
        """
        Persist agent-to-agent interactions to memory service for future semantic search.
        
        Why we store interactions:
        - Enable "has this been asked before?" queries
        - Learn from past agent responses and patterns
        - Provide context for multi-turn conversations
        - Track agent performance and reliability
        - Support debugging and troubleshooting
        
        The memory service uses Azure Cognitive Search with vector embeddings to enable
        semantic similarity search. This means agents can find relevant past conversations
        even when the wording is different.
        
        Example: If a user asks "What's our refund policy?" and later asks "Can I get my
        money back?", the agent can retrieve the previous policy explanation.
        
        Args:
            outbound_request: The original A2A message sent to the remote agent
            inbound_response: The response received from the remote agent
            agent_name: Name of the agent that processed the request
            processing_time: How long the agent took to respond (for performance tracking)
            span: OpenTelemetry span for distributed tracing
            context_id: Context ID for tenant isolation (required for multi-tenancy)
        """
        """Store complete A2A protocol payloads"""
        try:
            # Extract session_id for tenant isolation
            session_id = get_tenant_from_context(context_id) if context_id else None
            
            if not session_id:
                log_debug(f"[A2A Memory] Warning: No session_id available, skipping interaction storage for tenant isolation")
                return
            
            # Prepare interaction data with complete A2A payloads
            interaction_data = {
                "interaction_id": str(uuid.uuid4()),
                "agent_name": agent_name,
                "processing_time_seconds": processing_time,
                "timestamp": datetime.now(timezone.utc).strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + 'Z',
                
                # Complete outbound A2A payload
                "outbound_payload": outbound_request.model_dump() if hasattr(outbound_request, 'model_dump') else str(outbound_request),
                
                # Complete inbound A2A payload  
                "inbound_payload": inbound_response.model_dump() if hasattr(inbound_response, 'model_dump') else str(inbound_response)
            }
            
            # Store in memory service with session_id for tenant isolation
            success = await a2a_memory_service.store_interaction(interaction_data, session_id=session_id)
            
            if success:
                log_debug(f"[A2A Memory] Stored A2A payloads for {agent_name} (session: {session_id})")
                span.add_event("memory_stored", {"agent_name": agent_name, "session_id": session_id})
            else:
                log_debug(f"[A2A Memory] Failed to store A2A payloads for {agent_name}")
                span.add_event("memory_store_failed", {"agent_name": agent_name})
            
            # ‚úÖ EMIT A2A PAYLOAD TO WEBSOCKET FOR FRONTEND VISIBILITY
            # This enables the "Show Agent Workflow" view and sidebar status tracking
            try:
                from service.websocket_streamer import get_websocket_streamer
                streamer = await get_websocket_streamer()
                if streamer:
                    # Emit a2a_payload event with complete request/response data
                    a2a_payload_event = {
                        "interactionId": interaction_data["interaction_id"],
                        "agentName": agent_name,
                        "timestamp": interaction_data["timestamp"],
                        "processingTime": processing_time,
                        "outboundPayload": interaction_data["outbound_payload"],
                        "inboundPayload": interaction_data["inbound_payload"],
                        "contextId": context_id
                    }
                    
                    await streamer._send_event("a2a_payload", a2a_payload_event, context_id)
                    log_debug(f"üì° [A2A PAYLOAD] Emitted A2A payload to WebSocket for {agent_name}")
                else:
                    log_debug("‚ö†Ô∏è WebSocket streamer not available for A2A payload emission")
            except Exception as ws_error:
                log_debug(f"‚ö†Ô∏è Failed to emit A2A payload to WebSocket: {ws_error}")
                # Don't fail the entire storage operation if WebSocket emission fails
                
        except Exception as e:
            log_debug(f"[A2A Memory] Error storing A2A payloads: {str(e)}")
            span.add_event("memory_store_error", {
                "agent_name": agent_name,
                "error": str(e)
            })

    async def _store_user_host_interaction_safe(
        self,
        user_message_parts: List[Part],
        user_message_text: str,
        host_response: List[str],
        context_id: str,
        span: Any,
        artifact_info: Dict[int, Dict[str, str]] = None
    ):
        """Safe wrapper for User‚ÜíHost memory storage that won't block conversation"""
        try:
            await self._store_user_host_interaction(
                user_message_parts=user_message_parts,
                user_message_text=user_message_text,
                host_response=host_response,
                context_id=context_id,
                span=span,
                artifact_info=artifact_info
            )
        except Exception as e:
            print(f"‚ùå User‚ÜíHost interaction storage failed: {e}")
            log_error(f"Exception type: {type(e).__name__}")
            import traceback
            print(f"Traceback: {traceback.format_exc()}")

    async def _store_user_host_interaction(
        self,
        user_message_parts: List[Part],
        user_message_text: str,
        host_response: List[str],
        context_id: str,
        span: Any,
        artifact_info: Dict[int, Dict[str, str]] = None
    ):
        """Store User‚ÜíHost A2A protocol exchange"""
        log_debug(f"üöÄ _store_user_host_interaction: STARTING")
        print(f"- user_message_text: {user_message_text[:100]}...")
        print(f"- host_response count: {len(host_response)}")
        print(f"- context_id: {context_id}")
        print(f"- user_message_parts type: {type(user_message_parts)}")
        print(f"- user_message_parts length: {len(user_message_parts) if user_message_parts else 0}")
        
        try:
            log_debug(f"üìù Step 1: About to create A2A Message object...")
            
            # Create real A2A Message object for outbound
            log_debug(f"üìù Step 1a: Creating outbound_message with uuid...")
            message_id = str(uuid.uuid4())
            log_debug(f"üìù Step 1b: Generated messageId: {message_id}")
            
            log_debug(f"üìù Step 1c: About to create Message object...")
            # Clean file bytes from parts before storing in memory
            cleaned_parts = self._clean_file_bytes_from_parts(user_message_parts, artifact_info)
            log_debug(f"üìù Step 1d: Cleaned {len(user_message_parts)} parts for memory storage")
            
            outbound_message = Message(
                messageId=message_id,
                contextId=context_id,
                taskId=None,
                role="user",  # User‚ÜíHost message
                parts=cleaned_parts  # Use cleaned A2A Parts without file bytes
            )
            print(f"‚úÖ Step 1: Created outbound_message successfully")
            
            # Create real A2A MessageSendParams
            log_debug(f"üìù Step 2: About to create MessageSendParams...")
            request_id = str(uuid.uuid4())
            log_debug(f"üìù Step 2a: Generated request ID: {request_id}")
            
            log_debug(f"üìù Step 2b: About to create MessageSendConfiguration...")
            config = MessageSendConfiguration(
                acceptedOutputModes=["text", "text/plain", "image/png"]
            )
            print(f"‚úÖ Step 2c: Created MessageSendConfiguration")
            
            log_debug(f"üìù Step 2d: About to create MessageSendParams...")
            outbound_request = MessageSendParams(
                id=request_id,
                message=outbound_message,
                configuration=config
            )
            print(f"‚úÖ Step 2: Created MessageSendParams successfully")
            
            # Create real A2A Message object for inbound response
            log_debug(f"üìù Step 3: Creating inbound response parts...")
            response_parts = []
            for i, response in enumerate(host_response):
                # Skip artifact dicts - they're for UI display, not for memory storage
                if isinstance(response, dict) and ('artifact-uri' in response or 'artifact-id' in response):
                    log_debug(f"üìù Step 3.{i+1}: Skipping artifact dict (not storing in memory)")
                    continue
                    
                log_debug(f"üìù Step 3.{i+1}: Creating Part for response {i+1}")
                # Convert non-string responses to JSON string
                if isinstance(response, str):
                    text = response
                else:
                    text = json.dumps(response, ensure_ascii=False)
                text_part = TextPart(text=text)  # Don't pass kind - it's inferred from the class
                part = Part(root=text_part)
                response_parts.append(part)
                print(f"‚úÖ Step 3.{i+1}: Created Part successfully")
            
            log_debug(f"üìù Step 4: Creating inbound Message...")
            inbound_message_id = str(uuid.uuid4())
            inbound_message = Message(
                messageId=inbound_message_id,
                contextId=context_id,
                taskId=None,
                role="agent",  # Host‚ÜíUser response (A2A uses 'agent' not 'assistant')
                parts=response_parts
            )
            print(f"‚úÖ Step 4: Created inbound Message successfully")
            
            # Test model_dump before calling memory service
            try:
                outbound_dict = outbound_request.model_dump()
                print(f"‚úÖ Step 5a: outbound_request.model_dump() worked")
            except Exception as e:
                return
                
            try:
                inbound_dict = inbound_message.model_dump()
            except Exception as e:
                return
            
            # Store the User‚ÜíHost A2A interaction using real A2A objects
            log_debug(f"üìù Step 6: Storing User‚ÜíHost interaction in memory...")
            
            # Extract session_id for tenant isolation
            session_id = get_tenant_from_context(context_id) if context_id else None
            
            if not session_id:
                log_debug(f"[A2A Memory] Warning: No session_id available, skipping user-host interaction storage for tenant isolation")
                return
            
            # Create interaction data structure like the working Host‚ÜíRemote Agent code
            interaction_data = {
                "interaction_id": str(uuid.uuid4()),
                "agent_name": "host_agent",
                "processing_time_seconds": 1.0,
                "timestamp": datetime.now(timezone.utc).strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + 'Z',
                "outbound_payload": outbound_dict,
                "inbound_payload": inbound_dict
            }
            
            # Store in memory service with session_id for tenant isolation
            success = await a2a_memory_service.store_interaction(interaction_data, session_id=session_id)
            
            if success:
                print(f"‚úÖ Step 6: User‚ÜíHost interaction stored successfully (session: {session_id})")
                log_success(f"üéâ User‚ÜíHost A2A interaction now available for semantic search")
            else:
                print(f"‚ùå Step 6: Failed to store User‚ÜíHost interaction")
                print(f"‚ö†Ô∏è User‚ÜíHost interaction storage failed")
                
        except Exception as e:
            print(f"‚ùå EXCEPTION in _store_user_host_interaction: {e}")
            log_error(f"Exception type: {type(e).__name__}")
            import traceback
            log_error(f"Full traceback: {traceback.format_exc()}")
            span.add_event("user_host_memory_store_error", {
                "context_id": context_id,
                "error": str(e)
            })

    def _clean_file_bytes_from_parts(self, parts: List[Part], artifact_info: Dict[int, Dict[str, str]] = None) -> List[Part]:
        """Remove file bytes from A2A Parts to prevent large payloads in memory storage"""
        import copy
        cleaned_parts = []
        
        for i, part in enumerate(parts):
            if hasattr(part, 'root') and part.root.kind == 'file':
                # Create a copy of the part without the large file bytes
                cleaned_part = copy.deepcopy(part)
                if hasattr(cleaned_part.root.file, 'bytes'):
                    # Get original size before replacement
                    original_size = len(str(cleaned_part.root.file.bytes)) if cleaned_part.root.file.bytes else 0
                    
                    # If we have artifact info for this part, include the URI reference
                    if artifact_info and i in artifact_info:
                        artifact = artifact_info[i]
                        cleaned_part.root.file.bytes = f"<file_bytes_excluded_for_memory_storage_original_size_{original_size}_artifact_uri_{artifact.get('artifact_uri', 'unknown')}_artifact_id_{artifact.get('artifact_id', 'unknown')}>"
                    else:
                        # Fallback to simple metadata reference
                        cleaned_part.root.file.bytes = f"<file_bytes_excluded_for_memory_storage_original_size_{original_size}>"
                
                cleaned_parts.append(cleaned_part)
            else:
                # Non-file parts can be used as-is
                cleaned_parts.append(part)
                
        return cleaned_parts

    async def run_conversation_with_parts(self, message_parts: List[Part], context_id: Optional[str] = None, event_logger=None, agent_mode: bool = False, enable_inter_agent_memory: bool = False, workflow: Optional[str] = None) -> Any:
        """
        Process a user message that may include files, images, or multimodal content.
        
        This is the main entry point for complex user interactions involving:
        - File uploads (PDFs, images, documents)
        - Image editing requests (with base/mask images)
        - Multi-step workflows with file handoffs between agents
        
        Processing pipeline:
        1. **File Processing**: Extract text from documents, analyze images, handle masks
        2. **Artifact Storage**: Save files to Azure Blob or local storage with unique IDs
        3. **Context Preparation**: Package files for agent consumption (URIs + metadata)
        4. **Mode Selection**: Route to agent-mode orchestration or standard conversation
        5. **Response Synthesis**: Combine results from multiple agents if needed
        
        Agent Mode vs Standard Mode:
        - **Agent Mode**: AI orchestrator breaks down request into specialized tasks
        - **Standard Mode**: Single LLM call with tool use for agent delegation
        
        Args:
            message_parts: List of A2A Part objects (text, files, data)
            context_id: Conversation identifier for state management
            event_logger: Optional callback for logging conversation events
            agent_mode: If True, use multi-agent orchestration loop
            enable_inter_agent_memory: If True, agents can access conversation context
            workflow: Optional predefined workflow steps to execute
            
        Returns:
            List of response strings from the host agent
        """
        """Run conversation with A2A message parts (including files)."""
        log_debug(f"ENTRY: run_conversation_with_parts called with {len(message_parts) if message_parts else 0} parts")
        try:
            log_debug(f"Step: About to create tracer span...")
            scenario = "run_conversation_with_parts"
            with tracer.start_as_current_span(scenario) as span:
                log_debug(f"Step: Created tracer span successfully")
                
                log_debug(f"Step: About to call span.set_attribute...")
                span.set_attribute("context_id", context_id or self.default_context_id)
                log_debug(f"Step: span.set_attribute completed")
                log_debug(f"Step: Set span attribute")
            log_debug(f"run_conversation_with_parts: {len(message_parts)} parts")
            
            log_debug(f"üîç [run_conversation_with_parts] ENTRY - context_id param: {context_id}")
            
            # CRITICAL: context_id must be provided by caller (foundry_host_manager.process_message)
            # It should NEVER be None - if it is, that's a bug in the caller
            if not context_id:
                raise ValueError(f"context_id is required but was None or empty. This is a bug - foundry_host_manager should always provide context_id")
            
            log_debug(f"üîç [run_conversation_with_parts] Using context_id: {context_id}")
            
            # CRITICAL: Store the context_id so send_message_sync can access it
            # This is THE source of truth for the current request's contextId
            self._current_host_context_id = context_id
            log_debug(f"üîç [run_conversation_with_parts] SET _current_host_context_id to: {context_id}")
            
            # Extract text message for thread
            log_debug(f"Step: About to extract text message...")
            user_message = ""
            for part in message_parts:
                if hasattr(part, 'root') and part.root.kind == 'text':
                    user_message = part.root.text
                    break
            log_debug(f"Step: Extracted text message")
            
            log_debug(f"Extracted user message: {user_message}")
            print(f"Processing {len(message_parts)} parts including files")
            
            # Ensure agent is created (may be lazy creation if startup creation failed)
            log_debug(f"Step: About to ensure agent exists...")
            log_foundry_debug(f"Current agent state: {self.agent is not None}")
            if self.agent:
                log_foundry_debug(f"Agent exists with ID: {self.agent.get('id', 'unknown')}")
            else:
                print("‚ö†Ô∏è Agent not created at startup, creating now (lazy creation)...")
                log_foundry_debug(f"Calling create_agent()...")
                await self.create_agent()
                log_foundry_debug(f"create_agent() completed")
            log_debug(f"Step: Agent ready with ID: {self.agent.get('id', 'unknown') if self.agent else 'STILL_NULL'}")
            
            session_context = self.get_session_context(context_id)
            # Set agent mode in session context
            session_context.agent_mode = agent_mode
            session_context.enable_inter_agent_memory = enable_inter_agent_memory
            log_foundry_debug(f"Agent mode set to: {agent_mode}, Inter-agent memory: {enable_inter_agent_memory}")
            
            # HUMAN-IN-THE-LOOP: Check if an agent is waiting for input_required response
            hitl_result = await self._handle_pending_input_agent(
                session_context=session_context,
                message_parts=message_parts,
                context_id=context_id,
                event_logger=event_logger
            )
            if hitl_result is not None:
                return hitl_result
                
            # Reset any cached parts from prior turns so we don't resend stale attachments
            if hasattr(session_context, "_latest_processed_parts"):
                file_count_before = len(session_context._latest_processed_parts)
                log_foundry_debug(f"_latest_processed_parts has {file_count_before} parts before clearing check")
                log_foundry_debug(f"session_context.agent_mode = {session_context.agent_mode}")
                # In agent mode, preserve files so they flow between agents
                # In user mode, clear stale attachments from previous turns
                if not session_context.agent_mode:
                    print(f"‚ö†Ô∏è WARNING: Clearing {file_count_before} file parts because agent_mode is False")
                    session_context._latest_processed_parts = []
                    session_context._agent_generated_artifacts = []
                else:
                    # Keep files but log for debugging
                    file_count = len(session_context._latest_processed_parts)
                    print(f"üìé [Agent Mode] Preserving {file_count} file parts for agent-to-agent communication")
                    # Log file details for debugging
                    for idx, part in enumerate(session_context._latest_processed_parts):
                        if hasattr(part, 'root'):
                            if isinstance(part.root, FilePart):
                                file_name = getattr(part.root.file, 'name', 'unknown')
                                file_role = (part.root.metadata or {}).get("role", "no-role")
                                print(f"  ‚Ä¢ File {idx}: {file_name} (role={file_role})")
                            elif isinstance(part.root, DataPart) and isinstance(part.root.data, dict):
                                role = part.root.data.get('role', 'no-role')
                                name = part.root.data.get('file-name', 'unknown')
                                uri = part.root.data.get('artifact-uri', 'no-uri')
                                print(f"  ‚Ä¢ DataPart {idx}: {name} (role={role}, uri={uri[:50]}...)")
                        elif isinstance(part, dict):
                            role = part.get('role', 'no-role')
                            name = part.get('file-name', part.get('name', 'unknown'))
                            print(f"  ‚Ä¢ Dict {idx}: {name} (role={role})")
            
            log_foundry_debug(f"=================== STARTING MESSAGE PROCESSING ===================")
            log_foundry_debug(f"About to process {len(message_parts)} message parts")
            
            # Process all message parts (including files) BEFORE sending to thread
            # Use the SAME session_context so prepared parts are visible to send_message
            tool_context = DummyToolContext(session_context, self._azure_blob_client)
            processed_parts: List[Any] = []
            log_foundry_debug(f"processed_parts list initialized")
            
            # Count files to show appropriate status
            log_foundry_debug(f"Counting files in message parts...")
            file_count = 0
            for part in message_parts:
                if hasattr(part, 'root') and hasattr(part.root, 'kind') and part.root.kind == 'file':
                    file_count += 1
            log_foundry_debug(f"Found {file_count} files in {len(message_parts)} parts")
            
            if file_count > 0:
                log_foundry_debug(f"Emitting file processing status...")
                try:
                    if file_count == 1:
                        await self._emit_status_event("processing uploaded file", context_id)
                    else:
                        await self._emit_status_event(f"processing {file_count} uploaded files", context_id)
                    log_foundry_debug(f"File processing status emitted successfully")
                except Exception as e:
                    log_foundry_debug(f"‚ùå Exception emitting file processing status: {e}")
                    # Don't let status emission failures stop the main flow
            
            log_foundry_debug(f"PART: About to process {len(message_parts)} parts:")
            for i, part in enumerate(message_parts):
                log_foundry_debug(f"PART: Part {i}: {type(part)} - hasattr root: {hasattr(part, 'root')}")
                if hasattr(part, 'root'):
                    log_foundry_debug(f"PART: Part {i} root kind: {getattr(part.root, 'kind', 'no kind attr')}")
                    if hasattr(part.root, 'kind') and part.root.kind == 'file':
                        log_foundry_debug(f"PART: Part {i} is FILE - name: {getattr(part.root.file, 'name', 'no name')}, uri: {getattr(part.root.file, 'uri', 'no uri')}")
                
                log_foundry_debug(f"PART: About to call convert_part for part {i}")
                try:
                    processed_result = await self.convert_part(part, tool_context, context_id)
                    if isinstance(processed_result, list):
                        log_foundry_debug(f"PART: convert_part result for part {i}: list of {len(processed_result)} items")
                        processed_parts.extend(processed_result)
                    else:
                        if isinstance(processed_result, DataPart) and hasattr(processed_result, "data"):
                            log_foundry_debug(f"PART: convert_part result for part {i}: DataPart -> {processed_result.data}")
                        else:
                            log_foundry_debug(f"PART: convert_part result for part {i}: {type(processed_result)} - {str(processed_result)[:120]}...")
                        processed_parts.append(processed_result)
                except Exception as e:
                    print(f"‚ùå CRITICAL ERROR in convert_part for part {i}: {e}")
                    import traceback
                    print(f"‚ùå CONVERT_PART TRACEBACK: {traceback.format_exc()}")
                    raise
            
            log_debug(f"Processed {len(processed_parts)} parts")

            # Convert processed results into A2A Part wrappers for delegation
            prepared_parts_for_agents: List[Part] = []
            for processed in processed_parts:
                prepared_parts_for_agents.extend(self._wrap_item_for_agent(processed))

            # Store prepared parts for sending to agents (includes user uploads for refinement)
            session_context._latest_processed_parts = prepared_parts_for_agents
            session_context._agent_generated_artifacts = []
            log_debug(f"üì¶ Prepared {len(prepared_parts_for_agents)} parts to attach for remote agents")
            
            # If files were processed, include information about them in the message
            file_info = []
            file_contents = []
            for result in processed_parts:
                if isinstance(result, DataPart) and hasattr(result, 'data'):
                    if 'artifact-id' in result.data:
                        file_info.append(f"File uploaded: {result.data.get('file-name', 'unknown')} (Artifact ID: {result.data['artifact-id']})")
                elif isinstance(result, str) and result.startswith("File:") and "Content:" in result:
                    # This is processed file content from uploaded files
                    print(f"üìÑ Found processed file content: {len(result)} characters")
                    file_contents.append(result)
            
            # Emit completion status if files were processed
            if file_count > 0 and (file_info or file_contents):
                if file_count == 1:
                    await self._emit_status_event("file processing completed", context_id)
                else:
                    await self._emit_status_event(f"all {file_count} files processed successfully", context_id)
            
            # Enhance user message with file information and content
            enhanced_message = user_message
            if file_info:
                enhanced_message = f"{user_message}\n\n[Files uploaded: {'; '.join(file_info)}]"
            if file_contents:
                enhanced_message = f"{enhanced_message}\n\n{''.join(file_contents)}"

            # Add image edit guidance if base/mask attachments detected
            image_guidance = self._build_image_edit_guidance(processed_parts)
            if image_guidance:
                enhanced_message = f"{image_guidance}\n\n{enhanced_message}" if enhanced_message else image_guidance
            
            log_debug(f"Enhanced message prepared")
            
            # =====================================================================
            # MODE DETECTION: Auto-detect based on workflow presence
            # =====================================================================
            use_orchestration = workflow and workflow.strip()
            
            if use_orchestration:
                log_debug(f"üéØ [Workflow Mode] Workflow detected - using orchestration loop")
                await self._emit_status_event("Starting workflow orchestration...", context_id)
                
                # Use agent mode orchestration loop
                try:
                    orchestration_outputs = await self._agent_mode_orchestration_loop(
                        user_message=enhanced_message,
                        context_id=context_id,
                        session_context=session_context,
                        event_logger=event_logger,
                        workflow=workflow
                    )
                    
                    # WORKFLOW MODE: Combine outputs into single response without calling agents
                    # The orchestration loop has executed all workflow steps in order
                    print(f"‚úÖ [Workflow Mode] Workflow completed - {len(orchestration_outputs)} task outputs")
                    log_debug(f"‚úÖ [Workflow Mode] All workflow steps completed, combining outputs")
                    
                    # Combine all task outputs into a single coherent response
                    if orchestration_outputs:
                        combined_response = "\n\n".join(orchestration_outputs)
                        log_debug(f"‚úÖ [Workflow Mode] Combined {len(orchestration_outputs)} outputs into single response")
                    else:
                        combined_response = "Workflow completed successfully."
                    
                    # Return as single response (not a list)
                    final_responses = [combined_response]
                    
                    # Store the interaction and return
                    log_debug("About to store User‚ÜíHost interaction for context_id: {context_id}")
                    await self._store_user_host_interaction_safe(
                        user_message_parts=message_parts,
                        user_message_text=enhanced_message,
                        host_response=final_responses,
                        context_id=context_id,
                        span=span
                    )
                    
                    log_debug(f"üéØ [Workflow Mode] Orchestration complete, returning 1 combined response")
                    return final_responses
                    
                except Exception as e:
                    log_error(f"[Agent Mode] Orchestration error: {e}")
                    import traceback
                    traceback.print_exc()
                    
                    # Get error type and message
                    error_type = type(e).__name__
                    error_msg = str(e) if str(e) else error_type
                    
                    # If synthesis failed but we have agent outputs, return them directly
                    if orchestration_outputs:
                        print(f"‚ö†Ô∏è [Agent Mode] Synthesis failed ({error_msg}), but returning {len(orchestration_outputs)} agent outputs directly")
                        final_responses = orchestration_outputs
                        
                        # Add ONLY agent-generated artifacts if available
                        if hasattr(session_context, '_agent_generated_artifacts'):
                            artifact_dicts = []
                            for part in session_context._agent_generated_artifacts:
                                # Use utility to extract URI from any part type
                                uri = extract_uri(part)
                                if uri:
                                    # Convert to FilePart format for consistency
                                    file_part = convert_artifact_dict_to_file_part(part)
                                    if file_part:
                                        artifact_dicts.append(file_part)
                            
                            if artifact_dicts:
                                log_debug(f"üì¶ [Agent Mode] Including {len(artifact_dicts)} agent-generated artifact(s) in fallback response")
                                final_responses.extend(artifact_dicts)
                        
                        return final_responses
                    else:
                        # No outputs to return, show error
                        final_responses = [f"Agent Mode orchestration encountered an error: {error_msg}"]
                        return final_responses
            
            # Continue with standard conversation flow using Responses API (streaming)
            log_foundry_debug(f"=================== STARTING RESPONSE CREATION ===================")
            log_foundry_debug(f"Creating response with Responses API (streaming)")
            await self._emit_status_event("creating AI response with streaming", context_id)
            
            # Get tools for this agent
            tools = self._format_tools_for_responses_api()
            
            # Create streaming response
            response = await self._create_response_with_streaming(
                user_message=enhanced_message,
                context_id=context_id,
                session_context=session_context,
                tools=tools,
                instructions=self.agent.get('instructions', ''),
                event_logger=event_logger
            )
            
            log_foundry_debug(f"Response created successfully with ID: {response['id']}, status: {response['status']}")
            log_foundry_debug(f"=================== RESPONSE CREATED SUCCESSFULLY ===================")
            await self._emit_status_event(f"AI response created - status: {response['status']}", context_id)
            
            # Handle tool calls if needed (iterative loop for multi-turn tool execution)
            # NOTE: _create_response_with_streaming now handles tool calls internally, 
            # so this loop should rarely execute (only if internal loop hit max iterations)
            max_tool_iterations = 30
            tool_iteration = 0
            last_tool_output = None
            
            # Helper to check if response status requires action (handles enum and string)
            def response_requires_action(response_status):
                if response_status is None:
                    return False
                status_str = str(response_status).lower()
                return "requires_action" in status_str
            
            log_foundry_debug(f"Starting tool handling loop for response {response['id']}")
            while response_requires_action(response["status"]) and tool_iteration < max_tool_iterations:
                tool_iteration += 1
                log_foundry_debug(f"Tool iteration {tool_iteration}, response requires action")
                await self._emit_status_event(f"executing tools (attempt {tool_iteration})", context_id)
                
                # Execute all tool calls from this response
                tool_outputs = await self._execute_tool_calls_from_response(
                    tool_calls=response.get('tool_calls', []),
                    context_id=context_id,
                    session_context=session_context,
                    event_logger=event_logger
                )
                
                if tool_outputs:
                    last_tool_output = tool_outputs
                    await self._emit_status_event("tool execution completed, continuing conversation", context_id)
                    
                    # Create a new response with tool outputs to continue the conversation
                    # The tool outputs become part of the conversation history via previous_response_id chaining
                    response = await self._create_response_with_streaming(
                        user_message="",  # Empty message, tool outputs are in conversation history
                        context_id=context_id,
                        session_context=session_context,
                        tools=tools,
                        instructions=self.agent.get('instructions', ''),
                        event_logger=event_logger
                    )
                    log_foundry_debug(f"Created follow-up response after tool execution: {response['id']}, status: {response['status']}")
                else:
                    log_debug(f"‚ö†Ô∏è No tool outputs generated, breaking tool loop")
                    break
            
            log_foundry_debug(f"Tool handling loop completed. Final status: {response['status']}, iterations: {tool_iteration}")
            await self._emit_status_event("AI processing completed, finalizing response", context_id)
            
            # Get response text
            responses = []
            if response.get('text'):
                responses.append(response['text'])
                log_foundry_debug(f"Found response text: {response['text'][:100]}...")
            
            # If no valid response text found, surface tool output as fallback
            if not responses and last_tool_output:
                log_foundry_debug(f"No response text found, using tool output as fallback")
                # Extract response from tool outputs
                for tool_output in last_tool_output:
                    if isinstance(tool_output, dict) and 'output' in tool_output:
                        responses.append(tool_output['output'])
            
            log_foundry_debug(f"After response processing - responses count: {len(responses) if responses else 0}")
            if responses:
                log_foundry_debug(f"First response: {responses[0][:100]}...")
                
                # Check if files were processed and have extracted content
                has_extracted_content = False
                extracted_contents = []
                
                if processed_parts and any(isinstance(p, DataPart) for p in processed_parts):
                    artifact_info = []
                    
                    for p in processed_parts:
                        if isinstance(p, DataPart) and hasattr(p, 'data') and 'artifact-id' in p.data:
                            artifact_info.append(p.data)
                            
                            # Check if we have extracted content to display
                            if 'extracted_content' in p.data and p.data['extracted_content']:
                                file_name = p.data.get('file-name', 'uploaded file')
                                content = p.data['extracted_content']
                                extracted_contents.append(f"**{file_name}:**\n{content}")
                                has_extracted_content = True
                
                final_responses: List[str] = []

                # Include the assessment agent responses first if available
                if responses:
                    if isinstance(responses, list):
                        final_responses.extend(str(r) for r in responses if r)
                    else:
                        final_responses.append(str(responses))

                # Include ONLY agent-generated artifacts (not user uploads) in Standard Mode
                # This ensures NEW images show up with "Refine" buttons, but user uploads don't echo
                # Use centralized utility for clean artifact handling
                if hasattr(session_context, '_agent_generated_artifacts'):
                    artifact_file_parts = []  # FilePart objects (standard format)
                    video_metadata_parts = []  # DataParts with video_metadata (for video_id tracking)
                    
                    for part in session_context._agent_generated_artifacts:
                        # Check if this is a video_metadata DataPart - keep it for later
                        target = getattr(part, 'root', part)
                        if isinstance(target, DataPart) and isinstance(target.data, dict):
                            if target.data.get('type') == 'video_metadata':
                                video_metadata_parts.append(part)
                                print(f"üìé [VideoRemix] Collected video_metadata with video_id: {target.data.get('video_id')}")
                                continue  # Don't convert to FilePart
                        
                        # Use utility to extract URI from any format
                        uri = extract_uri(part)
                        if uri and uri.startswith('http'):
                            # Convert to FilePart if not already
                            if is_file_part(part):
                                # Already a FilePart, use directly
                                actual_part = part.root if hasattr(part, 'root') and is_file_part(part.root) else part
                                artifact_file_parts.append(actual_part)
                                log_debug(f"üì¶ Found FilePart artifact: {uri[:80]}...")
                            else:
                                # Convert legacy DataPart to FilePart
                                file_part = convert_artifact_dict_to_file_part(part)
                                if file_part:
                                    artifact_file_parts.append(file_part)
                                    log_debug(f"üì¶ Converted DataPart to FilePart: {uri[:80]}...")
                    
                    # Group video FileParts with their metadata to preserve video_id for remix
                    if artifact_file_parts:
                        log_debug(f"üì¶ [Standard Mode] Including {len(artifact_file_parts)} FilePart artifact(s) in response")
                        
                        # Create a Message containing all artifact parts (FileParts + video_metadata)
                        # This ensures video_id flows through to frontend
                        combined_parts = []
                        for fp in artifact_file_parts:
                            if hasattr(fp, 'root'):
                                combined_parts.append(fp)  # Already a Part
                            else:
                                combined_parts.append(Part(root=fp))
                        
                        # Add video_metadata DataParts so they're in the same Message
                        for vmp in video_metadata_parts:
                            if hasattr(vmp, 'root'):
                                combined_parts.append(vmp)
                            else:
                                combined_parts.append(Part(root=vmp))
                        
                        # Create a single Message with all parts (FileParts + video_metadata)
                        if combined_parts:
                            combined_message = Message(
                                role='agent',
                                parts=combined_parts,
                                messageId=str(uuid.uuid4()),
                            )
                            final_responses.append(combined_message)
                            print(f"üì¶ [VideoRemix] Created combined message with {len(artifact_file_parts)} FileParts and {len(video_metadata_parts)} video_metadata")
                        
                        for idx, fp in enumerate(artifact_file_parts):
                            file_obj = getattr(fp, 'file', None)
                            uri = getattr(file_obj, 'uri', '') if file_obj else ''
                            filename = getattr(file_obj, 'name', 'unknown') if file_obj else 'unknown'
                            print(f"  ‚Ä¢ FilePart Artifact {idx+1}: {filename} (URI: {uri[:80]}...)")

                # If we have extracted content, prepend it to the response
                if has_extracted_content:
                    extracted_content_message = (
                        "The file has been processed. Here is the extracted content:\n\n" + 
                        "\n\n---\n\n".join(extracted_contents)
                    )
                    log_debug(f"üìù Prepending extracted content to response...")
                    final_responses.insert(0, extracted_content_message)

                # Fallback if nothing collected yet
                if not final_responses:
                    final_responses = ["No response received"]

                # Add acknowledgement when files processed but no extracted content
                if (processed_parts and any(isinstance(p, DataPart) for p in processed_parts) and not has_extracted_content):
                    final_responses.append(
                        f"File processing completed. {len([p for p in processed_parts if isinstance(p, DataPart)])} file(s) uploaded and stored as artifacts."
                    )
                
                log_foundry_debug(f"final_responses set to: {final_responses} (FIRST PATH)")
                log_foundry_debug(f"final_responses count: {len(final_responses)} (FIRST PATH)")
                
                # Note: Conversation history is now managed by OpenAI threads - no need to store separately
                
                # Store User‚ÜíHost A2A interaction (fire-and-forget)
                log_debug(f"About to store User‚ÜíHost interaction for context_id: {context_id}")
                
                # Extract artifact info from processed parts for memory storage
                artifact_info = {}
                for i, result in enumerate(processed_parts):
                    if isinstance(result, DataPart) and hasattr(result, 'data') and 'artifact-id' in result.data:
                        artifact_info[i] = {
                            'artifact_id': result.data.get('artifact-id'),
                            'artifact_uri': result.data.get('artifact-uri'),
                            'file-name': result.data.get('file-name'),
                            'storage-type': result.data.get('storage-type')
                        }
                
                asyncio.create_task(self._store_user_host_interaction_safe(
                    user_message_parts=message_parts,  # Original parts - will be cleaned in memory storage
                    user_message_text=user_message,
                    host_response=final_responses,
                    context_id=context_id,
                    span=span,
                    artifact_info=artifact_info  # Pass artifact info for URI replacement
                ))
                
                log_foundry_debug(f"About to return final_responses: {final_responses} (FIRST PATH)")
                
                return final_responses
        
        except Exception as e:
            print(f"‚ùå CRITICAL ERROR in run_conversation_with_parts: {e}")
            import traceback
            print(f"‚ùå FULL TRACEBACK: {traceback.format_exc()}")
            raise

    async def _handle_tool_calls(self, run: Dict[str, Any], thread_id: str, context_id: str, session_context: SessionContext, event_logger=None):
        """
        Execute function tools requested by the host agent during conversation processing.
        
        The host agent can call two main tools:
        1. **list_remote_agents**: Get list of available specialized agents
        2. **send_message**: Delegate tasks to specific remote agents
        
        Key optimization: PARALLEL EXECUTION
        - When multiple agents are needed, we call them simultaneously rather than sequentially
        - This dramatically reduces latency for multi-agent workflows
        - Example: Analyzing an image AND searching a knowledge base can happen at once
        
        The function handles:
        - Parsing tool call arguments from Azure AI Foundry JSON format
        - Executing send_message calls in parallel using asyncio.gather
        - Processing list_remote_agents calls sequentially (fast operation)
        - Converting responses back to Azure AI Foundry expected format
        - Error handling for individual tool failures (doesn't fail entire batch)
        
        Args:
            run: Azure AI Foundry run object with required_action containing tool_calls
            thread_id: Azure AI Foundry thread identifier
            context_id: Conversation context for state management
            session_context: Session state with agent tracking
            event_logger: Optional callback for logging tool execution
            
        Returns:
            Last tool output for fallback response if host agent doesn't generate text
        """
        with tracer.start_as_current_span("handle_tool_calls") as span:
            span.set_attribute("context_id", context_id)
            print(f"_handle_tool_calls called for thread_id: {thread_id}, context_id: {context_id}")
            
            if not run.get('required_action'):
                log_debug("No required_action in run.")
                return None
                
            required_action = run.get('required_action')
            if not required_action.get('submit_tool_outputs'):
                log_debug("No submit_tool_outputs in required_action.")
                return None
                
            tool_calls = required_action['submit_tool_outputs']['tool_calls']
            log_debug(f"üî• OPTIMIZED: Processing {len(tool_calls)} tool calls")
            
            # Add status message for tool calls starting
            self._add_status_message_to_conversation(f"üõ†Ô∏è Executing {len(tool_calls)} tool call(s)", context_id)
            await self._emit_status_event(f"executing {len(tool_calls)} tool(s)", context_id)
            
            tool_outputs = []
            successful_tool_outputs: List[Dict[str, Any]] = []
            last_tool_output = None
            
            # NEW: Collect all send_message tool calls for parallel execution
            send_message_tasks = []
            send_message_tool_calls = []
            other_tool_calls = []
            
            # Separate send_message calls from other tool calls
            for tool_call in tool_calls:
                function_name = tool_call['function']['name']
                arguments = tool_call['function']['arguments']
                
                if isinstance(arguments, str):
                    arguments = json.loads(arguments)
                
                if function_name == "send_message":
                    # Collect send_message calls for parallel execution
                    send_message_tool_calls.append((tool_call, function_name, arguments))
                else:
                    # Keep other tool calls for sequential execution
                    other_tool_calls.append((tool_call, function_name, arguments))
            
            # NEW: Execute send_message calls in parallel
            if send_message_tool_calls:
                log_debug(f"üöÄ Executing {len(send_message_tool_calls)} send_message calls in parallel")
                span.add_event("parallel_agent_calls_started", {
                    "agent_calls_count": len(send_message_tool_calls),
                    "run_id": run["id"]
                })
                
                # Add status message for parallel agent calls
                self._add_status_message_to_conversation(f"üöÄ Executing {len(send_message_tool_calls)} agent calls in parallel", context_id)
                await self._emit_status_event(f"calling {len(send_message_tool_calls)} agent(s) in parallel", context_id)
                
                # Create tasks for all send_message calls
                for tool_call, function_name, arguments in send_message_tool_calls:
                    agent_name = arguments.get("agent_name")
                    message = arguments.get("message", "")
                    
                    # Add status message for each agent call
                    self._add_status_message_to_conversation(f"üõ†Ô∏è Executing tool: send_message to {agent_name}", context_id)
                    await self._emit_status_event(f"calling {agent_name} agent", context_id)
                    
                    # Enhanced agent selection tracking
                    span.add_event("parallel_agent_selected", {
                        "agent_name": agent_name,
                        "message_preview": message[:50] + "..." if len(message) > 50 else message
                    })
                    
                    # Stream granular tool call to WebSocket for thinking box visibility
                    asyncio.create_task(self._emit_tool_call_event(agent_name, "send_message", arguments, context_id))
                    
                    # Log tool call event - use agent_name as actor so frontend can attribute correctly
                    if event_logger:
                        event_logger({
                            "id": str(uuid.uuid4()),
                            "actor": agent_name,  # Use actual agent name, not host
                            "args": arguments,
                            "name": function_name,
                            "type": "tool_call"
                        })
                    
                    # FIXED: Use shared session context for parallel execution
                    # This ensures all parallel agents share the same conversation context
                    # Note: suppress_streaming=True prevents duplicate responses in chat (only host responds)
                    # while remote agent streaming for thinking box visibility works through task callback
                    tool_context = DummyToolContext(session_context, self._azure_blob_client)
                    task = self.send_message(agent_name, message, tool_context, suppress_streaming=True)
                    send_message_tasks.append((tool_call, task))
                
                # Execute all send_message calls in parallel
                try:
                    # Wait for all parallel tasks to complete
                    parallel_results = await asyncio.gather(*[task for _, task in send_message_tasks], return_exceptions=True)
                    
                    # Process results and create tool outputs
                    for i, (tool_call, _) in enumerate(send_message_tasks):
                        result = parallel_results[i]
                        agent_name = send_message_tool_calls[i][2].get("agent_name")
                        
                        if isinstance(result, Exception):
                            print(f"‚ùå Parallel agent call failed: {result}")
                            output = {"error": f"Agent call failed: {str(result)}"}
                            self._add_status_message_to_conversation(f"‚ùå Agent call to {agent_name} failed", context_id)
                            span.add_event("parallel_agent_call_failed", {
                                "agent_name": agent_name,
                                "error": str(result)
                            })
                            # Stream tool failure to WebSocket
                            asyncio.create_task(self._emit_tool_response_event(agent_name, "send_message", "failed", str(result), context_id))
                        else:
                            output = result
                            self._add_status_message_to_conversation(f"‚úÖ Agent call to {agent_name} completed", context_id)
                            span.add_event("parallel_agent_call_success", {
                                "agent_name": agent_name,
                                "output_type": type(output).__name__
                            })
                            # Stream tool success to WebSocket
                            asyncio.create_task(self._emit_tool_response_event(agent_name, "send_message", "success", None, context_id))
                        
                        # Log tool result event - use agent_name as actor so frontend can attribute correctly
                        if event_logger:
                            event_logger({
                                "id": str(uuid.uuid4()),
                                "actor": agent_name,  # Use actual agent name, not host
                                "name": "send_message",
                                "type": "tool_result",
                                "output": output
                            })
                        
                        # Format output for Azure AI Agents using clean text format
                        # The model expects readable text, not complex JSON wrappers
                        agent_name = send_message_tool_calls[i][2].get("agent_name")
                        normalized_text = self._format_agent_response_for_model(output, agent_name) if not isinstance(result, Exception) else json.dumps({"error": str(result), "agent": agent_name})

                        tool_outputs.append({
                            "tool_call_id": tool_call["id"],
                            "output": normalized_text
                        })

                        if not isinstance(result, Exception):
                            successful_tool_outputs.append({"agent": agent_name, "response": normalized_text})
                            last_tool_output = {"agent": agent_name, "response": normalized_text}
                    
                    print(f"‚úÖ Parallel agent calls completed successfully")
                    self._add_status_message_to_conversation("‚úÖ All parallel agent calls completed", context_id)
                    await self._emit_status_event("all agent calls completed", context_id)
                    span.add_event("parallel_agent_calls_completed", {
                        "successful_calls": len([r for r in parallel_results if not isinstance(r, Exception)]),
                        "failed_calls": len([r for r in parallel_results if isinstance(r, Exception)])
                    })
                    
                except Exception as e:
                    print(f"‚ùå Error in parallel agent execution: {e}")
                    self._add_status_message_to_conversation(f"‚ùå Parallel execution failed: {str(e)}", context_id)
                    span.add_event("parallel_agent_execution_error", {
                        "error": str(e)
                    })
                    # Fallback to sequential execution if parallel fails
                    for tool_call, function_name, arguments in send_message_tool_calls:
                        output = {"error": f"Parallel execution failed: {str(e)}"}
                        tool_outputs.append({
                            "tool_call_id": tool_call["id"],
                            "output": json.dumps({
                                "kind": "function_response",
                                "name": function_name,
                                "response": output
                            })
                        })
            
            # Execute other tool calls sequentially (as before)
            for tool_call, function_name, arguments in other_tool_calls:
                print(f"Handling tool: {function_name}, args: {arguments}")
                
                # Add status message for each tool call
                self._add_status_message_to_conversation(f"üõ†Ô∏è Executing tool: {function_name}", context_id)
                await self._emit_status_event(f"executing {function_name} tool", context_id)
                
                # Enhanced tool call tracking
                span.add_event(f"tool_call: {function_name}", {
                    "function_name": function_name,
                    "tool_call_id": tool_call["id"],
                    "arguments_keys": list(arguments.keys()) if isinstance(arguments, dict) else [],
                    "arguments_summary": str(arguments)[:200] + "..." if len(str(arguments)) > 200 else str(arguments)
                })
                
                # Stream tool call to WebSocket for thinking box visibility
                await self._emit_tool_call_event("foundry-host-agent", function_name, arguments, context_id)
                
                # Log tool call event and add span event
                if event_logger:
                    event_logger({
                        "id": str(uuid.uuid4()),
                        "actor": "foundry-host-agent",
                        "args": arguments,
                        "name": function_name,
                        "type": "tool_call"
                    })
                
                if function_name == "list_remote_agents":
                    output = self.list_remote_agents()
                    self._add_status_message_to_conversation(f"‚úÖ Tool {function_name} completed", context_id)
                    span.add_event("agents_listed", {
                        "available_agents_count": len(output),
                        "agent_names": [agent.get("name", "unknown") for agent in output] if output else []
                    })
                    # Stream tool success to WebSocket
                    await self._emit_tool_response_event("foundry-host-agent", function_name, "success", None, context_id)
                else:
                    output = {"error": f"Unknown function: {function_name}"}
                    self._add_status_message_to_conversation(f"‚ùå Unknown tool: {function_name}", context_id)
                    span.add_event("unknown_function_called", {
                        "function_name": function_name,
                        "available_functions": ["list_remote_agents", "send_message"]
                    })
                    # Stream tool failure to WebSocket
                    await self._emit_tool_response_event("foundry-host-agent", function_name, "failed", f"Unknown function: {function_name}", context_id)
                
                # Log tool result event
                if event_logger:
                    event_logger({
                        "id": str(uuid.uuid4()),
                        "actor": "foundry-host-agent",
                        "name": function_name,
                        "type": "tool_result",
                        "output": output
                    })
                
                # Track tool execution outcome
                if isinstance(output, dict) and "error" in output:
                    span.add_event("tool_execution_error", {
                        "function_name": function_name,
                        "error": output["error"]
                    })
                elif output:
                    span.add_event("tool_execution_success", {
                        "function_name": function_name,
                        "output_type": type(output).__name__,
                        "output_size": len(str(output))
                    })
                
                # Format output for Azure AI Agents
                normalized_output = self._normalize_function_response_text(output)
                if isinstance(normalized_output, list):
                    normalized_text = "\n\n".join(str(item) for item in normalized_output)
                elif isinstance(normalized_output, str):
                    normalized_text = normalized_output
                else:
                    normalized_text = str(normalized_output)

                payload = {
                    "kind": "function_response",
                    "name": function_name,
                    "response": normalized_text
                }

                tool_outputs.append({
                    "tool_call_id": tool_call["id"],
                    "output": json.dumps(payload)
                })

                successful_tool_outputs.append(payload)
                last_tool_output = payload
            
            # Track overall tool execution summary
            total_calls = len(send_message_tool_calls) + len(other_tool_calls)
            span.set_attribute("tools.total_calls", total_calls)
            span.set_attribute("tools.parallel_calls", len(send_message_tool_calls))
            span.set_attribute("tools.sequential_calls", len(other_tool_calls))
            span.set_attribute("tools.successful_calls", len([o for o in tool_outputs if o]))
            
            # Submit tool outputs
            span.add_event("tool_outputs_submitted", {
                "tool_outputs_count": len(tool_outputs),
                "parallel_calls": len(send_message_tool_calls),
                "sequential_calls": len(other_tool_calls),
                "run_id": run["id"],
                "thread_id": thread_id
            })
            
            # Submit tool outputs via HTTP API
            log_debug(f"üî• OPTIMIZED: Submitting {len(tool_outputs)} tool outputs in one batch")
            await self._http_submit_tool_outputs(thread_id, run["id"], tool_outputs)
            
            if successful_tool_outputs:
                combined_text = "\n\n---\n\n".join(
                    payload.get("response", "") for payload in successful_tool_outputs if isinstance(payload.get("response"), str)
                )
                return combined_text

            if isinstance(last_tool_output, dict):
                response_text = last_tool_output.get("response")
                if isinstance(response_text, str):
                    return response_text

            return last_tool_output

    async def convert_parts(self, parts: List[Part], tool_context: Any, context_id: str = None):
        rval = []
        log_debug(f"convert_parts: processing {len(parts)} parts")
        session_context = getattr(tool_context, "state", None)
        
        # In agent mode, preserve existing parts; otherwise start fresh
        if session_context is not None:
            if hasattr(session_context, "agent_mode") and session_context.agent_mode:
                # Agent mode: Preserve existing files from previous agents
                latest_parts = getattr(session_context, "_latest_processed_parts", [])
                log_debug(f"üìé [Agent Mode] Preserving {len(latest_parts)} existing file parts from previous agents")
            else:
                # User mode: Start fresh for each response
                latest_parts: List[Any] = []
            setattr(session_context, "_latest_processed_parts", latest_parts)
        else:
            latest_parts: List[Any] = []

        for i, p in enumerate(parts):
            result = await self.convert_part(p, tool_context, context_id)
            if result is None:
                continue
            if isinstance(result, list):
                rval.extend(result)
            else:
                rval.append(result)

        # URI tracking for deduplication
        uri_to_parts: Dict[str, List[Any]] = {}
        assigned_roles: Dict[str, str] = {}

        def _register_part_uri(part: Any, uri: Optional[str]) -> None:
            normalized_uri = self._normalize_uri(uri)
            if not normalized_uri:
                return
            uri_to_parts.setdefault(normalized_uri, []).append(part)

        def _register_role(uri: Optional[str], role: Optional[str]) -> None:
            if not role:
                return
            normalized_uri = self._normalize_uri(uri)
            if not normalized_uri:
                return
            assigned_roles[normalized_uri] = str(role).lower()

        # Use centralized utility for URI extraction
        def _local_extract_uri(part: Any) -> Optional[str]:
            return extract_uri(part)

        flattened_parts = []
        pending_file_parts: List[FilePart] = []
        refine_payload = None

        for item in rval:
            if isinstance(item, DataPart):
                if hasattr(item, "data") and isinstance(item.data, dict):
                    artifact_uri = item.data.get("artifact-uri")
                    existing_role = item.data.get("role") or (item.data.get("metadata") or {}).get("role")
                    name_hint = item.data.get("file-name") or item.data.get("name") or item.data.get("artifact-id")
                    role_value = self._infer_file_role(existing_role, name_hint)

                    if role_value and str(role_value).lower() != (existing_role or "").lower():
                        item.data["role"] = role_value
                        metadata_block = item.data.get("metadata") or {}
                        metadata_block["role"] = role_value
                        item.data["metadata"] = metadata_block

                    metadata = {
                        "artifact-id": item.data.get("artifact-id"),
                        "storage-type": item.data.get("storage-type"),
                        "file-name": item.data.get("file-name"),
                        "artifact-uri": artifact_uri,
                    }
                    existing_meta = (item.data.get("metadata") or {}).copy()
                    if role_value:
                        metadata["role"] = role_value
                        existing_meta["role"] = role_value
                    elif existing_role:
                        metadata["role"] = existing_role
                        existing_meta["role"] = existing_role
                    metadata["metadata"] = existing_meta

                    data_part = DataPart(data=metadata)
                    flattened_parts.append(data_part)
                    _register_part_uri(data_part, artifact_uri)
                    if role_value:
                        _register_role(artifact_uri, role_value)

                    if artifact_uri:
                        # Create FileWithUri for remote agent processing
                        file_with_uri_kwargs = {
                            "name": item.data.get("file-name", metadata["artifact-id"]) or metadata["artifact-id"],
                            "mimeType": item.data.get("media-type", "application/octet-stream"),
                            "uri": artifact_uri,
                        }
                        
                        # Create FilePart with metadata containing role (remote agents check p.metadata.role)
                        file_part_kwargs = {"file": FileWithUri(**file_with_uri_kwargs)}
                        if role_value:
                            file_part_kwargs["metadata"] = {"role": role_value}
                            log_debug(f"üé≠ Creating FilePart with metadata role='{role_value}' for {file_with_uri_kwargs['name']}")
                        
                        file_part = FilePart(**file_part_kwargs)
                        flattened_parts.append(file_part)
                        pending_file_parts.append(file_part)
                        _register_part_uri(file_part, artifact_uri)
                        if role_value:
                            _register_role(artifact_uri, role_value)

                    if "extracted_content" in item.data:
                        flattened_parts.append(
                            TextPart(text=str(item.data["extracted_content"]))
                        )
                else:
                    flattened_parts.append(TextPart(text=str(item.data)))
            elif isinstance(item, (TextPart, FilePart, DataPart)):
                flattened_parts.append(item)
                _register_part_uri(item, _local_extract_uri(item))
            elif isinstance(item, dict):
                if item.get("kind") == "refine-image":
                    refine_payload = item
                elif "artifact-uri" in item or "artifact-id" in item:
                    # This is artifact metadata from an agent - wrap in DataPart
                    artifact_uri = item.get("artifact-uri", "")
                    log_debug(f"üì¶ [DEBUG] Wrapping artifact dict in DataPart:")
                    log_debug(f"   artifact-uri (first 150 chars): {artifact_uri[:150]}")
                    log_debug(f"   Has SAS token (?): {'?' in artifact_uri}")
                    artifact_data_part = DataPart(data=item)
                    flattened_parts.append(artifact_data_part)
                    # Don't add to latest_parts here - it will be added via extend below to avoid duplicates
                    _register_part_uri(artifact_data_part, item.get("artifact-uri"))
                    if item.get("role"):
                        _register_role(item.get("artifact-uri"), item.get("role"))
                else:
                    text = item.get("response") or item.get("text") or json.dumps(item, ensure_ascii=False)
                    flattened_parts.append(TextPart(text=text))
            elif item is not None:
                flattened_parts.append(TextPart(text=str(item)))

        # Add all flattened parts to latest_parts (includes artifacts already wrapped in DataParts above)
        latest_parts.extend(flattened_parts)

        if refine_payload:
            refine_part = DataPart(data=refine_payload)
            flattened_parts.append(refine_part)
            latest_parts.append(refine_part)

        # If we collected file parts, ensure downstream agents can access them
        if pending_file_parts:
            latest_parts.extend(pending_file_parts)

        base_uri_hint = self._normalize_uri((refine_payload or {}).get("image_url"))
        mask_uri_hint = self._normalize_uri((refine_payload or {}).get("mask_url"))

        if base_uri_hint or mask_uri_hint:
            for part in flattened_parts:
                candidate_uri = self._normalize_uri(_local_extract_uri(part))
                if base_uri_hint and candidate_uri == base_uri_hint:
                    self._apply_role_to_part(part, "base")
                    _register_role(candidate_uri, "base")
                if mask_uri_hint and candidate_uri == mask_uri_hint:
                    self._apply_role_to_part(part, "mask")
                    _register_role(candidate_uri, "mask")

        for uri_value, parts_list in uri_to_parts.items():
            if assigned_roles.get(uri_value):
                continue
            
            # Check if this is a generated/edited artifact - don't assign default overlay role
            # Extract filename from URI to check
            file_name_from_uri = uri_value.split('/')[-1].split('?')[0].lower() if uri_value else ""
            is_generated_artifact = "generated_" in file_name_from_uri or "edit_" in file_name_from_uri
            
            if is_generated_artifact:
                log_debug(f"Skipping default 'overlay' role for generated artifact: {file_name_from_uri}")
                # Don't assign any role - keep generated artifacts separate for display
                continue
            
            # For other files (user uploads, logos, etc.), assign overlay as default
            for part in parts_list:
                self._apply_role_to_part(part, "overlay")
            assigned_roles[uri_value] = "overlay"

        def _apply_assigned_roles(parts: Iterable[Any]) -> None:
            for part in parts:
                uri = self._normalize_uri(_local_extract_uri(part))
                if not uri:
                    continue
                role_for_uri = assigned_roles.get(uri)
                if role_for_uri:
                    self._apply_role_to_part(part, role_for_uri)

        _apply_assigned_roles(flattened_parts)
        _apply_assigned_roles(latest_parts)

        # DEBUG: Log what we're returning
        log_foundry_debug(f"convert_parts returning {len(flattened_parts)} parts:")
        for idx, part in enumerate(flattened_parts):
            if isinstance(part, (TextPart, DataPart, FilePart)):
                log_debug(f"  ‚Ä¢ Part {idx}: {type(part).__name__} (kind={getattr(part, 'kind', 'N/A')})")
            elif isinstance(part, dict):
                log_debug(f"  ‚Ä¢ Part {idx}: dict with keys={list(part.keys())}")
            elif isinstance(part, str):
                log_debug(f"  ‚Ä¢ Part {idx}: string (length={len(part)})")
            else:
                log_debug(f"  ‚Ä¢ Part {idx}: {type(part)}")

        return flattened_parts

    async def convert_part(self, part: Part, tool_context: Any, context_id: str = None):
        """
        Convert A2A Part objects into formats suitable for processing and agent delegation.
        
        Part types and their handling:
        
        1. **TextPart**: Simple text content, passed through unchanged
        
        2. **FilePart**: Uploaded files requiring processing
           - Extract text from PDFs, Word docs, etc. using document processor
           - Store in Azure Blob Storage or local filesystem
           - Generate artifact URIs for agent access
           - Handle special cases like image masks for editing
        
        3. **DataPart**: Structured data or metadata
           - JSON objects with agent-specific information
           - Configuration parameters
           - File metadata and references
        
        File processing workflow:
        - Validate file size (50MB limit for security)
        - Determine storage strategy (blob vs local based on size)
        - Extract text content using Azure AI Document Intelligence
        - Create artifact with unique ID and accessible URI
        - Return metadata for agent consumption
        
        Special handling for image editing:
        - Base images: Source image to be edited
        - Mask images: Transparency mask defining edit regions
        - Overlay images: Images to composite onto base
        
        Args:
            part: A2A Part object to convert
            tool_context: Context with artifact storage and session state
            context_id: Optional conversation context for status updates
            
        Returns:
            Converted part(s) ready for agent consumption or host processing
        """
        # Don't print the entire part (contains large base64 data)
        if hasattr(part, 'root') and part.root.kind == 'file':
            file_name = getattr(part.root.file, 'name', 'unknown')
            mime_type = getattr(part.root.file, 'mimeType', 'unknown')
            log_debug(f"convert_part: FilePart - name: {file_name}, mimeType: {mime_type}")
            
            # Emit status event for file processing
            if context_id:
                await self._emit_status_event(f"processing file: {file_name}", context_id)
        else:
            log_debug(f"convert_part: {type(part)} - kind: {getattr(part.root, 'kind', 'unknown') if hasattr(part, 'root') else 'no root'}")
        
        # Handle dicts coming from streaming conversions or patched remote agents
        if isinstance(part, dict):
            # Simple heuristic: if it looks like {'kind': 'text', 'text': '...'}
            if part.get('kind') == 'text' and 'text' in part:
                text_content = part['text']
                log_debug(f"convert_part: dict text content: {text_content[:200]}...")
                return text_content
            if part.get('kind') == 'data' and 'data' in part:
                return part['data']
            # Fallthrough ‚Äì stringify the dict
            return json.dumps(part)

        # Fallback to standard A2A Part handling
        if hasattr(part, 'root') and part.root.kind == 'text':
            text_content = part.root.text or ""
            log_debug(f"convert_part: text part content: {text_content[:200]}...")

            # Check for [refine-image] markers (image editing workflow)
            refine_matches = list(re.finditer(r"\[refine-image\]\s+(https?://\S+)", text_content, flags=re.IGNORECASE))
            if refine_matches:
                mask_matches = list(re.finditer(r"\[refine-mask\]\s+(https?://\S+)", text_content, flags=re.IGNORECASE))
                image_url = refine_matches[-1].group(1)
                mask_url = mask_matches[-1].group(1) if mask_matches else None

                # Strip markers from text
                cleaned_text = re.sub(r"\[refine-image\]\s+https?://\S+", "", text_content, flags=re.IGNORECASE)
                cleaned_text = re.sub(r"\[refine-mask\]\s+https?://\S+", "", cleaned_text, flags=re.IGNORECASE)

                refine_data = {"kind": "refine-image", "image_url": image_url}
                if mask_url:
                    refine_data["mask_url"] = mask_url

                self._store_parts_in_session(tool_context, DataPart(data=refine_data))
                log_debug(f"convert_part: captured refine request with image_url={image_url}")

                return cleaned_text.strip() or "Refine the previous image."

            return text_content
        elif hasattr(part, 'root') and part.root.kind == 'data':
            data = part.root.data
            # Skip token_usage DataParts - already extracted earlier, not for main chat
            if isinstance(data, dict) and data.get('type') == 'token_usage':
                return None
            log_debug(f"DataPart data: {data} (type: {type(data)})")
            # IMPORTANT: Preserve DataPart wrapper for metadata types that need to flow through
            # (video_metadata, image_metadata, etc.) so they can be detected by artifact processing
            if isinstance(data, dict) and data.get('type') in ('video_metadata', 'image_metadata'):
                log_debug(f"üìé [VideoRemix] Preserving DataPart wrapper for {data.get('type')} with video_id={data.get('video_id')}")
                return part  # Return the full Part(root=DataPart(...)) to preserve structure
            return data
        elif hasattr(part, 'root') and part.root.kind == 'file':
            # A2A protocol compliant file handling
            file_id = part.root.file.name
            log_debug(f"Processing file: {file_id}")
            
            file_role_attr = getattr(part.root.file, 'role', None)
            
            # Load file bytes from URI, inline bytes, or HTTP download
            file_bytes, load_error = self._load_file_bytes(part.root.file, context_id)
            if load_error:
                log_debug(f"File load error: {load_error}")
                return f"Error: {load_error}"
            
            # Security: Validate file size (50MB limit)
            if len(file_bytes) > 50 * 1024 * 1024:
                return DataPart(data={'error': 'File too large', 'max_size': '50MB'})
            
            # Infer role if not explicitly set
            if not file_role_attr:
                file_role_attr = self._infer_file_role(None, file_id)
            
            is_mask_artifact = (file_role_attr == "mask")
            artifact_response = None

            artifact_info: dict[str, Any] = {
                'file_name': file_id,
                'file_bytes': file_bytes,
            }
            if file_role_attr:
                artifact_info['role'] = str(file_role_attr).lower()
            
            # Save artifact using A2A protocol
            if not hasattr(tool_context, 'save_artifact'):
                log_error(f"tool_context missing save_artifact method")
                return DataPart(data={'error': f'File processing unavailable for {file_id}'})
            
            try:
                # Create A2A file part structure
                a2a_file_part = {
                    'kind': 'file',
                    'file': {
                        'name': file_id,
                        'mimeType': getattr(part.root.file, 'mimeType', 'application/octet-stream'),
                        'data': file_bytes,
                        'force_blob': is_mask_artifact,
                        **({'role': str(file_role_attr)} if file_role_attr else {}),
                    }
                }
                
                artifact_response = await tool_context.save_artifact(file_id, a2a_file_part)
                tool_context.actions.skip_summarization = True
                tool_context.actions.escalate = True
                
                if isinstance(artifact_response, DataPart) and hasattr(artifact_response, 'data'):
                    if file_role_attr:
                        artifact_response.data['role'] = str(file_role_attr).lower()
                        meta = artifact_response.data.get('metadata') or {}
                        meta['role'] = str(file_role_attr).lower()
                        artifact_response.data['metadata'] = meta
                    
                    artifact_info.update({
                        'artifact_id': artifact_response.data.get('artifact-id'),
                        'artifact_uri': artifact_response.data.get('artifact-uri'),
                        'storage_type': artifact_response.data.get('storage-type')
                    })
            except Exception as e:
                log_error(f"save_artifact failed: {e}")
                artifact_response = DataPart(data={'error': f'Failed to process file: {str(e)}'})
            
            if is_mask_artifact:
                log_debug(f"Skipping document processing for mask artifact: {file_id}")
                
                # Get artifact URI from response or info
                artifact_uri = None
                if isinstance(artifact_response, DataPart) and hasattr(artifact_response, 'data'):
                    artifact_uri = artifact_response.data.get('artifact-uri')
                if not artifact_uri:
                    artifact_uri = artifact_info.get('artifact_uri') or getattr(part.root.file, 'uri', None)
                
                # Build mask parts using helper
                mime_type = getattr(part.root.file, 'mimeType', 'application/octet-stream')
                if isinstance(artifact_response, DataPart) and hasattr(artifact_response, 'data'):
                    mime_type = artifact_response.data.get('media-type', mime_type)
                
                mask_metadata_part, mask_file_part = self._build_mask_parts(
                    file_id=file_id,
                    mime_type=mime_type,
                    artifact_uri=artifact_uri,
                    artifact_info=artifact_info,
                    file_bytes=file_bytes,
                    artifact_response=artifact_response if isinstance(artifact_response, DataPart) else None,
                )

                # Store parts in session context for later access
                self._store_parts_in_session(tool_context, mask_metadata_part, mask_file_part)

                # Emit completion status event for mask file
                if context_id:
                    await self._emit_status_event(f"file processed successfully: {file_id}", context_id)
                
                return [mask_metadata_part, mask_file_part]

            # Process file content (text extraction for documents)
            session_id = get_tenant_from_context(context_id) if context_id else None
            try:
                processing_result = await a2a_document_processor.process_file_part(
                    part.root.file, 
                    artifact_info,
                    session_id=session_id
                )
                
                if processing_result and isinstance(processing_result, dict) and processing_result.get("success"):
                    content = processing_result.get("content", "")
                    if context_id:
                        await self._emit_status_event(f"file processed successfully: {file_id}", context_id)
                    
                    if isinstance(artifact_response, DataPart) and hasattr(artifact_response, 'data'):
                        artifact_response.data['extracted_content'] = content
                        artifact_response.data['content_preview'] = content[:500] + "..." if len(content) > 500 else content
            except Exception as e:
                log_debug(f"Document processing error for {file_id}: {e}")
            
            # Return artifact with optional FilePart for remote agents
            if isinstance(artifact_response, DataPart):
                artifact_uri = artifact_response.data.get('artifact-uri')
                if artifact_uri:
                    file_part_for_remote = FilePart(
                        kind='file',
                        file=FileWithUri(
                            name=file_id,
                            mimeType=artifact_response.data.get('media-type', getattr(part.root.file, 'mimeType', 'application/octet-stream')),
                            uri=artifact_uri,
                            role=str(file_role_attr).lower() if file_role_attr else None,
                        ),
                    )
                    self._store_parts_in_session(tool_context, artifact_response, Part(root=file_part_for_remote))
                
                return artifact_response
            
            return DataPart(data={'error': f'File {file_id} processed without artifact metadata'})

    async def register_remote_agent(self, agent_address: str, agent_card: Optional[AgentCard] = None) -> bool:
        """Handle self-registration from remote agents.
        
        Args:
            agent_address: The URL/address of the remote agent
            agent_card: Optional pre-built agent card (if not provided, will retrieve from address)
            
        Returns:
            bool: True if registration successful, False otherwise
        """
        try:
            print(f"ü§ù Self-registration request from agent at: {agent_address}")
            
            if agent_card:
                # Use provided agent card
                print(f"‚úÖ Using provided agent card: {agent_card.name}")
                self.register_agent_card(agent_card)
            else:
                # Retrieve agent card from address
                print(f"üîç Retrieving agent card from: {agent_address}")
                await self.retrieve_card(agent_address)
            
            log_success(f"Successfully registered remote agent from: {agent_address}")
            print(f"üìä Total registered agents: {len(self.remote_agent_connections)}")
            log_debug(f"üìã Agent names: {list(self.remote_agent_connections.keys())}")
            
            # Agent will appear in UI sidebar within 15 seconds via periodic sync
            
            return True
            
        except Exception as e:
            print(f"‚ùå Failed to register remote agent from {agent_address}: {e}")
            import traceback
            print(f"‚ùå Registration error traceback: {traceback.format_exc()}")
            return False

    async def unregister_remote_agent(self, agent_name: str) -> bool:
        """Handle unregistration of remote agents.
        
        Args:
            agent_name: The name of the agent to unregister
            
        Returns:
            bool: True if unregistration successful, False otherwise
        """
        try:
            print(f"üóëÔ∏è Unregistration request for agent: {agent_name}")
            
            # Check if agent exists
            if agent_name not in self.remote_agent_connections and agent_name not in self.cards:
                print(f"‚ùå Agent {agent_name} not found in registry")
                return False
            
            # Remove from remote_agent_connections
            if agent_name in self.remote_agent_connections:
                del self.remote_agent_connections[agent_name]
                print(f"‚úÖ Removed {agent_name} from remote_agent_connections")
            
            # Remove from cards
            if agent_name in self.cards:
                del self.cards[agent_name]
                print(f"‚úÖ Removed {agent_name} from cards")
            
            # Update the agents list used in prompts
            self.agents = json.dumps(self.list_remote_agents(), indent=2)
            print(f"‚úÖ Updated agents list for prompts")
            
            log_success(f"Successfully unregistered agent: {agent_name}")
            print(f"üìä Total registered agents: {len(self.remote_agent_connections)}")
            log_debug(f"üìã Agent names: {list(self.remote_agent_connections.keys())}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Failed to unregister agent {agent_name}: {e}")
            import traceback
            print(f"‚ùå Unregistration error traceback: {traceback.format_exc()}")
            return False

    @staticmethod
    def create_with_shared_client(remote_agent_addresses: List[str], task_callback: Optional[TaskUpdateCallback] = None, enable_task_evaluation: bool = True, create_agent_at_startup: bool = False):
        """
        Factory method to create a FoundryHostAgent2 with a shared httpx.AsyncClient and optional task evaluation.
        
        Note: create_agent_at_startup defaults to False since Responses API is stateless and doesn't need agent creation.
        """
        shared_client = httpx.AsyncClient()
        return FoundryHostAgent2(remote_agent_addresses, http_client=shared_client, task_callback=task_callback, enable_task_evaluation=enable_task_evaluation, create_agent_at_startup=create_agent_at_startup)

    def set_host_manager(self, host_manager):
        """Set reference to the host manager for UI integration."""
        self._host_manager = host_manager

    def _add_status_message_to_conversation(self, status_text: str, contextId: str):
        """Add a status message directly to the conversation for immediate UI display."""
        # Use WebSocket streaming for real-time status updates
        asyncio.create_task(self._emit_granular_agent_event("foundry-host-agent", status_text, contextId))

    async def _emit_status_event(self, status_text: str, context_id: str):
        """Emit status event to WebSocket for real-time frontend updates."""
        # Use WebSocket streaming for real-time status updates
        await self._emit_granular_agent_event("foundry-host-agent", status_text, context_id)

    async def _emit_text_chunk(self, chunk: str, context_id: str):
        """
        Emit text chunk to WebSocket for real-time streaming display.
        
        This enables ChatGPT-style token-by-token streaming in the UI.
        """
        try:
            from service.websocket_streamer import get_websocket_streamer
            websocket_streamer = await get_websocket_streamer()
            
            if websocket_streamer:
                await websocket_streamer._send_event(
                    "message_chunk",
                    {
                        "contextId": context_id,
                        "chunk": chunk,
                        "timestamp": datetime.now().isoformat()
                    },
                    partition_key=context_id
                )
        except Exception as e:
            log_error(f"Failed to emit text chunk: {e}")

    @staticmethod
    def _normalize_function_response_text(raw_response: Any) -> Any:
        """Flatten tool payload wrappers so the UI receives plain text.

        Azure AI Foundry sometimes echoes the tool output JSON (or Python repr)
        directly into the thread. When that happens the chat renders the raw
        dictionary instead of the underlying text. This helper converts those
        structures back into strings.
        """

        if not isinstance(raw_response, str):
            if isinstance(raw_response, dict) and raw_response.get("kind") == "function_response":
                payload = raw_response.get("response")
                if isinstance(payload, list):
                    return "\n\n".join(str(item) for item in payload)
                if isinstance(payload, str):
                    return payload
                return payload
            return raw_response

        candidate = raw_response.strip()
        if not candidate:
            return raw_response

        parsed = None

        # Try JSON first
        try:
            parsed = json.loads(candidate)
        except Exception:
            # Fall back to Python literal (single-quoted) repr
            try:
                parsed = ast.literal_eval(candidate)
            except Exception:
                parsed = None

        if isinstance(parsed, dict) and parsed.get("kind") == "function_response":
            payload = parsed.get("response")
            if isinstance(payload, list):
                return "\n\n".join(str(item) for item in payload)
            if isinstance(payload, str):
                return payload
            return payload

        if isinstance(parsed, list):
            return [str(item) for item in parsed]

        return raw_response

